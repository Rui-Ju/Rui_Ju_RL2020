{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vSscTqPgwFz",
    "outputId": "d72d455d-ef5f-4d21-8321-49c8ccb17d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import gym\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saT54K-d8HMX"
   },
   "source": [
    "## **Problem 1**: Cartpole-v0\n",
    "Train a simple neural net that models the policy. Use discount factor = 0.95. Plot (average) episode reward versus training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r03TU5-ljY5r"
   },
   "source": [
    "Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YdfjkOxiicNo"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwoMfnT5jdUn"
   },
   "source": [
    "Environment and Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LlTCPSEdjcZX"
   },
   "outputs": [],
   "source": [
    "# Environment hyperparameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "possible_actions = np.identity(action_size,dtype=int).tolist()\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500\n",
    "batch_size = 1000\n",
    "\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YF1jR2ONMQKL"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.95, normalization=False):\n",
    "    \"take 1D float array of rewards and compute discounted rewards\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "  \n",
    "    if normalization:\n",
    "        mean = np.mean(discounted_r)\n",
    "        std = np.std(discounted_r)\n",
    "        discounted_r = (discounted_r - mean)/std\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh9HCcNOOEua"
   },
   "source": [
    "Prepare 4 last processed frames to be fed to Conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3BEeMK5Gfpqi"
   },
   "outputs": [],
   "source": [
    "class PGNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = \"PGNetwork\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name = \"inputs_\")\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name = \"actions\")\n",
    "                self.discounted_episode_reward_delta = tf.placeholder(tf.float32, [None, ], name = \"discounted_episode_reward_delta\")\n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.inputs_,\n",
    "                                          units = 512,\n",
    "                                          activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                          name = \"fc1\")\n",
    "\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc,\n",
    "                                          units = action_size,\n",
    "                                          activation = None,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.actions)\n",
    "                self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_reward_delta)\n",
    "                self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4Z4IxakD9qc4"
   },
   "outputs": [],
   "source": [
    "class ValueEstimator:\n",
    "    def __init__(self, state_size, learning_rate, name = \"ValueEstimator\"):\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name = \"inputs_\")\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name = \"discounted_episode_rewards_\")\n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.inputs_,\n",
    "                                          units = 512,\n",
    "                                          activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                          name = \"fc1\")\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.output = tf.layers.dense(inputs = self.fc,\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            with tf.name_scope(\"output\"):\n",
    "                self.state_value_estimation = tf.squeeze(self.output)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.state_value_estimation, self.discounted_episode_rewards_))\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dlLaIfJVG_q"
   },
   "outputs": [],
   "source": [
    "def make_batch(batch_size):\n",
    "    states, episode_states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards, discounted_reward_delta = [],[],[],[],[],[],[]\n",
    "\n",
    "    episode_num = 1\n",
    "\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, feed_dict = {PGNetwork.inputs_:state.reshape(1, state_size)}) \n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), p = action_probability_distribution.ravel())\n",
    "\n",
    "        next_step, reward, done, info = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        episode_states.append(state)\n",
    "        action_ = [1 if i==action else 0 for i in range(action_size)]\n",
    "        actions.append(action_)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            discounted_rewards_of_episode = discount_rewards(rewards_of_episode, normalization = False)\n",
    "            discounted_rewards.append(discounted_rewards_of_episode)\n",
    "\n",
    "            value_est = sess.run(ValueEstimator.state_value_estimation, feed_dict={ValueEstimator.inputs_:np.stack(episode_states).reshape((len(episode_states), state_size))})\n",
    "            discounted_reward_delta.append(discounted_rewards_of_episode - value_est)\n",
    "\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "\n",
    "            rewards_of_episode = []\n",
    "            episode_states = []\n",
    "            episode_num += 1\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_step\n",
    "\n",
    "\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), np.concatenate(discounted_reward_delta), episode_num\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBiGzMg5Ig_T"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "PGNetwork = PGNetwork(state_size, action_size, learning_rate)\n",
    "ValueEstimator = ValueEstimator(state_size, learning_rate)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20PXJDtrVjnt",
    "outputId": "55b99ea3-815e-4941-a41c-14b518b7bd17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch:  6 / 500\n",
      "---------------\n",
      "Number of training episodes: 48\n",
      "Total reward of the batch: 1020.0\n",
      "Mean reward of the batch: 21.25\n",
      "Average reward of all training: 22.528392639106926\n",
      "Max reward for a batch so far: 1029.0\n",
      "Training Loss: 5.9456305503845215\n",
      "Cross Entropy: [0.69399774 0.66529703 0.6933142  ... 0.7229912  0.7407712  0.6326965 ]\n",
      "=======================================================\n",
      "Epoch:  7 / 500\n",
      "---------------\n",
      "Number of training episodes: 47\n",
      "Total reward of the batch: 1012.0\n",
      "Mean reward of the batch: 21.53191489361702\n",
      "Average reward of all training: 22.38603867546551\n",
      "Max reward for a batch so far: 1029.0\n",
      "Training Loss: 6.135486125946045\n",
      "Cross Entropy: [0.69433445 0.6632672  0.69421387 ... 0.7869379  0.81710654 0.847996  ]\n",
      "=======================================================\n",
      "Epoch:  8 / 500\n",
      "---------------\n",
      "Number of training episodes: 48\n",
      "Total reward of the batch: 1006.0\n",
      "Mean reward of the batch: 20.958333333333332\n",
      "Average reward of all training: 22.207575507698987\n",
      "Max reward for a batch so far: 1029.0\n",
      "Training Loss: 6.022331714630127\n",
      "Cross Entropy: [0.6933247  0.72187555 0.74919295 ... 0.71373296 0.7380195  0.62503326]\n",
      "=======================================================\n",
      "Epoch:  9 / 500\n",
      "---------------\n",
      "Number of training episodes: 43\n",
      "Total reward of the batch: 1005.0\n",
      "Mean reward of the batch: 23.372093023255815\n",
      "Average reward of all training: 22.336966342760856\n",
      "Max reward for a batch so far: 1029.0\n",
      "Training Loss: 6.222599506378174\n",
      "Cross Entropy: [0.6940732  0.72086203 0.7491794  ... 0.6994972  0.71576405 0.65737104]\n",
      "=======================================================\n",
      "Epoch:  10 / 500\n",
      "---------------\n",
      "Number of training episodes: 45\n",
      "Total reward of the batch: 1017.0\n",
      "Mean reward of the batch: 22.6\n",
      "Average reward of all training: 22.363269708484772\n",
      "Max reward for a batch so far: 1029.0\n",
      "Training Loss: 6.190125465393066\n",
      "Cross Entropy: [0.69489044 0.6662563  0.6907009  ... 0.73283297 0.7514073  0.62273544]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  11 / 500\n",
      "---------------\n",
      "Number of training episodes: 40\n",
      "Total reward of the batch: 1026.0\n",
      "Mean reward of the batch: 25.65\n",
      "Average reward of all training: 22.66206337134979\n",
      "Max reward for a batch so far: 1029.0\n",
      "Training Loss: 6.665538787841797\n",
      "Cross Entropy: [0.69704133 0.7299161  0.6313236  ... 0.7460835  0.62706214 0.7531385 ]\n",
      "=======================================================\n",
      "Epoch:  12 / 500\n",
      "---------------\n",
      "Number of training episodes: 37\n",
      "Total reward of the batch: 1031.0\n",
      "Mean reward of the batch: 27.864864864864863\n",
      "Average reward of all training: 23.095630162476045\n",
      "Max reward for a batch so far: 1031.0\n",
      "Training Loss: 6.879271507263184\n",
      "Cross Entropy: [0.694753   0.65837747 0.6927788  ... 0.8452958  0.8780424  0.5139066 ]\n",
      "=======================================================\n",
      "Epoch:  13 / 500\n",
      "---------------\n",
      "Number of training episodes: 43\n",
      "Total reward of the batch: 1038.0\n",
      "Mean reward of the batch: 24.13953488372093\n",
      "Average reward of all training: 23.17593052564873\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.292187213897705\n",
      "Cross Entropy: [0.6854859  0.70926905 0.7230189  ... 0.8872702  0.50745815 0.8878142 ]\n",
      "=======================================================\n",
      "Epoch:  14 / 500\n",
      "---------------\n",
      "Number of training episodes: 41\n",
      "Total reward of the batch: 1016.0\n",
      "Mean reward of the batch: 24.78048780487805\n",
      "Average reward of all training: 23.2905417598794\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.229061126708984\n",
      "Cross Entropy: [0.6934296  0.66089284 0.6906281  ... 0.70911294 0.73465276 0.75573885]\n",
      "=======================================================\n",
      "Epoch:  15 / 500\n",
      "---------------\n",
      "Number of training episodes: 38\n",
      "Total reward of the batch: 1016.0\n",
      "Mean reward of the batch: 26.736842105263158\n",
      "Average reward of all training: 23.52029511623832\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.555782318115234\n",
      "Cross Entropy: [0.69362295 0.6581496  0.69015574 ... 0.7678257  0.59606016 0.77302253]\n",
      "=======================================================\n",
      "Epoch:  16 / 500\n",
      "---------------\n",
      "Number of training episodes: 45\n",
      "Total reward of the batch: 1017.0\n",
      "Mean reward of the batch: 22.6\n",
      "Average reward of all training: 23.46277667147342\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.76528263092041\n",
      "Cross Entropy: [0.6906917  0.67224205 0.69307685 ... 0.6138734  0.6220731  0.63241553]\n",
      "=======================================================\n",
      "Epoch:  17 / 500\n",
      "---------------\n",
      "Number of training episodes: 39\n",
      "Total reward of the batch: 1018.0\n",
      "Mean reward of the batch: 26.102564102564102\n",
      "Average reward of all training: 23.61805828506699\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.307863712310791\n",
      "Cross Entropy: [0.69379663 0.73520774 0.76762295 ... 0.7634477  0.6105682  0.7731782 ]\n",
      "=======================================================\n",
      "Epoch:  18 / 500\n",
      "---------------\n",
      "Number of training episodes: 41\n",
      "Total reward of the batch: 1016.0\n",
      "Mean reward of the batch: 24.78048780487805\n",
      "Average reward of all training: 23.68263770283427\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.055316925048828\n",
      "Cross Entropy: [0.6842715  0.6740932  0.6853696  ... 0.5878134  0.78058815 0.58426726]\n",
      "=======================================================\n",
      "Epoch:  19 / 500\n",
      "---------------\n",
      "Number of training episodes: 36\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 28.083333333333332\n",
      "Average reward of all training: 23.914253262334217\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.634246349334717\n",
      "Cross Entropy: [0.685425   0.65707684 0.68590385 ... 0.8130146  0.8362756  0.8612692 ]\n",
      "=======================================================\n",
      "Epoch:  20 / 500\n",
      "---------------\n",
      "Number of training episodes: 33\n",
      "Total reward of the batch: 1004.0\n",
      "Mean reward of the batch: 30.424242424242426\n",
      "Average reward of all training: 24.239752720429628\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 6.900148868560791\n",
      "Cross Entropy: [0.6963409 0.73993   0.7743582 ... 0.54088   0.8376002 0.8769713]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  21 / 500\n",
      "---------------\n",
      "Number of training episodes: 45\n",
      "Total reward of the batch: 1005.0\n",
      "Mean reward of the batch: 22.333333333333332\n",
      "Average reward of all training: 24.148970844853615\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.426534175872803\n",
      "Cross Entropy: [0.7060473  0.6454519  0.70668477 ... 0.8181968  0.8575479  0.52308905]\n",
      "=======================================================\n",
      "Epoch:  22 / 500\n",
      "---------------\n",
      "Number of training episodes: 44\n",
      "Total reward of the batch: 1007.0\n",
      "Mean reward of the batch: 22.886363636363637\n",
      "Average reward of all training: 24.091579608104073\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.590728282928467\n",
      "Cross Entropy: [0.7010807  0.64456683 0.6866681  ... 0.7451462  0.6213706  0.63622767]\n",
      "=======================================================\n",
      "Epoch:  23 / 500\n",
      "---------------\n",
      "Number of training episodes: 44\n",
      "Total reward of the batch: 1035.0\n",
      "Mean reward of the batch: 23.522727272727273\n",
      "Average reward of all training: 24.0668468978703\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.570122718811035\n",
      "Cross Entropy: [0.69696426 0.6460481  0.70075923 ... 0.72614735 0.7432368  0.625655  ]\n",
      "=======================================================\n",
      "Epoch:  24 / 500\n",
      "---------------\n",
      "Number of training episodes: 40\n",
      "Total reward of the batch: 1028.0\n",
      "Mean reward of the batch: 25.7\n",
      "Average reward of all training: 24.13489494379237\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.751333713531494\n",
      "Cross Entropy: [0.69938934 0.6618552  0.6848484  ... 0.5637815  0.8301832  0.85759276]\n",
      "=======================================================\n",
      "Epoch:  25 / 500\n",
      "---------------\n",
      "Number of training episodes: 44\n",
      "Total reward of the batch: 1007.0\n",
      "Mean reward of the batch: 22.886363636363637\n",
      "Average reward of all training: 24.08495369149522\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.429040431976318\n",
      "Cross Entropy: [0.68643117 0.66034436 0.6931328  ... 0.59902996 0.611385   0.7654714 ]\n",
      "=======================================================\n",
      "Epoch:  26 / 500\n",
      "---------------\n",
      "Number of training episodes: 40\n",
      "Total reward of the batch: 1006.0\n",
      "Mean reward of the batch: 25.15\n",
      "Average reward of all training: 24.125917011053097\n",
      "Max reward for a batch so far: 1038.0\n",
      "Training Loss: 5.765693664550781\n",
      "Cross Entropy: [0.6972512  0.66430694 0.6943309  ... 0.9155732  0.96046317 0.45463118]\n",
      "=======================================================\n",
      "Epoch:  27 / 500\n",
      "---------------\n",
      "Number of training episodes: 39\n",
      "Total reward of the batch: 1043.0\n",
      "Mean reward of the batch: 26.743589743589745\n",
      "Average reward of all training: 24.222867852998895\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.881690502166748\n",
      "Cross Entropy: [0.69541705 0.64383113 0.6864965  ... 0.73453224 0.620864   0.64125645]\n",
      "=======================================================\n",
      "Epoch:  28 / 500\n",
      "---------------\n",
      "Number of training episodes: 40\n",
      "Total reward of the batch: 1033.0\n",
      "Mean reward of the batch: 25.825\n",
      "Average reward of all training: 24.28008685824894\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.745122909545898\n",
      "Cross Entropy: [0.67546606 0.6619644  0.6719275  ... 0.7818579  0.5772012  0.60487497]\n",
      "=======================================================\n",
      "Epoch:  29 / 500\n",
      "---------------\n",
      "Number of training episodes: 44\n",
      "Total reward of the batch: 1016.0\n",
      "Mean reward of the batch: 23.09090909090909\n",
      "Average reward of all training: 24.239080728340667\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.2151360511779785\n",
      "Cross Entropy: [0.7063952  0.7384975  0.6289726  ... 0.7789848  0.8225293  0.54392767]\n",
      "=======================================================\n",
      "Epoch:  30 / 500\n",
      "---------------\n",
      "Number of training episodes: 41\n",
      "Total reward of the batch: 1031.0\n",
      "Mean reward of the batch: 25.146341463414632\n",
      "Average reward of all training: 24.269322752843134\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.406142711639404\n",
      "Cross Entropy: [0.68296415 0.64272016 0.69999105 ... 0.5548605  0.56830597 0.5828754 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  31 / 500\n",
      "---------------\n",
      "Number of training episodes: 45\n",
      "Total reward of the batch: 1024.0\n",
      "Mean reward of the batch: 22.755555555555556\n",
      "Average reward of all training: 24.220491552930632\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.9639739990234375\n",
      "Cross Entropy: [0.69903713 0.73574203 0.630379   ... 0.57279605 0.7916173  0.56615174]\n",
      "=======================================================\n",
      "Epoch:  32 / 500\n",
      "---------------\n",
      "Number of training episodes: 42\n",
      "Total reward of the batch: 1028.0\n",
      "Mean reward of the batch: 24.476190476190474\n",
      "Average reward of all training: 24.228482144282502\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.9679646492004395\n",
      "Cross Entropy: [0.70210135 0.6317307  0.706601   ... 0.57964087 0.5953148  0.78014183]\n",
      "=======================================================\n",
      "Epoch:  33 / 500\n",
      "---------------\n",
      "Number of training episodes: 31\n",
      "Total reward of the batch: 1005.0\n",
      "Mean reward of the batch: 32.41935483870968\n",
      "Average reward of all training: 24.47669040774999\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 6.284162521362305\n",
      "Cross Entropy: [0.6827423  0.6493809  0.6994256  ... 0.82022977 0.85311496 0.88805676]\n",
      "=======================================================\n",
      "Epoch:  34 / 500\n",
      "---------------\n",
      "Number of training episodes: 40\n",
      "Total reward of the batch: 1010.0\n",
      "Mean reward of the batch: 25.25\n",
      "Average reward of all training: 24.49943480752205\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.060650825500488\n",
      "Cross Entropy: [0.6840979  0.6315732  0.70790035 ... 0.91846776 0.9737654  1.0314066 ]\n",
      "=======================================================\n",
      "Epoch:  35 / 500\n",
      "---------------\n",
      "Number of training episodes: 49\n",
      "Total reward of the batch: 1019.0\n",
      "Mean reward of the batch: 20.79591836734694\n",
      "Average reward of all training: 24.393620052088476\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.234164237976074\n",
      "Cross Entropy: [0.68791133 0.7462264  0.77518547 ... 0.59553814 0.7592355  0.8118835 ]\n",
      "=======================================================\n",
      "Epoch:  36 / 500\n",
      "---------------\n",
      "Number of training episodes: 43\n",
      "Total reward of the batch: 1014.0\n",
      "Mean reward of the batch: 23.58139534883721\n",
      "Average reward of all training: 24.371058254775942\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.787773132324219\n",
      "Cross Entropy: [0.70240176 0.6253108  0.6780564  ... 0.53953856 0.8511727  0.8909173 ]\n",
      "=======================================================\n",
      "Epoch:  37 / 500\n",
      "---------------\n",
      "Number of training episodes: 38\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 26.605263157894736\n",
      "Average reward of all training: 24.431442171076448\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.116061687469482\n",
      "Cross Entropy: [0.69129604 0.62947416 0.68754244 ... 0.71184736 0.77118903 0.57607293]\n",
      "=======================================================\n",
      "Epoch:  38 / 500\n",
      "---------------\n",
      "Number of training episodes: 39\n",
      "Total reward of the batch: 1006.0\n",
      "Mean reward of the batch: 25.794871794871796\n",
      "Average reward of all training: 24.46732189801843\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.959318161010742\n",
      "Cross Entropy: [0.7085989  0.7711351  0.5786651  ... 0.48799044 0.9001758  0.48227668]\n",
      "=======================================================\n",
      "Epoch:  39 / 500\n",
      "---------------\n",
      "Number of training episodes: 36\n",
      "Total reward of the batch: 1012.0\n",
      "Mean reward of the batch: 28.11111111111111\n",
      "Average reward of all training: 24.560752390661833\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.193223476409912\n",
      "Cross Entropy: [0.69589156 0.7659547  0.81784564 ... 0.9949403  1.0578141  0.39319214]\n",
      "=======================================================\n",
      "Epoch:  40 / 500\n",
      "---------------\n",
      "Number of training episodes: 38\n",
      "Total reward of the batch: 1006.0\n",
      "Mean reward of the batch: 26.473684210526315\n",
      "Average reward of all training: 24.608575686158446\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.742660045623779\n",
      "Cross Entropy: [0.7099085  0.6223352  0.67105514 ... 0.6129847  0.74326444 0.6038718 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  41 / 500\n",
      "---------------\n",
      "Number of training episodes: 32\n",
      "Total reward of the batch: 1024.0\n",
      "Mean reward of the batch: 32.0\n",
      "Average reward of all training: 24.78885432795946\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 5.334108829498291\n",
      "Cross Entropy: [0.65445304 0.7553625  0.81466585 ... 0.8163568  0.8795526  0.49180442]\n",
      "=======================================================\n",
      "Epoch:  42 / 500\n",
      "---------------\n",
      "Number of training episodes: 36\n",
      "Total reward of the batch: 1034.0\n",
      "Mean reward of the batch: 28.72222222222222\n",
      "Average reward of all training: 24.882505944489523\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.853793144226074\n",
      "Cross Entropy: [0.72072375 0.78626764 0.57583964 ... 0.7121324  0.6183479  0.73239505]\n",
      "=======================================================\n",
      "Epoch:  43 / 500\n",
      "---------------\n",
      "Number of training episodes: 36\n",
      "Total reward of the batch: 1010.0\n",
      "Mean reward of the batch: 28.055555555555557\n",
      "Average reward of all training: 24.956297795909666\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.8219218254089355\n",
      "Cross Entropy: [0.7192253  0.77984935 0.81991136 ... 1.1838152  0.3319253  0.36116123]\n",
      "=======================================================\n",
      "Epoch:  44 / 500\n",
      "---------------\n",
      "Number of training episodes: 41\n",
      "Total reward of the batch: 1003.0\n",
      "Mean reward of the batch: 24.463414634146343\n",
      "Average reward of all training: 24.945095905869593\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.05455207824707\n",
      "Cross Entropy: [0.6905508  0.608827   0.7076945  ... 0.50006425 0.53902173 0.5820111 ]\n",
      "=======================================================\n",
      "Epoch:  45 / 500\n",
      "---------------\n",
      "Number of training episodes: 38\n",
      "Total reward of the batch: 1004.0\n",
      "Mean reward of the batch: 26.42105263157895\n",
      "Average reward of all training: 24.97789494421869\n",
      "Max reward for a batch so far: 1043.0\n",
      "Training Loss: 4.197473526000977\n",
      "Cross Entropy: [0.7116972  0.60590386 0.67758536 ... 0.66851866 0.6485473  0.6850471 ]\n",
      "=======================================================\n",
      "Epoch:  46 / 500\n",
      "---------------\n",
      "Number of training episodes: 29\n",
      "Total reward of the batch: 1056.0\n",
      "Mean reward of the batch: 36.41379310344828\n",
      "Average reward of all training: 25.226501425941073\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 5.547106742858887\n",
      "Cross Entropy: [0.68838894 0.6122737  0.70775396 ... 0.4141366  0.4357661  0.45744598]\n",
      "=======================================================\n",
      "Epoch:  47 / 500\n",
      "---------------\n",
      "Number of training episodes: 32\n",
      "Total reward of the batch: 1002.0\n",
      "Mean reward of the batch: 31.3125\n",
      "Average reward of all training: 25.355990757304028\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 4.656581401824951\n",
      "Cross Entropy: [0.68808395 0.597574   0.6886229  ... 0.9416334  1.0052276  0.41845572]\n",
      "=======================================================\n",
      "Epoch:  48 / 500\n",
      "---------------\n",
      "Number of training episodes: 34\n",
      "Total reward of the batch: 1004.0\n",
      "Mean reward of the batch: 29.529411764705884\n",
      "Average reward of all training: 25.442937028291567\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 4.219108581542969\n",
      "Cross Entropy: [0.67064136 0.5975794  0.67475337 ... 1.0115657  0.40400013 0.44271645]\n",
      "=======================================================\n",
      "Epoch:  49 / 500\n",
      "---------------\n",
      "Number of training episodes: 34\n",
      "Total reward of the batch: 1009.0\n",
      "Mean reward of the batch: 29.676470588235293\n",
      "Average reward of all training: 25.529335672372053\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 4.471002578735352\n",
      "Cross Entropy: [0.6988406  0.58388233 0.6903555  ... 1.0863826  0.37583253 0.3983886 ]\n",
      "=======================================================\n",
      "Epoch:  50 / 500\n",
      "---------------\n",
      "Number of training episodes: 26\n",
      "Total reward of the batch: 1010.0\n",
      "Mean reward of the batch: 38.84615384615385\n",
      "Average reward of all training: 25.795672035847687\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 5.250821113586426\n",
      "Cross Entropy: [0.6926978  0.82399374 0.51373994 ... 0.9057926  0.9781327  0.4289469 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  51 / 500\n",
      "---------------\n",
      "Number of training episodes: 35\n",
      "Total reward of the batch: 1014.0\n",
      "Mean reward of the batch: 28.97142857142857\n",
      "Average reward of all training: 25.857941771839467\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 3.9211108684539795\n",
      "Cross Entropy: [0.72620267 0.57708764 0.6667415  ... 0.48957545 0.8844538  0.97117674]\n",
      "=======================================================\n",
      "Epoch:  52 / 500\n",
      "---------------\n",
      "Number of training episodes: 36\n",
      "Total reward of the batch: 1041.0\n",
      "Mean reward of the batch: 28.916666666666668\n",
      "Average reward of all training: 25.916763404432302\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 3.685529947280884\n",
      "Cross Entropy: [0.6782404  0.5796466  0.6826981  ... 1.0062232  1.0902746  0.36732227]\n",
      "=======================================================\n",
      "Epoch:  53 / 500\n",
      "---------------\n",
      "Number of training episodes: 29\n",
      "Total reward of the batch: 1029.0\n",
      "Mean reward of the batch: 35.48275862068966\n",
      "Average reward of all training: 26.097253880210744\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 4.490756988525391\n",
      "Cross Entropy: [0.6701615  0.81112033 0.53172547 ... 0.4009972  0.43122566 0.4638557 ]\n",
      "=======================================================\n",
      "Epoch:  54 / 500\n",
      "---------------\n",
      "Number of training episodes: 33\n",
      "Total reward of the batch: 1038.0\n",
      "Mean reward of the batch: 31.454545454545453\n",
      "Average reward of all training: 26.196462983439165\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 4.058974742889404\n",
      "Cross Entropy: [0.7187432  0.8566125  0.9567477  ... 0.82733524 0.9159709  0.45368233]\n",
      "=======================================================\n",
      "Epoch:  55 / 500\n",
      "---------------\n",
      "Number of training episodes: 37\n",
      "Total reward of the batch: 1023.0\n",
      "Mean reward of the batch: 27.64864864864865\n",
      "Average reward of all training: 26.222866359170247\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 3.1821327209472656\n",
      "Cross Entropy: [0.7081197  0.56097937 0.67491996 ... 0.5164196  0.5695121  0.75056887]\n",
      "=======================================================\n",
      "Epoch:  56 / 500\n",
      "---------------\n",
      "Number of training episodes: 30\n",
      "Total reward of the batch: 1025.0\n",
      "Mean reward of the batch: 34.166666666666664\n",
      "Average reward of all training: 26.364719936089816\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 3.9342904090881348\n",
      "Cross Entropy: [0.69887054 0.54579365 0.68614554 ... 0.5604252  0.7387382  0.5355964 ]\n",
      "=======================================================\n",
      "Epoch:  57 / 500\n",
      "---------------\n",
      "Number of training episodes: 26\n",
      "Total reward of the batch: 1008.0\n",
      "Mean reward of the batch: 38.76923076923077\n",
      "Average reward of all training: 26.582342933162465\n",
      "Max reward for a batch so far: 1056.0\n",
      "Training Loss: 4.476321220397949\n",
      "Cross Entropy: [0.71188235 0.5576868  0.6939759  ... 0.6886288  0.5565791  0.65252477]\n",
      "=======================================================\n",
      "Epoch:  58 / 500\n",
      "---------------\n",
      "Number of training episodes: 29\n",
      "Total reward of the batch: 1057.0\n",
      "Mean reward of the batch: 36.44827586206897\n",
      "Average reward of all training: 26.752445225040166\n",
      "Max reward for a batch so far: 1057.0\n",
      "Training Loss: 4.125987529754639\n",
      "Cross Entropy: [0.7198112  0.53571546 0.73853815 ... 1.2257693  0.30260232 0.33090457]\n",
      "=======================================================\n",
      "Epoch:  59 / 500\n",
      "---------------\n",
      "Number of training episodes: 31\n",
      "Total reward of the batch: 1041.0\n",
      "Mean reward of the batch: 33.58064516129032\n",
      "Average reward of all training: 26.868177427349487\n",
      "Max reward for a batch so far: 1057.0\n",
      "Training Loss: 3.472064256668091\n",
      "Cross Entropy: [0.7276836 0.9066969 0.4381869 ... 1.1642842 0.3130502 0.3591678]\n",
      "=======================================================\n",
      "Epoch:  60 / 500\n",
      "---------------\n",
      "Number of training episodes: 25\n",
      "Total reward of the batch: 1021.0\n",
      "Mean reward of the batch: 40.84\n",
      "Average reward of all training: 27.101041136893663\n",
      "Max reward for a batch so far: 1057.0\n",
      "Training Loss: 4.024025917053223\n",
      "Cross Entropy: [0.6484054  0.8550089  0.48125803 ... 0.5969835  0.67599976 0.56092095]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  61 / 500\n",
      "---------------\n",
      "Number of training episodes: 29\n",
      "Total reward of the batch: 1042.0\n",
      "Mean reward of the batch: 35.93103448275862\n",
      "Average reward of all training: 27.245795126170137\n",
      "Max reward for a batch so far: 1057.0\n",
      "Training Loss: 3.5154237747192383\n",
      "Cross Entropy: [0.71089697 0.5119717  0.66792214 ... 0.62761736 0.6181439  0.7207467 ]\n",
      "=======================================================\n",
      "Epoch:  62 / 500\n",
      "---------------\n",
      "Number of training episodes: 26\n",
      "Total reward of the batch: 1043.0\n",
      "Mean reward of the batch: 40.11538461538461\n",
      "Average reward of all training: 27.453369150189722\n",
      "Max reward for a batch so far: 1057.0\n",
      "Training Loss: 3.7808432579040527\n",
      "Cross Entropy: [0.72192174 0.503292   0.7254358  ... 0.7815492  0.4902014  0.8232627 ]\n",
      "=======================================================\n",
      "Epoch:  63 / 500\n",
      "---------------\n",
      "Number of training episodes: 26\n",
      "Total reward of the batch: 1059.0\n",
      "Mean reward of the batch: 40.73076923076923\n",
      "Average reward of all training: 27.664121532421145\n",
      "Max reward for a batch so far: 1059.0\n",
      "Training Loss: 3.8306972980499268\n",
      "Cross Entropy: [0.6695212  0.89113986 0.44870773 ... 0.5294385  0.75847924 0.93015605]\n",
      "=======================================================\n",
      "Epoch:  64 / 500\n",
      "---------------\n",
      "Number of training episodes: 25\n",
      "Total reward of the batch: 1043.0\n",
      "Mean reward of the batch: 41.72\n",
      "Average reward of all training: 27.883744633477065\n",
      "Max reward for a batch so far: 1059.0\n",
      "Training Loss: 3.603954792022705\n",
      "Cross Entropy: [0.6751949  0.5295972  0.66379446 ... 0.97462285 1.1758509  0.29146427]\n",
      "=======================================================\n",
      "Epoch:  65 / 500\n",
      "---------------\n",
      "Number of training episodes: 24\n",
      "Total reward of the batch: 1016.0\n",
      "Mean reward of the batch: 42.333333333333336\n",
      "Average reward of all training: 28.106045998090238\n",
      "Max reward for a batch so far: 1059.0\n",
      "Training Loss: 3.5558509826660156\n",
      "Cross Entropy: [0.66350216 0.5113953  0.6545738  ... 1.0856559  0.32768607 0.3876593 ]\n",
      "=======================================================\n",
      "Epoch:  66 / 500\n",
      "---------------\n",
      "Number of training episodes: 26\n",
      "Total reward of the batch: 1072.0\n",
      "Mean reward of the batch: 41.23076923076923\n",
      "Average reward of all training: 28.304905441009616\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 3.2804830074310303\n",
      "Cross Entropy: [0.6909182  0.5017196  0.7154784  ... 1.0290779  0.3416622  0.41396058]\n",
      "=======================================================\n",
      "Epoch:  67 / 500\n",
      "---------------\n",
      "Number of training episodes: 28\n",
      "Total reward of the batch: 1021.0\n",
      "Mean reward of the batch: 36.464285714285715\n",
      "Average reward of all training: 28.42668723613314\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.7510735988616943\n",
      "Cross Entropy: [0.70623904 0.9622389  1.1511637  ... 0.699112   0.49462694 0.62269187]\n",
      "=======================================================\n",
      "Epoch:  68 / 500\n",
      "---------------\n",
      "Number of training episodes: 22\n",
      "Total reward of the batch: 1048.0\n",
      "Mean reward of the batch: 47.63636363636363\n",
      "Average reward of all training: 28.709182477313004\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 3.60943341255188\n",
      "Cross Entropy: [0.6806183  0.9531267  0.37639353 ... 0.49173337 0.78422135 0.44970775]\n",
      "=======================================================\n",
      "Epoch:  69 / 500\n",
      "---------------\n",
      "Number of training episodes: 22\n",
      "Total reward of the batch: 1050.0\n",
      "Mean reward of the batch: 47.72727272727273\n",
      "Average reward of all training: 28.98480697368923\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 3.3269107341766357\n",
      "Cross Entropy: [0.71872765 0.4627036  0.7270466  ... 1.0066376  1.2439911  0.25730634]\n",
      "=======================================================\n",
      "Epoch:  70 / 500\n",
      "---------------\n",
      "Number of training episodes: 25\n",
      "Total reward of the batch: 1005.0\n",
      "Mean reward of the batch: 40.2\n",
      "Average reward of all training: 29.145024016922243\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.6204092502593994\n",
      "Cross Entropy: [0.69511205 0.49083555 0.7047419  ... 0.97643477 1.2062684  0.27336955]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  71 / 500\n",
      "---------------\n",
      "Number of training episodes: 24\n",
      "Total reward of the batch: 1038.0\n",
      "Mean reward of the batch: 43.25\n",
      "Average reward of all training: 29.343685650486716\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.935650110244751\n",
      "Cross Entropy: [0.67037624 0.4740116  0.6788019  ... 0.9697392  0.36342782 0.43978238]\n",
      "=======================================================\n",
      "Epoch:  72 / 500\n",
      "---------------\n",
      "Number of training episodes: 22\n",
      "Total reward of the batch: 1061.0\n",
      "Mean reward of the batch: 48.22727272727273\n",
      "Average reward of all training: 29.605957693219857\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.9706430435180664\n",
      "Cross Entropy: [0.69723415 0.9658259  0.3834358  ... 0.40432832 0.91277516 0.37431166]\n",
      "=======================================================\n",
      "Epoch:  73 / 500\n",
      "---------------\n",
      "Number of training episodes: 21\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 48.142857142857146\n",
      "Average reward of all training: 29.859887822666945\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.866718292236328\n",
      "Cross Entropy: [0.6987481  0.98239917 1.177952   ... 0.53493595 0.72135407 0.49038035]\n",
      "=======================================================\n",
      "Epoch:  74 / 500\n",
      "---------------\n",
      "Number of training episodes: 22\n",
      "Total reward of the batch: 1023.0\n",
      "Mean reward of the batch: 46.5\n",
      "Average reward of all training: 30.08475420344172\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.741798162460327\n",
      "Cross Entropy: [0.67890805 0.45871723 0.6936739  ... 0.228886   1.3946335  1.6572233 ]\n",
      "=======================================================\n",
      "Epoch:  75 / 500\n",
      "---------------\n",
      "Number of training episodes: 21\n",
      "Total reward of the batch: 1018.0\n",
      "Mean reward of the batch: 48.476190476190474\n",
      "Average reward of all training: 30.329973353745032\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.7137928009033203\n",
      "Cross Entropy: [0.66206324 0.46985394 0.6503739  ... 0.23173235 0.28853297 0.35979   ]\n",
      "=======================================================\n",
      "Epoch:  76 / 500\n",
      "---------------\n",
      "Number of training episodes: 21\n",
      "Total reward of the batch: 1059.0\n",
      "Mean reward of the batch: 50.42857142857143\n",
      "Average reward of all training: 30.5944285915717\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.5998988151550293\n",
      "Cross Entropy: [0.7093467  0.44722354 0.6981276  ... 1.3718543  0.21602716 1.4465978 ]\n",
      "=======================================================\n",
      "Epoch:  77 / 500\n",
      "---------------\n",
      "Number of training episodes: 21\n",
      "Total reward of the batch: 1021.0\n",
      "Mean reward of the batch: 48.61904761904762\n",
      "Average reward of all training: 30.82851455296749\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 2.363887071609497\n",
      "Cross Entropy: [0.6840478  1.0024695  0.3422968  ... 0.7882247  1.1303744  0.27684298]\n",
      "=======================================================\n",
      "Epoch:  78 / 500\n",
      "---------------\n",
      "Number of training episodes: 15\n",
      "Total reward of the batch: 1001.0\n",
      "Mean reward of the batch: 66.73333333333333\n",
      "Average reward of all training: 31.288832742459356\n",
      "Max reward for a batch so far: 1072.0\n",
      "Training Loss: 3.181305408477783\n",
      "Cross Entropy: [0.74791324 0.41640317 0.63152003 ... 0.83482254 0.39499685 0.51170266]\n",
      "=======================================================\n",
      "Epoch:  79 / 500\n",
      "---------------\n",
      "Number of training episodes: 13\n",
      "Total reward of the batch: 1085.0\n",
      "Mean reward of the batch: 83.46153846153847\n",
      "Average reward of all training: 31.949246738903398\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 3.5559916496276855\n",
      "Cross Entropy: [0.736085   0.41723984 0.7385504  ... 0.2800499  0.36588976 0.48374587]\n",
      "=======================================================\n",
      "Epoch:  80 / 500\n",
      "---------------\n",
      "Number of training episodes: 17\n",
      "Total reward of the batch: 1009.0\n",
      "Mean reward of the batch: 59.35294117647059\n",
      "Average reward of all training: 32.29179291937298\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.510223150253296\n",
      "Cross Entropy: [0.6993009  1.0859622  0.29298174 ... 0.8049225  0.3890676  0.89275634]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  81 / 500\n",
      "---------------\n",
      "Number of training episodes: 20\n",
      "Total reward of the batch: 1013.0\n",
      "Mean reward of the batch: 50.65\n",
      "Average reward of all training: 32.51843745123258\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 1.7155712842941284\n",
      "Cross Entropy: [0.62797266 0.47200978 0.6213734  ... 0.19480808 0.24143541 0.300221  ]\n",
      "=======================================================\n",
      "Epoch:  82 / 500\n",
      "---------------\n",
      "Number of training episodes: 19\n",
      "Total reward of the batch: 1023.0\n",
      "Mean reward of the batch: 53.8421052631579\n",
      "Average reward of all training: 32.7784821806463\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.0230371952056885\n",
      "Cross Entropy: [0.7371881  0.3921861  0.6372268  ... 0.33744976 0.9892718  0.30063525]\n",
      "=======================================================\n",
      "Epoch:  83 / 500\n",
      "---------------\n",
      "Number of training episodes: 13\n",
      "Total reward of the batch: 1043.0\n",
      "Mean reward of the batch: 80.23076923076923\n",
      "Average reward of all training: 33.35019648245501\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.884798049926758\n",
      "Cross Entropy: [0.62644506 0.44736776 0.7496939  ... 0.7799344  0.3918947  0.8887154 ]\n",
      "=======================================================\n",
      "Epoch:  84 / 500\n",
      "---------------\n",
      "Number of training episodes: 16\n",
      "Total reward of the batch: 1056.0\n",
      "Mean reward of the batch: 66.0\n",
      "Average reward of all training: 33.73888461956864\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.13496994972229\n",
      "Cross Entropy: [0.6633837  1.0124136  0.32608756 ... 0.75332516 0.49247393 0.71667385]\n",
      "=======================================================\n",
      "Epoch:  85 / 500\n",
      "---------------\n",
      "Number of training episodes: 14\n",
      "Total reward of the batch: 1003.0\n",
      "Mean reward of the batch: 71.64285714285714\n",
      "Average reward of all training: 34.18481370807792\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.2993509769439697\n",
      "Cross Entropy: [0.7626649  0.37592322 0.62283903 ... 1.5207     0.16177322 0.21647963]\n",
      "=======================================================\n",
      "Epoch:  86 / 500\n",
      "---------------\n",
      "Number of training episodes: 11\n",
      "Total reward of the batch: 1066.0\n",
      "Mean reward of the batch: 96.9090909090909\n",
      "Average reward of all training: 34.91416576855482\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.7408499717712402\n",
      "Cross Entropy: [0.7165326  1.0809686  0.2965283  ... 0.27562344 1.174223   0.23891382]\n",
      "=======================================================\n",
      "Epoch:  87 / 500\n",
      "---------------\n",
      "Number of training episodes: 13\n",
      "Total reward of the batch: 1013.0\n",
      "Mean reward of the batch: 77.92307692307692\n",
      "Average reward of all training: 35.40852106918151\n",
      "Max reward for a batch so far: 1085.0\n",
      "Training Loss: 2.2272274494171143\n",
      "Cross Entropy: [0.7013205  0.39100468 0.73195493 ... 0.13096227 0.16360897 0.20452057]\n",
      "=======================================================\n",
      "Epoch:  88 / 500\n",
      "---------------\n",
      "Number of training episodes: 12\n",
      "Total reward of the batch: 1153.0\n",
      "Mean reward of the batch: 96.08333333333333\n",
      "Average reward of all training: 36.09800757218323\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 2.346667528152466\n",
      "Cross Entropy: [0.6056485  0.46699393 0.58758485 ... 0.66466606 0.4943443  0.6503058 ]\n",
      "=======================================================\n",
      "Epoch:  89 / 500\n",
      "---------------\n",
      "Number of training episodes: 12\n",
      "Total reward of the batch: 1032.0\n",
      "Mean reward of the batch: 86.0\n",
      "Average reward of all training: 36.658704116316\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.945364236831665\n",
      "Cross Entropy: [0.671388   1.1017715  0.2683152  ... 1.1439686  0.24583732 0.3428064 ]\n",
      "=======================================================\n",
      "Epoch:  90 / 500\n",
      "---------------\n",
      "Number of training episodes: 13\n",
      "Total reward of the batch: 1108.0\n",
      "Mean reward of the batch: 85.23076923076923\n",
      "Average reward of all training: 37.19839372869881\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.8186378479003906\n",
      "Cross Entropy: [0.7026726  1.1275958  0.24748646 ... 0.9013957  1.4078472  0.17717223]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  91 / 500\n",
      "---------------\n",
      "Number of training episodes: 12\n",
      "Total reward of the batch: 1066.0\n",
      "Mean reward of the batch: 88.83333333333333\n",
      "Average reward of all training: 37.76581064743106\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.7386714220046997\n",
      "Cross Entropy: [0.7416815  0.3675181  0.63757265 ... 0.14343263 1.7091271  0.12185951]\n",
      "=======================================================\n",
      "Epoch:  92 / 500\n",
      "---------------\n",
      "Number of training episodes: 10\n",
      "Total reward of the batch: 1024.0\n",
      "Mean reward of the batch: 102.4\n",
      "Average reward of all training: 38.46835618387203\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.8569170236587524\n",
      "Cross Entropy: [0.68839914 1.1182389  0.26125762 ... 1.2401308  0.19158936 0.27601048]\n",
      "=======================================================\n",
      "Epoch:  93 / 500\n",
      "---------------\n",
      "Number of training episodes: 11\n",
      "Total reward of the batch: 1094.0\n",
      "Mean reward of the batch: 99.45454545454545\n",
      "Average reward of all training: 39.12412165990078\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.6141656637191772\n",
      "Cross Entropy: [0.6339724  1.080572   0.2670269  ... 1.6018018  0.1291325  0.18441898]\n",
      "=======================================================\n",
      "Epoch:  94 / 500\n",
      "---------------\n",
      "Number of training episodes: 12\n",
      "Total reward of the batch: 1073.0\n",
      "Mean reward of the batch: 89.41666666666667\n",
      "Average reward of all training: 39.65914873444084\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.4182394742965698\n",
      "Cross Entropy: [0.7607108  0.34345245 0.6140892  ... 2.3339226  0.0625474  0.08625516]\n",
      "=======================================================\n",
      "Epoch:  95 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1044.0\n",
      "Mean reward of the batch: 149.14285714285714\n",
      "Average reward of all training: 40.81160882295049\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 2.3379929065704346\n",
      "Cross Entropy: [0.6355422  0.39677534 0.7185612  ... 1.5591255  0.13540056 0.18997765]\n",
      "=======================================================\n",
      "Epoch:  96 / 500\n",
      "---------------\n",
      "Number of training episodes: 9\n",
      "Total reward of the batch: 1099.0\n",
      "Mean reward of the batch: 122.11111111111111\n",
      "Average reward of all training: 41.65847863845216\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.6204395294189453\n",
      "Cross Entropy: [0.6360247  0.4056366  0.6171994  ... 0.41053864 0.5816219  0.7953766 ]\n",
      "=======================================================\n",
      "Epoch:  97 / 500\n",
      "---------------\n",
      "Number of training episodes: 10\n",
      "Total reward of the batch: 1075.0\n",
      "Mean reward of the batch: 107.5\n",
      "Average reward of all training: 42.337257209189765\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.3309333324432373\n",
      "Cross Entropy: [0.719399   1.2617501  0.19832113 ... 0.97517455 0.25480187 1.1882335 ]\n",
      "=======================================================\n",
      "Epoch:  98 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1077.0\n",
      "Mean reward of the batch: 134.625\n",
      "Average reward of all training: 43.27896887032048\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.558346152305603\n",
      "Cross Entropy: [0.6636875  0.34052417 0.66459244 ... 0.5396513  0.6919828  0.87301767]\n",
      "=======================================================\n",
      "Epoch:  99 / 500\n",
      "---------------\n",
      "Number of training episodes: 9\n",
      "Total reward of the batch: 1040.0\n",
      "Mean reward of the batch: 115.55555555555556\n",
      "Average reward of all training: 44.009035402494575\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.8170455098152161\n",
      "Cross Entropy: [0.7316951  1.336025   0.17360699 ... 0.8320733  1.1792156  0.223274  ]\n",
      "=======================================================\n",
      "Epoch:  100 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1100.0\n",
      "Mean reward of the batch: 137.5\n",
      "Average reward of all training: 44.94394504846963\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.1547176837921143\n",
      "Cross Entropy: [0.6437473  0.34610724 0.6421656  ... 1.3136468  1.8528274  0.0884253 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  101 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 144.42857142857142\n",
      "Average reward of all training: 45.92894134926272\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.2952831983566284\n",
      "Cross Entropy: [0.56592715 0.397891   0.8948425  ... 1.0113246  0.23058838 0.36015263]\n",
      "=======================================================\n",
      "Epoch:  102 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1126.0\n",
      "Mean reward of the batch: 160.85714285714286\n",
      "Average reward of all training: 47.05568842286939\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.3694672584533691\n",
      "Cross Entropy: [0.62562495 0.35011932 0.64126766 ... 1.645061   0.11083677 0.16414458]\n",
      "=======================================================\n",
      "Epoch:  103 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1052.0\n",
      "Mean reward of the batch: 150.28571428571428\n",
      "Average reward of all training: 48.05792168367371\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.1243696212768555\n",
      "Cross Entropy: [0.6521298  1.2443504  0.17901094 ... 1.7590055  2.454412   0.04529208]\n",
      "=======================================================\n",
      "Epoch:  104 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1050.0\n",
      "Mean reward of the batch: 175.0\n",
      "Average reward of all training: 49.27851859056145\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.1617226600646973\n",
      "Cross Entropy: [0.6144193  0.36284223 0.8249141  ... 1.3600317  1.9305342  0.07752584]\n",
      "=======================================================\n",
      "Epoch:  105 / 500\n",
      "---------------\n",
      "Number of training episodes: 10\n",
      "Total reward of the batch: 1071.0\n",
      "Mean reward of the batch: 107.1\n",
      "Average reward of all training: 49.82919936588944\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.239328533411026\n",
      "Cross Entropy: [0.57884336 0.37959057 0.5564633  ... 0.5107436  0.64663947 1.009429  ]\n",
      "=======================================================\n",
      "Epoch:  106 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1054.0\n",
      "Mean reward of the batch: 175.66666666666666\n",
      "Average reward of all training: 51.0163452838213\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 1.1485294103622437\n",
      "Cross Entropy: [0.73963064 1.4228097  0.14189288 ... 0.38085976 0.6578475  0.43870547]\n",
      "=======================================================\n",
      "Epoch:  107 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1152.0\n",
      "Mean reward of the batch: 164.57142857142858\n",
      "Average reward of all training: 52.07760774445315\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.9416686296463013\n",
      "Cross Entropy: [0.6507401  0.3317935  0.74535835 ... 1.2561688  1.8899329  0.07626159]\n",
      "=======================================================\n",
      "Epoch:  108 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1077.0\n",
      "Mean reward of the batch: 153.85714285714286\n",
      "Average reward of all training: 53.02001084734842\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.5344063639640808\n",
      "Cross Entropy: [0.84085804 1.5396353  2.198148   ... 0.83834815 0.33744714 0.5110767 ]\n",
      "=======================================================\n",
      "Epoch:  109 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1062.0\n",
      "Mean reward of the batch: 151.71428571428572\n",
      "Average reward of all training: 53.925462910347846\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.3895150125026703\n",
      "Cross Entropy: [0.70736116 0.31352934 0.68928444 ... 1.3238665  0.19803749 1.2395442 ]\n",
      "=======================================================\n",
      "Epoch:  110 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1031.0\n",
      "Mean reward of the batch: 147.28571428571428\n",
      "Average reward of all training: 54.77419246830572\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.3143366277217865\n",
      "Cross Entropy: [0.671984   0.33474874 0.62232786 ... 1.8436364  0.08371045 2.1581814 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  111 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1146.0\n",
      "Mean reward of the batch: 163.71428571428572\n",
      "Average reward of all training: 55.75563474980103\n",
      "Max reward for a batch so far: 1153.0\n",
      "Training Loss: 0.4804474413394928\n",
      "Cross Entropy: [0.6327578  0.3095555  0.6806771  ... 0.6149089  0.40435982 0.6373373 ]\n",
      "=======================================================\n",
      "Epoch:  112 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1177.0\n",
      "Mean reward of the batch: 168.14285714285714\n",
      "Average reward of all training: 56.75909209259618\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.5666781067848206\n",
      "Cross Entropy: [0.64256966 0.314807   0.8051181  ... 1.1490966  0.20125982 0.4359882 ]\n",
      "=======================================================\n",
      "Epoch:  113 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1173.0\n",
      "Mean reward of the batch: 167.57142857142858\n",
      "Average reward of all training: 57.73973223842655\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.4971264600753784\n",
      "Cross Entropy: [0.7944945 0.2404282 0.7975297 ... 0.1948741 0.3683762 0.6574172]\n",
      "=======================================================\n",
      "Epoch:  114 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1102.0\n",
      "Mean reward of the batch: 183.66666666666666\n",
      "Average reward of all training: 58.84435447025322\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.44101694226264954\n",
      "Cross Entropy: [0.6280046  0.332778   0.5894572  ... 0.78384453 0.30184576 0.51105595]\n",
      "=======================================================\n",
      "Epoch:  115 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1103.0\n",
      "Mean reward of the batch: 183.83333333333334\n",
      "Average reward of all training: 59.931215156019135\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.4994174838066101\n",
      "Cross Entropy: [0.83054686 0.21989694 0.52803844 ... 1.3407564  0.1488589  0.2904365 ]\n",
      "=======================================================\n",
      "Epoch:  116 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1157.0\n",
      "Mean reward of the batch: 144.625\n",
      "Average reward of all training: 60.66133399088103\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.01454260852187872\n",
      "Cross Entropy: [0.9152264  1.7588202  0.08918831 ... 1.747608   0.09125026 0.13989139]\n",
      "=======================================================\n",
      "Epoch:  117 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1088.0\n",
      "Mean reward of the batch: 181.33333333333334\n",
      "Average reward of all training: 61.692718600645584\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.2790137529373169\n",
      "Cross Entropy: [0.7418412  1.483651   0.1162959  ... 0.75192523 0.3546236  0.775077  ]\n",
      "=======================================================\n",
      "Epoch:  118 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1131.0\n",
      "Mean reward of the batch: 188.5\n",
      "Average reward of all training: 62.76735657860621\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.33867478370666504\n",
      "Cross Entropy: [0.6100934  0.28917345 0.723405   ... 1.0793991  0.2117884  0.40765157]\n",
      "=======================================================\n",
      "Epoch:  119 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1154.0\n",
      "Mean reward of the batch: 164.85714285714286\n",
      "Average reward of all training: 63.6252539422914\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.0880974680185318\n",
      "Cross Entropy: [0.6728423  0.2732181  0.7339405  ... 1.4463222  0.11480218 0.1961779 ]\n",
      "=======================================================\n",
      "Epoch:  120 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1081.0\n",
      "Mean reward of the batch: 154.42857142857142\n",
      "Average reward of all training: 64.38194825467707\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.2267695516347885\n",
      "Cross Entropy: [0.64633155 1.4077607  0.12734145 ... 0.0652854  2.191143   0.04818503]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  121 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1139.0\n",
      "Mean reward of the batch: 189.83333333333334\n",
      "Average reward of all training: 65.41873656111224\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.23289558291435242\n",
      "Cross Entropy: [0.891019   0.19068442 0.5184042  ... 0.33838513 0.7341326  0.30806428]\n",
      "=======================================================\n",
      "Epoch:  122 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1042.0\n",
      "Mean reward of the batch: 173.66666666666666\n",
      "Average reward of all training: 66.30601467673155\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: -0.01226747315376997\n",
      "Cross Entropy: [0.8455296  1.6713197  0.0900635  ... 0.39579007 0.5654059  0.35981864]\n",
      "=======================================================\n",
      "Epoch:  123 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1022.0\n",
      "Mean reward of the batch: 170.33333333333334\n",
      "Average reward of all training: 67.1517652349153\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.19900807738304138\n",
      "Cross Entropy: [0.6936251  0.27018243 0.6882702  ... 0.9054216  0.2900881  0.55868375]\n",
      "=======================================================\n",
      "Epoch:  124 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1090.0\n",
      "Mean reward of the batch: 181.66666666666666\n",
      "Average reward of all training: 68.0752725045262\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.03826524317264557\n",
      "Cross Entropy: [0.6294942  0.2722343  0.68151367 ... 0.4940005  0.44367513 0.86834955]\n",
      "=======================================================\n",
      "Epoch:  125 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1021.0\n",
      "Mean reward of the batch: 170.16666666666666\n",
      "Average reward of all training: 68.89200365782331\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.08686012774705887\n",
      "Cross Entropy: [0.7339577  0.22799702 0.6171692  ... 0.08103459 0.1466952  0.26002994]\n",
      "=======================================================\n",
      "Epoch:  126 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1025.0\n",
      "Mean reward of the batch: 170.83333333333334\n",
      "Average reward of all training: 69.70106182985117\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.15982797741889954\n",
      "Cross Entropy: [0.64327985 1.4566073  0.10642708 ... 0.67092085 0.3372199  0.5950601 ]\n",
      "=======================================================\n",
      "Epoch:  127 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1165.0\n",
      "Mean reward of the batch: 194.16666666666666\n",
      "Average reward of all training: 70.68110596242452\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.09900619089603424\n",
      "Cross Entropy: [0.60432297 0.29004815 0.5666528  ... 0.45941076 1.1226091  0.15541309]\n",
      "=======================================================\n",
      "Epoch:  128 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1137.0\n",
      "Mean reward of the batch: 189.5\n",
      "Average reward of all training: 71.60937857209308\n",
      "Max reward for a batch so far: 1177.0\n",
      "Training Loss: 0.03981100395321846\n",
      "Cross Entropy: [0.7491548  0.22306201 0.758011   ... 0.5950364  0.43760297 0.81500214]\n",
      "=======================================================\n",
      "Epoch:  129 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1195.0\n",
      "Mean reward of the batch: 170.71428571428572\n",
      "Average reward of all training: 72.3776336662186\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: 0.028930524364113808\n",
      "Cross Entropy: [0.79726005 1.6772307  0.08378456 ... 0.32391635 0.65007174 0.36575872]\n",
      "=======================================================\n",
      "Epoch:  130 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1144.0\n",
      "Mean reward of the batch: 190.66666666666666\n",
      "Average reward of all training: 73.2875493046836\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: 0.08904080837965012\n",
      "Cross Entropy: [0.6845748  1.5509195  0.09571738 ... 1.6039968  0.10275274 0.20218328]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  131 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1189.0\n",
      "Mean reward of the batch: 198.16666666666666\n",
      "Average reward of all training: 74.24082500973691\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: 0.05491777881979942\n",
      "Cross Entropy: [0.65443474 0.25444922 0.6592638  ... 0.27236307 0.64448875 0.39223966]\n",
      "=======================================================\n",
      "Epoch:  132 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1139.0\n",
      "Mean reward of the batch: 162.71428571428572\n",
      "Average reward of all training: 74.91107849992287\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: -0.13792996108531952\n",
      "Cross Entropy: [0.7821963  1.7407657  0.07846971 ... 0.13173896 1.3972375  0.08973357]\n",
      "=======================================================\n",
      "Epoch:  133 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1127.0\n",
      "Mean reward of the batch: 187.83333333333334\n",
      "Average reward of all training: 75.76011800994853\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: -0.02583019807934761\n",
      "Cross Entropy: [0.6057025  0.28659156 0.8214451  ... 0.11515072 0.2667746  0.80730486]\n",
      "=======================================================\n",
      "Epoch:  134 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1145.0\n",
      "Mean reward of the batch: 190.83333333333334\n",
      "Average reward of all training: 76.61887334818273\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: 0.14050248265266418\n",
      "Cross Entropy: [0.7440691  0.23925978 0.6967939  ... 0.15831569 0.37602487 0.8204477 ]\n",
      "=======================================================\n",
      "Epoch:  135 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1107.0\n",
      "Mean reward of the batch: 184.5\n",
      "Average reward of all training: 77.41799280486286\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: 0.09805971384048462\n",
      "Cross Entropy: [0.7161399  0.24684235 0.6888425  ... 0.5737307  0.3701845  0.8690054 ]\n",
      "=======================================================\n",
      "Epoch:  136 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1175.0\n",
      "Mean reward of the batch: 195.83333333333334\n",
      "Average reward of all training: 78.28869383816046\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: -0.012498628348112106\n",
      "Cross Entropy: [0.73081946 0.24808827 0.6842328  ... 0.8455128  1.3614011  0.14820024]\n",
      "=======================================================\n",
      "Epoch:  137 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1172.0\n",
      "Mean reward of the batch: 195.33333333333334\n",
      "Average reward of all training: 79.14303427243178\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: -0.16583029925823212\n",
      "Cross Entropy: [0.6650219  1.4110781  0.11633144 ... 0.04486407 0.08126816 0.15193786]\n",
      "=======================================================\n",
      "Epoch:  138 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1108.0\n",
      "Mean reward of the batch: 184.66666666666666\n",
      "Average reward of all training: 79.90769827528857\n",
      "Max reward for a batch so far: 1195.0\n",
      "Training Loss: 0.25908079743385315\n",
      "Cross Entropy: [0.46169716 0.41092473 1.0450194  ... 0.59266376 0.4164904  0.80461997]\n",
      "=======================================================\n",
      "Epoch:  139 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1196.0\n",
      "Mean reward of the batch: 199.33333333333334\n",
      "Average reward of all training: 80.76687550592197\n",
      "Max reward for a batch so far: 1196.0\n",
      "Training Loss: 0.08851799368858337\n",
      "Cross Entropy: [0.6465954  0.26614517 0.67231    ... 0.38899156 0.9178275  0.19944558]\n",
      "=======================================================\n",
      "Epoch:  140 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 168.5\n",
      "Average reward of all training: 81.39354068087967\n",
      "Max reward for a batch so far: 1196.0\n",
      "Training Loss: -0.6349272131919861\n",
      "Cross Entropy: [0.64386714 0.28560624 0.80243033 ... 0.18960157 0.5382507  0.30231962]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  141 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1180.0\n",
      "Mean reward of the batch: 196.66666666666666\n",
      "Average reward of all training: 82.2110805814881\n",
      "Max reward for a batch so far: 1196.0\n",
      "Training Loss: 0.02743961289525032\n",
      "Cross Entropy: [0.60917485 1.4446689  0.10463074 ... 0.2743551  0.942396   0.1658156 ]\n",
      "=======================================================\n",
      "Epoch:  142 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 83.04058001401283\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.07673142850399017\n",
      "Cross Entropy: [0.6449421  0.2695964  0.8256397  ... 0.90814054 1.6009351  0.09721714]\n",
      "=======================================================\n",
      "Epoch:  143 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1134.0\n",
      "Mean reward of the batch: 189.0\n",
      "Average reward of all training: 83.7815549789498\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.0072970944456756115\n",
      "Cross Entropy: [0.60630083 0.27871248 0.55075914 ... 0.3468577  0.8062086  0.25901967]\n",
      "=======================================================\n",
      "Epoch:  144 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 84.58862751381818\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.046002332121133804\n",
      "Cross Entropy: [0.5383166  0.29250798 0.5249971  ... 0.36669055 0.8642765  1.597936  ]\n",
      "=======================================================\n",
      "Epoch:  145 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1198.0\n",
      "Mean reward of the batch: 199.66666666666666\n",
      "Average reward of all training: 85.38226916314817\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.055849555879831314\n",
      "Cross Entropy: [0.8322374  0.17354536 0.5809021  ... 0.11260743 0.29670358 0.6696607 ]\n",
      "=======================================================\n",
      "Epoch:  146 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1138.0\n",
      "Mean reward of the batch: 189.66666666666666\n",
      "Average reward of all training: 86.09654585837774\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3207865059375763\n",
      "Cross Entropy: [0.6795905  1.6665657  0.06864019 ... 0.3109538  0.7231579  0.31140512]\n",
      "=======================================================\n",
      "Epoch:  147 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 86.8713992879126\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.029892858117818832\n",
      "Cross Entropy: [0.6778213  0.19163838 0.77883136 ... 1.6008892  0.06975703 0.20822822]\n",
      "=======================================================\n",
      "Epoch:  148 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 87.63578172515642\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.31167447566986084\n",
      "Cross Entropy: [0.8668575  1.9686865  0.05650255 ... 0.2311893  0.78407824 1.4313829 ]\n",
      "=======================================================\n",
      "Epoch:  149 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1153.0\n",
      "Mean reward of the batch: 192.16666666666666\n",
      "Average reward of all training: 88.33733128852226\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.09841059148311615\n",
      "Cross Entropy: [0.79727113 0.16308695 0.5107527  ... 0.08569323 1.8148649  0.05191861]\n",
      "=======================================================\n",
      "Epoch:  150 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 89.08174907993211\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.1082613617181778\n",
      "Cross Entropy: [0.625688   0.24834572 0.56888515 ... 0.33547992 0.40964127 0.5054937 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  151 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 89.81630703304515\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.07100008428096771\n",
      "Cross Entropy: [0.6481549  1.630892   0.07564394 ... 0.2651469  0.7391449  0.2129691 ]\n",
      "=======================================================\n",
      "Epoch:  152 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1150.0\n",
      "Mean reward of the batch: 191.66666666666666\n",
      "Average reward of all training: 90.48637518852952\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12150400876998901\n",
      "Cross Entropy: [0.6486289  0.21482591 0.65477866 ... 0.08441483 0.16763236 0.323689  ]\n",
      "=======================================================\n",
      "Epoch:  153 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1108.0\n",
      "Mean reward of the batch: 184.66666666666666\n",
      "Average reward of all training: 91.10193264917093\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.19183047115802765\n",
      "Cross Entropy: [0.6948247  0.21669495 0.64796275 ... 0.07137546 2.019755   0.04518772]\n",
      "=======================================================\n",
      "Epoch:  154 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1064.0\n",
      "Mean reward of the batch: 177.33333333333334\n",
      "Average reward of all training: 91.66187680945771\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.04449713975191116\n",
      "Cross Entropy: [0.5712042  0.27996132 0.5516255  ... 0.23826417 0.8315532  0.23124784]\n",
      "=======================================================\n",
      "Epoch:  155 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1199.0\n",
      "Mean reward of the batch: 199.83333333333334\n",
      "Average reward of all training: 92.35975717412788\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3239433169364929\n",
      "Cross Entropy: [0.7688606  1.8470308  0.0613635  ... 0.91199684 0.21727903 0.55175036]\n",
      "=======================================================\n",
      "Epoch:  156 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 93.04975873070397\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08956017345190048\n",
      "Cross Entropy: [0.71134645 0.19604602 0.75121385 ... 0.30656803 0.5888101  0.22533564]\n",
      "=======================================================\n",
      "Epoch:  157 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 93.73097045853389\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.43269404768943787\n",
      "Cross Entropy: [0.6012841  0.2529712  0.5583807  ... 0.93997943 0.15506372 0.39363694]\n",
      "=======================================================\n",
      "Epoch:  158 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1163.0\n",
      "Mean reward of the batch: 193.83333333333334\n",
      "Average reward of all training: 94.36452971723516\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.40046021342277527\n",
      "Cross Entropy: [0.51195085 1.3454465  0.10431975 ... 0.0226592  0.04950761 2.0931954 ]\n",
      "=======================================================\n",
      "Epoch:  159 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1105.0\n",
      "Mean reward of the batch: 184.16666666666666\n",
      "Average reward of all training: 94.92932303138252\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3078129291534424\n",
      "Cross Entropy: [0.7777363  0.16553116 0.59481335 ... 0.59131587 0.29477698 0.6451899 ]\n",
      "=======================================================\n",
      "Epoch:  160 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 95.58601476243636\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.42230021953582764\n",
      "Cross Entropy: [0.82374775 0.1616788  0.5934799  ... 0.09662487 0.2718864  0.7377177 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  161 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1182.0\n",
      "Mean reward of the batch: 197.0\n",
      "Average reward of all training: 96.21591529186223\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08338987082242966\n",
      "Cross Entropy: [0.8309765  0.15090272 0.96215177 ... 0.15300804 1.1542165  0.11190557]\n",
      "=======================================================\n",
      "Epoch:  162 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 96.8565577900606\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.0678592398762703\n",
      "Cross Entropy: [0.7639023  0.17974824 0.8471832  ... 1.1706464  0.1773823  0.5064666 ]\n",
      "=======================================================\n",
      "Epoch:  163 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 97.48933964410932\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.10889358818531036\n",
      "Cross Entropy: [0.5973682  1.4263569  0.10346706 ... 0.28828442 0.7972568  1.5395826 ]\n",
      "=======================================================\n",
      "Epoch:  164 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1111.0\n",
      "Mean reward of the batch: 185.16666666666666\n",
      "Average reward of all training: 98.02395749180783\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18390318751335144\n",
      "Cross Entropy: [0.6520878  0.22955662 0.6634505  ... 1.4163508  2.5946927  0.02452591]\n",
      "=======================================================\n",
      "Epoch:  165 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 98.6419941130696\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.020752079784870148\n",
      "Cross Entropy: [0.820348   0.16174504 0.93236184 ... 0.7208114  0.23533434 0.7831819 ]\n",
      "=======================================================\n",
      "Epoch:  166 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 99.25258450997883\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3608444333076477\n",
      "Cross Entropy: [0.57497805 1.3634562  0.11091616 ... 0.48767304 1.0866162  0.14921045]\n",
      "=======================================================\n",
      "Epoch:  167 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1112.0\n",
      "Mean reward of the batch: 185.33333333333334\n",
      "Average reward of all training: 99.76803809574741\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.39175546169281006\n",
      "Cross Entropy: [0.52224064 1.3276128  0.11114222 ... 2.038426   0.05128552 0.12812905]\n",
      "=======================================================\n",
      "Epoch:  168 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 100.36465691660607\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3218974769115448\n",
      "Cross Entropy: [0.7120222  0.19886622 0.65281075 ... 0.5313017  0.40902084 0.42318016]\n",
      "=======================================================\n",
      "Epoch:  169 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1168.0\n",
      "Mean reward of the batch: 194.66666666666666\n",
      "Average reward of all training: 100.92265697429873\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12810754776000977\n",
      "Cross Entropy: [0.6490289  1.5713284  0.07701384 ... 0.3366276  0.8955293  0.18662651]\n",
      "=======================================================\n",
      "Epoch:  170 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1030.0\n",
      "Mean reward of the batch: 171.66666666666666\n",
      "Average reward of all training: 101.33879820778324\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.20100413262844086\n",
      "Cross Entropy: [0.47349256 0.3465229  0.43611324 ... 0.4302527  0.4822836  1.1585608 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  171 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1079.0\n",
      "Mean reward of the batch: 179.83333333333334\n",
      "Average reward of all training: 101.79783057693852\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4013791084289551\n",
      "Cross Entropy: [0.8763891  0.1492542  0.5163033  ... 0.97266746 0.2062691  0.5294717 ]\n",
      "=======================================================\n",
      "Epoch:  172 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1177.0\n",
      "Mean reward of the batch: 168.14285714285714\n",
      "Average reward of all training: 102.18355747557757\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.550006628036499\n",
      "Cross Entropy: [0.73910254 0.19851626 0.6767668  ... 0.7206594  0.26507896 0.9185123 ]\n",
      "=======================================================\n",
      "Epoch:  173 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1114.0\n",
      "Mean reward of the batch: 185.66666666666666\n",
      "Average reward of all training: 102.66611880038154\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.07660935074090958\n",
      "Cross Entropy: [0.59446037 0.24546461 0.6302814  ... 0.3087681  1.0058098  0.11597302]\n",
      "=======================================================\n",
      "Epoch:  174 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1047.0\n",
      "Mean reward of the batch: 174.5\n",
      "Average reward of all training: 103.07895719808049\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1979455202817917\n",
      "Cross Entropy: [0.37572813 0.41672102 0.33369437 ... 0.7478912  0.26107338 0.6966904 ]\n",
      "=======================================================\n",
      "Epoch:  175 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1048.0\n",
      "Mean reward of the batch: 174.66666666666666\n",
      "Average reward of all training: 103.48802982361528\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.23098044097423553\n",
      "Cross Entropy: [1.0150154  0.11339287 0.42756262 ... 1.1806613  0.12107415 1.2059708 ]\n",
      "=======================================================\n",
      "Epoch:  176 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1120.0\n",
      "Mean reward of the batch: 186.66666666666666\n",
      "Average reward of all training: 103.96063571476901\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.6034176349639893\n",
      "Cross Entropy: [1.103899   0.09858342 0.34857666 ... 0.48065278 1.1383153  0.12963285]\n",
      "=======================================================\n",
      "Epoch:  177 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1130.0\n",
      "Mean reward of the batch: 161.42857142857142\n",
      "Average reward of all training: 104.28531331767184\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3436904549598694\n",
      "Cross Entropy: [0.72119534 0.17688282 0.6952241  ... 0.4283734  0.49414736 0.34808913]\n",
      "=======================================================\n",
      "Epoch:  178 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1122.0\n",
      "Mean reward of the batch: 187.0\n",
      "Average reward of all training: 104.75000256869615\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.21702535450458527\n",
      "Cross Entropy: [0.592664   1.5306666  0.08662461 ... 0.16017291 0.60368466 0.24373628]\n",
      "=======================================================\n",
      "Epoch:  179 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 105.28212545937382\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3230992257595062\n",
      "Cross Entropy: [0.77240276 0.16652565 0.78582454 ... 0.22082229 0.5750602  1.1576228 ]\n",
      "=======================================================\n",
      "Epoch:  180 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1134.0\n",
      "Mean reward of the batch: 189.0\n",
      "Average reward of all training: 105.7472247623773\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.04157930612564087\n",
      "Cross Entropy: [0.6611854  0.2142937  0.7311459  ... 0.66964096 0.19372094 0.8405473 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  181 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1150.0\n",
      "Mean reward of the batch: 191.66666666666666\n",
      "Average reward of all training: 106.22191781157228\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.006540162488818169\n",
      "Cross Entropy: [0.48003232 1.4390504  0.08411667 ... 0.12597483 0.40010527 1.195625  ]\n",
      "=======================================================\n",
      "Epoch:  182 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 106.73718199942078\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.008703123778104782\n",
      "Cross Entropy: [0.5852399  1.558719   0.07832785 ... 0.37201813 1.0755985  0.12838353]\n",
      "=======================================================\n",
      "Epoch:  183 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 107.24681488466983\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.16483138501644135\n",
      "Cross Entropy: [0.6090537  0.23027216 0.83356476 ... 0.10734921 0.2714821  0.7320847 ]\n",
      "=======================================================\n",
      "Epoch:  184 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 107.75090828203577\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.07546424865722656\n",
      "Cross Entropy: [0.53793734 0.27555564 0.97799826 ... 1.5944742  0.05693068 0.15050243]\n",
      "=======================================================\n",
      "Epoch:  185 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1131.0\n",
      "Mean reward of the batch: 188.5\n",
      "Average reward of all training: 108.18738985888963\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.280758261680603\n",
      "Cross Entropy: [0.46219206 0.32988426 0.4232441  ... 0.34060618 1.0573165  0.10445045]\n",
      "=======================================================\n",
      "Epoch:  186 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 108.68100604244398\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.09282361716032028\n",
      "Cross Entropy: [0.68926775 0.21060418 0.6333739  ... 0.95087373 0.13733935 0.38583916]\n",
      "=======================================================\n",
      "Epoch:  187 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 109.16934290852717\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.0750092938542366\n",
      "Cross Entropy: [0.8469751  0.1462686  0.4719402  ... 0.25816578 0.6145494  0.24162042]\n",
      "=======================================================\n",
      "Epoch:  188 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 109.65248470156692\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2790065109729767\n",
      "Cross Entropy: [0.62906367 0.24042256 0.562307   ... 1.6201229  0.06606481 0.16332532]\n",
      "=======================================================\n",
      "Epoch:  189 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 110.13051388304011\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.15128430724143982\n",
      "Cross Entropy: [0.5630311  0.22556952 0.6243738  ... 0.2769113  0.78201735 0.2232927 ]\n",
      "=======================================================\n",
      "Epoch:  190 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1199.0\n",
      "Mean reward of the batch: 199.83333333333334\n",
      "Average reward of all training: 110.60263398541008\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5970937609672546\n",
      "Cross Entropy: [0.70749557 0.18985736 0.6867939  ... 0.23527008 0.72952354 0.23091528]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  191 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1116.0\n",
      "Mean reward of the batch: 186.0\n",
      "Average reward of all training: 110.9973845928163\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.6499426960945129\n",
      "Cross Entropy: [0.62580204 0.19803494 0.67616695 ... 0.34553048 0.39391452 0.9695407 ]\n",
      "=======================================================\n",
      "Epoch:  192 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 111.46093988139539\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10016710311174393\n",
      "Cross Entropy: [0.8443565  0.13935383 0.92783767 ... 0.6864976  1.6248691  0.06406874]\n",
      "=======================================================\n",
      "Epoch:  193 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1081.0\n",
      "Mean reward of the batch: 180.16666666666666\n",
      "Average reward of all training: 111.81692810308074\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.24300427734851837\n",
      "Cross Entropy: [0.5073359  1.3348162  0.10084955 ... 0.8148604  0.206417   0.47872448]\n",
      "=======================================================\n",
      "Epoch:  194 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 112.27148002007516\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.11685671657323837\n",
      "Cross Entropy: [0.8250096  0.14450102 0.84026194 ... 0.09855641 0.26505968 0.6965707 ]\n",
      "=======================================================\n",
      "Epoch:  195 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 112.72136986612607\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.16158035397529602\n",
      "Cross Entropy: [0.80009264 0.12344958 0.49182343 ... 0.38425425 0.4497599  0.3540048 ]\n",
      "=======================================================\n",
      "Epoch:  196 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1116.0\n",
      "Mean reward of the batch: 186.0\n",
      "Average reward of all training: 113.09524042803358\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.07042189687490463\n",
      "Cross Entropy: [0.76674104 0.14674358 0.64638317 ... 0.1393222  1.0404286  0.09891736]\n",
      "=======================================================\n",
      "Epoch:  197 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1126.0\n",
      "Mean reward of the batch: 187.66666666666666\n",
      "Average reward of all training: 113.47377558660533\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.17606571316719055\n",
      "Cross Entropy: [0.42102453 1.4227878  0.07012951 ... 0.16072457 0.80877936 0.19078825]\n",
      "=======================================================\n",
      "Epoch:  198 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1114.0\n",
      "Mean reward of the batch: 185.66666666666666\n",
      "Average reward of all training: 113.83838614761575\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.21611221134662628\n",
      "Cross Entropy: [0.3966197  0.30285895 0.3800714  ... 0.9583149  0.155907   0.44026005]\n",
      "=======================================================\n",
      "Epoch:  199 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 114.27135908154733\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18913336098194122\n",
      "Cross Entropy: [0.39698768 0.32478908 0.34657973 ... 1.0302801  0.12292852 0.44510597]\n",
      "=======================================================\n",
      "Epoch:  200 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 114.70000228613956\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.22151118516921997\n",
      "Cross Entropy: [0.5952186  0.21288826 0.9032186  ... 0.23751804 0.50418437 0.9783645 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  201 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1195.0\n",
      "Mean reward of the batch: 199.16666666666666\n",
      "Average reward of all training: 115.12023444723673\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.12276321649551392\n",
      "Cross Entropy: [0.44069833 1.3980721  0.07471422 ... 0.08149542 0.32244676 0.41710827]\n",
      "=======================================================\n",
      "Epoch:  202 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 115.54043130640882\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.08633045852184296\n",
      "Cross Entropy: [0.84972835 0.11933907 0.8939595  ... 0.5889283  0.21572916 0.64735657]\n",
      "=======================================================\n",
      "Epoch:  203 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 115.95648829504721\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.05560022220015526\n",
      "Cross Entropy: [0.87927026 0.12037113 0.58503485 ... 0.5500286  0.22352448 0.6561357 ]\n",
      "=======================================================\n",
      "Epoch:  204 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1160.0\n",
      "Mean reward of the batch: 193.33333333333334\n",
      "Average reward of all training: 116.3357865550388\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.09813199937343597\n",
      "Cross Entropy: [0.5047792  0.21694562 0.47510344 ... 0.37467882 0.3657506  1.0479764 ]\n",
      "=======================================================\n",
      "Epoch:  205 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1191.0\n",
      "Mean reward of the batch: 198.5\n",
      "Average reward of all training: 116.73658759623373\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.04066092148423195\n",
      "Cross Entropy: [0.8169723  0.11236934 0.9259354  ... 1.4841877  0.08788271 0.33829394]\n",
      "=======================================================\n",
      "Epoch:  206 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1188.0\n",
      "Mean reward of the batch: 198.0\n",
      "Average reward of all training: 117.13107018071803\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.15289676189422607\n",
      "Cross Entropy: [0.4562016  0.23626237 1.0162903  ... 0.38098347 0.5044479  0.3380827 ]\n",
      "=======================================================\n",
      "Epoch:  207 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 117.53140317501408\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1772809475660324\n",
      "Cross Entropy: [0.6214617 0.174708  0.6168571 ... 0.8182348 0.1613867 0.51991  ]\n",
      "=======================================================\n",
      "Epoch:  208 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1183.0\n",
      "Mean reward of the batch: 197.16666666666666\n",
      "Average reward of all training: 117.91426501872394\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.23556531965732574\n",
      "Cross Entropy: [0.6692285  0.1780737  0.58083266 ... 0.26196104 1.0292535  2.2872422 ]\n",
      "=======================================================\n",
      "Epoch:  209 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1152.0\n",
      "Mean reward of the batch: 192.0\n",
      "Average reward of all training: 118.26874221959129\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.29267123341560364\n",
      "Cross Entropy: [0.8068652  2.1573684  0.0338719  ... 1.7376112  0.04501018 0.13313207]\n",
      "=======================================================\n",
      "Epoch:  210 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 118.65793868521227\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.30637601017951965\n",
      "Cross Entropy: [0.6710396  1.9077189  0.04024215 ... 0.46456313 0.33052537 0.34889224]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  211 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 119.04344608480842\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.15462690591812134\n",
      "Cross Entropy: [0.7130014  0.1481585  0.65853125 ... 1.2842855  0.08853693 0.41452652]\n",
      "=======================================================\n",
      "Epoch:  212 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 119.42531662214424\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.03536184877157211\n",
      "Cross Entropy: [0.8484421  2.2188985  0.03457069 ... 0.03611699 0.12453162 0.51193   ]\n",
      "=======================================================\n",
      "Epoch:  213 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 119.80360152063183\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.16670610010623932\n",
      "Cross Entropy: [0.82354635 2.3065007  0.02509177 ... 0.14662983 0.71019423 2.1002808 ]\n",
      "=======================================================\n",
      "Epoch:  214 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 120.17835104623634\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.136455699801445\n",
      "Cross Entropy: [0.60016966 0.16618508 0.804716   ... 0.14732584 0.7767702  0.13172403]\n",
      "=======================================================\n",
      "Epoch:  215 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1174.0\n",
      "Mean reward of the batch: 195.66666666666666\n",
      "Average reward of all training: 120.52945949098256\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.11308726668357849\n",
      "Cross Entropy: [0.59613687 0.16829064 0.76788443 ... 0.5829921  0.2421353  0.39627153]\n",
      "=======================================================\n",
      "Epoch:  216 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1116.0\n",
      "Mean reward of the batch: 186.0\n",
      "Average reward of all training: 120.83256384519098\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.014125850982964039\n",
      "Cross Entropy: [0.8166743  0.10803726 0.49373737 ... 0.08045509 0.3309253  0.33213255]\n",
      "=======================================================\n",
      "Epoch:  217 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 121.19739073991359\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.16604162752628326\n",
      "Cross Entropy: [0.82851976 0.12832241 0.6658123  ... 0.09731719 0.95858735 0.11637782]\n",
      "=======================================================\n",
      "Epoch:  218 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 121.55887059890482\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.25829604268074036\n",
      "Cross Entropy: [0.6982664  2.1365495  0.02749514 ... 0.15279391 0.67101586 0.18886264]\n",
      "=======================================================\n",
      "Epoch:  219 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 121.91704927196918\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2914942800998688\n",
      "Cross Entropy: [0.7587502  0.11598321 0.52486736 ... 1.4512416  0.06354801 1.6811858 ]\n",
      "=======================================================\n",
      "Epoch:  220 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 122.2719717752784\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4730014503002167\n",
      "Cross Entropy: [0.8588992  0.09939193 1.0058916  ... 0.51689065 1.486151   0.06322242]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  221 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 122.6236823102319\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3554691970348358\n",
      "Cross Entropy: [0.6818024  0.15882595 0.6627159  ... 0.69590366 0.15603785 0.80075437]\n",
      "=======================================================\n",
      "Epoch:  222 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 122.97222428180743\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.060102760791778564\n",
      "Cross Entropy: [0.78427804 0.11515486 0.5208805  ... 0.87500775 2.1628284  0.03656624]\n",
      "=======================================================\n",
      "Epoch:  223 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1197.0\n",
      "Mean reward of the batch: 199.5\n",
      "Average reward of all training: 123.3153981639518\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10231823474168777\n",
      "Cross Entropy: [0.4766904  0.2297304  0.45771903 ... 0.50003976 1.4479661  0.07870091]\n",
      "=======================================================\n",
      "Epoch:  224 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 123.65774013643413\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08973110467195511\n",
      "Cross Entropy: [0.85868907 0.10347437 0.5262562  ... 1.6024623  0.05417498 0.23439518]\n",
      "=======================================================\n",
      "Epoch:  225 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1197.0\n",
      "Mean reward of the batch: 199.5\n",
      "Average reward of all training: 123.99481684693887\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.35154417157173157\n",
      "Cross Entropy: [0.6102135  0.1854719  0.55626607 ... 1.4341393  2.7842922  0.01374447]\n",
      "=======================================================\n",
      "Epoch:  226 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1191.0\n",
      "Mean reward of the batch: 198.5\n",
      "Average reward of all training: 124.32448579894357\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.23302385210990906\n",
      "Cross Entropy: [0.30926102 1.182414   0.10032251 ... 1.1514437  0.10499142 0.36212367]\n",
      "=======================================================\n",
      "Epoch:  227 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 124.65785810819932\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10399341583251953\n",
      "Cross Entropy: [0.93162644 0.0859594  1.0216607  ... 0.21264918 0.57820255 0.19892573]\n",
      "=======================================================\n",
      "Epoch:  228 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1102.0\n",
      "Mean reward of the batch: 183.66666666666666\n",
      "Average reward of all training: 124.91666867205225\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.09627024829387665\n",
      "Cross Entropy: [0.6418733  0.18331154 0.74121445 ... 0.7823038  1.6732434  0.07511716]\n",
      "=======================================================\n",
      "Epoch:  229 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1164.0\n",
      "Mean reward of the batch: 194.0\n",
      "Average reward of all training: 125.21834260798215\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.29492971301078796\n",
      "Cross Entropy: [0.6721613  1.9395702  0.03384747 ... 0.07019664 0.19859934 0.6281765 ]\n",
      "=======================================================\n",
      "Epoch:  230 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 125.54348024881702\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.24509520828723907\n",
      "Cross Entropy: [0.60261285 0.18635009 0.5336329  ... 0.05719359 0.24771148 0.45588544]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  231 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 125.86580284514248\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2838917374610901\n",
      "Cross Entropy: [0.7632916  0.10045869 0.5831265  ... 1.7400049  0.046166   0.15062712]\n",
      "=======================================================\n",
      "Epoch:  232 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 126.18534679839618\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1800653338432312\n",
      "Cross Entropy: [0.83033025 0.10046203 0.80738086 ... 0.27302358 1.2821835  0.06327882]\n",
      "=======================================================\n",
      "Epoch:  233 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 126.50214788509834\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2866371273994446\n",
      "Cross Entropy: [0.8934356  0.08180501 0.94089085 ... 0.15799856 0.92208076 0.07306141]\n",
      "=======================================================\n",
      "Epoch:  234 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 126.81624127020476\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2310866266489029\n",
      "Cross Entropy: [0.810278   0.10295559 0.7719941  ... 0.18333516 0.8722547  0.11698385]\n",
      "=======================================================\n",
      "Epoch:  235 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 127.12766152011878\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.1606813669204712\n",
      "Cross Entropy: [0.47628534 0.20624591 1.0975078  ... 0.08122497 0.5360943  0.14206778]\n",
      "=======================================================\n",
      "Epoch:  236 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 127.43644261537251\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.24300597608089447\n",
      "Cross Entropy: [0.8165839  0.09850705 0.94354165 ... 0.16640282 0.6611921  0.1990582 ]\n",
      "=======================================================\n",
      "Epoch:  237 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 127.74261796298698\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.15050442516803741\n",
      "Cross Entropy: [0.618573   1.9621849  0.03805036 ... 1.4684306  0.06365562 0.26888514]\n",
      "=======================================================\n",
      "Epoch:  238 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 128.04622040852064\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2862473428249359\n",
      "Cross Entropy: [0.7314988  0.1150875  0.65155846 ... 0.30170938 0.34556365 0.2705825 ]\n",
      "=======================================================\n",
      "Epoch:  239 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 128.34728224781554\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.07715468853712082\n",
      "Cross Entropy: [0.54632497 0.16897778 0.97149575 ... 0.23568569 0.497034   0.19824019]\n",
      "=======================================================\n",
      "Epoch:  240 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1194.0\n",
      "Mean reward of the batch: 199.0\n",
      "Average reward of all training: 128.641668571783\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.031536877155303955\n",
      "Cross Entropy: [1.011474   0.08104327 0.4015702  ... 0.9834292  0.11560098 0.4206787 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  241 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 128.93776123331085\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.6245608329772949\n",
      "Cross Entropy: [1.103524   2.686347   0.01985952 ... 0.4875412  0.2692544  0.53706294]\n",
      "=======================================================\n",
      "Epoch:  242 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 129.23140684804923\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2977156937122345\n",
      "Cross Entropy: [0.8128279  0.08304846 0.46612564 ... 0.08083823 1.4063141  0.06769145]\n",
      "=======================================================\n",
      "Epoch:  243 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1166.0\n",
      "Mean reward of the batch: 194.33333333333334\n",
      "Average reward of all training: 129.49931601054013\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3073219954967499\n",
      "Cross Entropy: [1.0612221  0.06983207 1.0346317  ... 0.04802246 0.20567961 0.753881  ]\n",
      "=======================================================\n",
      "Epoch:  244 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 129.78825324000513\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.17129142582416534\n",
      "Cross Entropy: [0.372424   0.2178949  0.40224183 ... 0.48773694 0.20873514 0.9849844 ]\n",
      "=======================================================\n",
      "Epoch:  245 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 130.07483179820917\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.17254909873008728\n",
      "Cross Entropy: [0.6154651  0.13667814 0.5585639  ... 0.6698324  0.16420345 0.77703   ]\n",
      "=======================================================\n",
      "Epoch:  246 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 130.35908044943596\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.22256354987621307\n",
      "Cross Entropy: [0.37454316 0.2506472  0.3154807  ... 0.12578584 0.5882706  1.674787  ]\n",
      "=======================================================\n",
      "Epoch:  247 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 130.6410274921508\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.16760683059692383\n",
      "Cross Entropy: [0.3637139  0.20043257 1.0556424  ... 0.80577093 0.12025825 0.8697427 ]\n",
      "=======================================================\n",
      "Epoch:  248 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 130.92070076839212\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.008269761689007282\n",
      "Cross Entropy: [0.70222735 0.1179336  0.81618744 ... 1.1951215  0.05676006 0.23749276]\n",
      "=======================================================\n",
      "Epoch:  249 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 131.19812767293675\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.12812696397304535\n",
      "Cross Entropy: [0.47214258 0.16179848 0.44966093 ... 0.20091099 0.7767046  0.13903105]\n",
      "=======================================================\n",
      "Epoch:  250 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1150.0\n",
      "Mean reward of the batch: 191.66666666666666\n",
      "Average reward of all training: 131.44000182891165\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.0699920505285263\n",
      "Cross Entropy: [0.6307394  2.2130446  0.02145043 ... 0.22866133 0.5267688  0.19478777]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  251 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 131.71314923198372\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.009119031019508839\n",
      "Cross Entropy: [0.442968   0.188309   0.36226335 ... 0.05929151 1.346145   0.04532296]\n",
      "=======================================================\n",
      "Epoch:  252 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 131.98412879852347\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2933000326156616\n",
      "Cross Entropy: [0.66920316 0.09352535 0.8310243  ... 1.6029853  0.0371676  0.15057586]\n",
      "=======================================================\n",
      "Epoch:  253 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 132.25296623410242\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4621841311454773\n",
      "Cross Entropy: [0.64796674 0.12573192 0.88765043 ... 0.27087185 0.7951498  1.8305783 ]\n",
      "=======================================================\n",
      "Epoch:  254 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 132.51968683948\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1230616644024849\n",
      "Cross Entropy: [0.6140629  0.12539546 0.8613326  ... 0.09617843 0.7926492  0.0710967 ]\n",
      "=======================================================\n",
      "Epoch:  255 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 132.78431551854084\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10381830483675003\n",
      "Cross Entropy: [0.7033806  0.10585214 0.78054476 ... 0.21093874 0.887736   0.13372742]\n",
      "=======================================================\n",
      "Epoch:  256 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 133.04687678604654\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.06882552057504654\n",
      "Cross Entropy: [0.4029988  0.22310226 0.3565351  ... 0.6008899  0.19009377 1.0717311 ]\n",
      "=======================================================\n",
      "Epoch:  257 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 133.3073947752059\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.13436327874660492\n",
      "Cross Entropy: [0.45706666 0.21419738 1.162932   ... 0.2010901  0.41101217 1.5130054 ]\n",
      "=======================================================\n",
      "Epoch:  258 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1114.0\n",
      "Mean reward of the batch: 185.66666666666666\n",
      "Average reward of all training: 133.51033768951388\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.004757249262183905\n",
      "Cross Entropy: [1.1260493  0.05812275 0.3015555  ... 3.1553128  0.01000839 0.02749966]\n",
      "=======================================================\n",
      "Epoch:  259 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1186.0\n",
      "Mean reward of the batch: 197.66666666666666\n",
      "Average reward of all training: 133.75804552340247\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.7864160537719727\n",
      "Cross Entropy: [0.83747554 0.08182896 0.5884944  ... 1.027273   0.11358411 0.5650337 ]\n",
      "=======================================================\n",
      "Epoch:  260 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 134.01282227138938\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.409218430519104\n",
      "Cross Entropy: [0.9741972  0.08188948 0.5595497  ... 0.60911304 0.1474103  0.74074453]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  261 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 134.2656467071312\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3460448682308197\n",
      "Cross Entropy: [0.5288621  0.1624003  0.96094346 ... 0.43491963 0.26054087 1.1724329 ]\n",
      "=======================================================\n",
      "Epoch:  262 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 134.51654118534825\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.16197971999645233\n",
      "Cross Entropy: [0.5601152  0.17002255 0.9791995  ... 0.12437657 0.80899835 0.12571488]\n",
      "=======================================================\n",
      "Epoch:  263 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 134.7655277207652\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.03788422420620918\n",
      "Cross Entropy: [0.63235897 0.11957487 0.69294864 ... 0.44468242 0.24522555 0.9832767 ]\n",
      "=======================================================\n",
      "Epoch:  264 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 135.01262799455017\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.03260698541998863\n",
      "Cross Entropy: [0.5090051  0.15393583 0.89874876 ... 0.4616233  0.20004597 0.92191124]\n",
      "=======================================================\n",
      "Epoch:  265 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 135.25786336060847\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3012992739677429\n",
      "Cross Entropy: [0.5267941  0.1552696  0.53303814 ... 0.35167304 0.32206577 0.30534488]\n",
      "=======================================================\n",
      "Epoch:  266 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1121.0\n",
      "Mean reward of the batch: 186.83333333333334\n",
      "Average reward of all training: 135.45175610486683\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.0794588029384613\n",
      "Cross Entropy: [0.4037029  0.2290102  1.1695566  ... 0.06793863 1.801425   0.03500279]\n",
      "=======================================================\n",
      "Epoch:  267 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 135.69350982732053\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2638112008571625\n",
      "Cross Entropy: [0.7269525  2.1414406  0.03120182 ... 0.6224398  0.16063696 0.5452773 ]\n",
      "=======================================================\n",
      "Epoch:  268 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 135.93345941751707\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.18368418514728546\n",
      "Cross Entropy: [0.48455    1.7691168  0.03651717 ... 0.26163423 0.95658684 0.12346728]\n",
      "=======================================================\n",
      "Epoch:  269 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 136.17162499589062\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2488817572593689\n",
      "Cross Entropy: [1.1752158  0.05794311 0.32751718 ... 0.38029042 0.3421992  0.33057657]\n",
      "=======================================================\n",
      "Epoch:  270 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1186.0\n",
      "Mean reward of the batch: 197.66666666666666\n",
      "Average reward of all training: 136.3993844094861\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.09292274713516235\n",
      "Cross Entropy: [0.8674076  0.09640449 0.6223002  ... 0.8606664  0.11044894 0.4985524 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  271 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1191.0\n",
      "Mean reward of the batch: 198.5\n",
      "Average reward of all training: 136.6285379725507\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5742043256759644\n",
      "Cross Entropy: [0.849975   0.09394573 0.57944894 ... 1.6199     0.04872779 0.17285343]\n",
      "=======================================================\n",
      "Epoch:  272 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 136.86152128882813\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.06578970700502396\n",
      "Cross Entropy: [0.463045   0.19506499 0.46583965 ... 0.4795261  1.8538787  0.02629882]\n",
      "=======================================================\n",
      "Epoch:  273 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 137.09279776762364\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3002309799194336\n",
      "Cross Entropy: [0.31420937 0.29928085 0.2762054  ... 0.2530239  0.40889776 0.25430152]\n",
      "=======================================================\n",
      "Epoch:  274 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 137.32238609693886\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.06534387916326523\n",
      "Cross Entropy: [1.2067175  0.05616609 0.275984   ... 1.754004   0.03554676 0.1310019 ]\n",
      "=======================================================\n",
      "Epoch:  275 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 137.55030469295\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.01146631594747305\n",
      "Cross Entropy: [0.62050027 0.11019721 0.6140716  ... 0.4996825  0.18116052 0.90319526]\n",
      "=======================================================\n",
      "Epoch:  276 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 137.77657170493205\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.32726728916168213\n",
      "Cross Entropy: [0.6470349  0.12681139 0.8526893  ... 0.6776708  1.9937067  0.02839517]\n",
      "=======================================================\n",
      "Epoch:  277 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 138.00120502007672\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.8021413087844849\n",
      "Cross Entropy: [0.3887789  0.23751983 0.31565008 ... 0.32728362 0.31693006 0.20999771]\n",
      "=======================================================\n",
      "Epoch:  278 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1136.0\n",
      "Mean reward of the batch: 189.33333333333334\n",
      "Average reward of all training: 138.18585296364958\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.13318249583244324\n",
      "Cross Entropy: [0.49002236 0.1757753  0.4853263  ... 0.15307607 0.5847904  0.2181446 ]\n",
      "=======================================================\n",
      "Epoch:  279 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1096.0\n",
      "Mean reward of the batch: 182.66666666666666\n",
      "Average reward of all training: 138.34528240344534\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2603972554206848\n",
      "Cross Entropy: [0.37702554 0.20078596 0.38186672 ... 0.298481   1.2303646  2.7861366 ]\n",
      "=======================================================\n",
      "Epoch:  280 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1096.0\n",
      "Mean reward of the batch: 182.66666666666666\n",
      "Average reward of all training: 138.5035730615283\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.093390092253685\n",
      "Cross Entropy: [0.43103883 0.17441906 0.4159472  ... 0.21721111 0.4767941  0.19583121]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  281 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1131.0\n",
      "Mean reward of the batch: 188.5\n",
      "Average reward of all training: 138.68149628906733\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.17851269245147705\n",
      "Cross Entropy: [0.33340922 1.5451493  0.04121796 ... 0.20302339 0.4486214  0.19642565]\n",
      "=======================================================\n",
      "Epoch:  282 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1164.0\n",
      "Mean reward of the batch: 194.0\n",
      "Average reward of all training: 138.8776611958437\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08300726115703583\n",
      "Cross Entropy: [1.0408974  0.06635066 0.9246436  ... 0.4538568  0.21486072 0.5331756 ]\n",
      "=======================================================\n",
      "Epoch:  283 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 139.0936411916181\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3271668553352356\n",
      "Cross Entropy: [0.7693416  0.10055334 0.64532405 ... 0.07253892 0.30127385 0.44749197]\n",
      "=======================================================\n",
      "Epoch:  284 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 139.30810020150676\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2607686221599579\n",
      "Cross Entropy: [0.36145237 1.6036645  0.03824773 ... 0.11460724 0.6310207  0.148169  ]\n",
      "=======================================================\n",
      "Epoch:  285 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 139.52105423588745\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.45833492279052734\n",
      "Cross Entropy: [0.45514524 0.1917423  0.40005782 ... 1.8505764  0.0282429  0.11033483]\n",
      "=======================================================\n",
      "Epoch:  286 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 139.7325190812165\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.46954405307769775\n",
      "Cross Entropy: [0.2907194  0.29472485 1.4758351  ... 0.08070991 0.3775325  0.3034429 ]\n",
      "=======================================================\n",
      "Epoch:  287 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 139.94251030393005\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.029964599758386612\n",
      "Cross Entropy: [0.39875725 0.22702053 0.32961527 ... 0.4541049  0.31031516 0.37743747]\n",
      "=======================================================\n",
      "Epoch:  288 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 140.15104325426358\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.5305618643760681\n",
      "Cross Entropy: [0.59926975 0.11972209 0.7245591  ... 0.23583956 0.46508574 1.7117122 ]\n",
      "=======================================================\n",
      "Epoch:  289 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1152.0\n",
      "Mean reward of the batch: 192.0\n",
      "Average reward of all training: 140.33045140909311\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.1047234833240509\n",
      "Cross Entropy: [1.1519601  0.05258399 0.32338297 ... 2.9707122  0.01057108 0.0319228 ]\n",
      "=======================================================\n",
      "Epoch:  290 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1131.0\n",
      "Mean reward of the batch: 188.5\n",
      "Average reward of all training: 140.4965533007859\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.23318251967430115\n",
      "Cross Entropy: [0.2801318  0.30157986 0.26746216 ... 0.169085   0.54334456 1.5817899 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  291 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1130.0\n",
      "Mean reward of the batch: 161.42857142857142\n",
      "Average reward of all training: 140.56848463455836\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.19465364515781403\n",
      "Cross Entropy: [1.1188781  0.05110011 0.40004277 ... 0.13142157 0.77977556 0.10285159]\n",
      "=======================================================\n",
      "Epoch:  292 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1107.0\n",
      "Mean reward of the batch: 184.5\n",
      "Average reward of all training: 140.71893502964548\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.7853556275367737\n",
      "Cross Entropy: [0.21429649 1.1364021  2.7251275  ... 0.19064276 0.9535153  0.08417169]\n",
      "=======================================================\n",
      "Epoch:  293 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1038.0\n",
      "Mean reward of the batch: 173.0\n",
      "Average reward of all training: 140.82910931282075\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2530127763748169\n",
      "Cross Entropy: [0.22074486 1.1926848  0.06059051 ... 0.08278732 0.4586658  0.23236518]\n",
      "=======================================================\n",
      "Epoch:  294 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1168.0\n",
      "Mean reward of the batch: 194.66666666666666\n",
      "Average reward of all training: 141.0122302562012\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5480976700782776\n",
      "Cross Entropy: [0.22615206 0.38473955 0.1845856  ... 0.27911428 0.28703466 1.1706716 ]\n",
      "=======================================================\n",
      "Epoch:  295 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1155.0\n",
      "Mean reward of the batch: 192.5\n",
      "Average reward of all training: 141.18676506889204\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2666723132133484\n",
      "Cross Entropy: [0.22872545 1.1555603  0.06732051 ... 0.06804354 0.32305837 0.37619886]\n",
      "=======================================================\n",
      "Epoch:  296 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1189.0\n",
      "Mean reward of the batch: 198.16666666666666\n",
      "Average reward of all training: 141.37926473645211\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.10004913806915283\n",
      "Cross Entropy: [1.3066452  0.04744976 0.29723454 ... 0.9016944  0.09079249 0.8262885 ]\n",
      "=======================================================\n",
      "Epoch:  297 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1070.0\n",
      "Mean reward of the batch: 178.33333333333334\n",
      "Average reward of all training: 141.50368920984226\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4115592837333679\n",
      "Cross Entropy: [0.39385164 0.18888    0.4873463  ... 0.0621089  1.2956463  0.06023466]\n",
      "=======================================================\n",
      "Epoch:  298 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1057.0\n",
      "Mean reward of the batch: 151.0\n",
      "Average reward of all training: 141.53555602457433\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.486311137676239\n",
      "Cross Entropy: [0.27450207 0.2719615  0.27999663 ... 0.07294085 1.1729094  0.06501622]\n",
      "=======================================================\n",
      "Epoch:  299 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1143.0\n",
      "Mean reward of the batch: 190.5\n",
      "Average reward of all training: 141.69931670676638\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3871779441833496\n",
      "Cross Entropy: [0.46979225 0.15774661 0.8840738  ... 0.10018156 0.5105287  0.2004991 ]\n",
      "=======================================================\n",
      "Epoch:  300 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1180.0\n",
      "Mean reward of the batch: 196.66666666666666\n",
      "Average reward of all training: 141.88254120663274\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5945671200752258\n",
      "Cross Entropy: [1.1765134  0.05833194 0.42774445 ... 0.11455356 1.0427163  0.05905285]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  301 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1154.0\n",
      "Mean reward of the batch: 192.33333333333334\n",
      "Average reward of all training: 142.0501518117048\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4938585162162781\n",
      "Cross Entropy: [0.45614558 0.18075964 1.1201091  ... 0.25742134 0.39185762 0.2829895 ]\n",
      "=======================================================\n",
      "Epoch:  302 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1193.0\n",
      "Mean reward of the batch: 198.83333333333334\n",
      "Average reward of all training: 142.23817559157774\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2559737265110016\n",
      "Cross Entropy: [0.32632542 0.2564056  0.280225   ... 0.5159941  0.16224636 0.6003804 ]\n",
      "=======================================================\n",
      "Epoch:  303 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1100.0\n",
      "Mean reward of the batch: 183.33333333333334\n",
      "Average reward of all training: 142.37380317488388\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.036559171974658966\n",
      "Cross Entropy: [1.2900918  0.04254178 0.30143738 ... 0.0489083  1.4690534  0.04903237]\n",
      "=======================================================\n",
      "Epoch:  304 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 142.56336303286125\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.41272294521331787\n",
      "Cross Entropy: [0.32490537 0.19323023 0.3861364  ... 0.04375706 0.27217355 1.266995  ]\n",
      "=======================================================\n",
      "Epoch:  305 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 142.75167987537645\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4809201657772064\n",
      "Cross Entropy: [0.46334493 1.9502158  0.02474516 ... 0.05150214 1.3287702  0.04379483]\n",
      "=======================================================\n",
      "Epoch:  306 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1192.0\n",
      "Mean reward of the batch: 198.66666666666666\n",
      "Average reward of all training: 142.93440859038068\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.008509678766131401\n",
      "Cross Entropy: [0.47084653 0.14356162 0.93783367 ... 0.45500237 1.8348126  0.02675284]\n",
      "=======================================================\n",
      "Epoch:  307 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1164.0\n",
      "Mean reward of the batch: 194.0\n",
      "Average reward of all training: 143.10074602168237\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.48725587129592896\n",
      "Cross Entropy: [0.26741007 0.27653417 0.25107193 ... 0.16082588 0.6420614  0.12472821]\n",
      "=======================================================\n",
      "Epoch:  308 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 143.2854838592743\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3531160354614258\n",
      "Cross Entropy: [0.4158221  0.18013589 0.4139843  ... 0.10376348 0.7461535  0.10628271]\n",
      "=======================================================\n",
      "Epoch:  309 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1156.0\n",
      "Mean reward of the batch: 192.66666666666666\n",
      "Average reward of all training: 143.44529351237264\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.0039770230650901794\n",
      "Cross Entropy: [1.413008   0.03748801 0.2634639  ... 1.6237469  0.03848715 0.18187664]\n",
      "=======================================================\n",
      "Epoch:  310 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1125.0\n",
      "Mean reward of the batch: 187.5\n",
      "Average reward of all training: 143.58740546878437\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.14436368644237518\n",
      "Cross Entropy: [1.1890206  0.04137764 1.3916454  ... 0.0801497  0.56006235 0.13769083]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  311 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1159.0\n",
      "Mean reward of the batch: 165.57142857142858\n",
      "Average reward of all training: 143.65809364596328\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.46170395612716675\n",
      "Cross Entropy: [0.23484233 0.29290268 0.25142038 ... 0.6714824  0.14032537 0.60109013]\n",
      "=======================================================\n",
      "Epoch:  312 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1102.0\n",
      "Mean reward of the batch: 183.66666666666666\n",
      "Average reward of all training: 143.78632625179887\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3758721649646759\n",
      "Cross Entropy: [0.23967429 1.2791378  0.05326418 ... 0.08489152 0.51777065 0.16871801]\n",
      "=======================================================\n",
      "Epoch:  313 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1055.0\n",
      "Mean reward of the batch: 175.83333333333334\n",
      "Average reward of all training: 143.88871285589323\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12018886208534241\n",
      "Cross Entropy: [0.24022889 1.3575553  0.04505292 ... 0.24015088 1.0752329  2.5717876 ]\n",
      "=======================================================\n",
      "Epoch:  314 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1172.0\n",
      "Mean reward of the batch: 195.33333333333334\n",
      "Average reward of all training: 144.05254922684048\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.006911173462867737\n",
      "Cross Entropy: [1.380322   0.03243626 0.21882707 ... 0.14971533 0.5752674  0.1496476 ]\n",
      "=======================================================\n",
      "Epoch:  315 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1060.0\n",
      "Mean reward of the batch: 151.42857142857142\n",
      "Average reward of all training: 144.07596517033807\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.582550048828125\n",
      "Cross Entropy: [1.451023   0.03100884 0.2416209  ... 0.08522031 1.0024558  0.05959513]\n",
      "=======================================================\n",
      "Epoch:  316 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1117.0\n",
      "Mean reward of the batch: 159.57142857142858\n",
      "Average reward of all training: 144.1250014469238\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.24250490963459015\n",
      "Cross Entropy: [0.2426919  0.27297223 0.2512587  ... 0.23640947 1.0872474  0.06878153]\n",
      "=======================================================\n",
      "Epoch:  317 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1036.0\n",
      "Mean reward of the batch: 129.5\n",
      "Average reward of all training: 144.07886579567165\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.27128705382347107\n",
      "Cross Entropy: [0.20157102 0.29332888 0.23982044 ... 1.9144859  0.02423844 0.17223045]\n",
      "=======================================================\n",
      "Epoch:  318 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1120.0\n",
      "Mean reward of the batch: 160.0\n",
      "Average reward of all training: 144.12893225543368\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5131016373634338\n",
      "Cross Entropy: [0.20253868 0.32815024 1.6000773  ... 0.83007926 0.08116771 1.1007352 ]\n",
      "=======================================================\n",
      "Epoch:  319 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1001.0\n",
      "Mean reward of the batch: 143.0\n",
      "Average reward of all training: 144.12539328284612\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.037905141711235046\n",
      "Cross Entropy: [0.15256909 0.47718668 0.1330731  ... 2.9921262  0.00929646 0.04895756]\n",
      "=======================================================\n",
      "Epoch:  320 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1017.0\n",
      "Mean reward of the batch: 169.5\n",
      "Average reward of all training: 144.20468892883724\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5346354842185974\n",
      "Cross Entropy: [0.5878897  0.10372705 0.64374524 ... 0.22185507 0.41606462 0.21498968]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  321 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1137.0\n",
      "Mean reward of the batch: 189.5\n",
      "Average reward of all training: 144.3457958169094\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.026465900242328644\n",
      "Cross Entropy: [0.34911743 0.20090874 0.4285229  ... 0.12052881 0.5986891  1.8586018 ]\n",
      "=======================================================\n",
      "Epoch:  322 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1013.0\n",
      "Mean reward of the batch: 168.83333333333334\n",
      "Average reward of all training: 144.42184407006596\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.09828007221221924\n",
      "Cross Entropy: [1.3611658  0.03679067 0.26013812 ... 0.13501224 0.65747094 0.1551664 ]\n",
      "=======================================================\n",
      "Epoch:  323 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1006.0\n",
      "Mean reward of the batch: 167.66666666666666\n",
      "Average reward of all training: 144.4938094651019\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.16815215349197388\n",
      "Cross Entropy: [0.3377877  0.23126309 1.1919003  ... 0.0720939  1.167336   0.06409177]\n",
      "=======================================================\n",
      "Epoch:  324 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1010.0\n",
      "Mean reward of the batch: 168.33333333333334\n",
      "Average reward of all training: 144.56738824247296\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.23109808564186096\n",
      "Cross Entropy: [0.1840416  1.0607104  2.6751573  ... 0.5045835  0.15702012 0.6588686 ]\n",
      "=======================================================\n",
      "Epoch:  325 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1040.0\n",
      "Mean reward of the batch: 173.33333333333334\n",
      "Average reward of all training: 144.65589884275258\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.07339781522750854\n",
      "Cross Entropy: [0.17168702 0.41554454 0.17214745 ... 0.3759224  0.26674446 1.2702491 ]\n",
      "=======================================================\n",
      "Epoch:  326 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1065.0\n",
      "Mean reward of the batch: 177.5\n",
      "Average reward of all training: 144.75664761930855\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.17190760374069214\n",
      "Cross Entropy: [0.4388746  0.16650799 0.46648055 ... 0.44378674 0.20754467 0.94315135]\n",
      "=======================================================\n",
      "Epoch:  327 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1193.0\n",
      "Mean reward of the batch: 198.83333333333334\n",
      "Average reward of all training: 144.92201974687435\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4010557532310486\n",
      "Cross Entropy: [0.33651114 0.21114512 1.2623928  ... 0.04133166 0.26962507 0.3317881 ]\n",
      "=======================================================\n",
      "Epoch:  328 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1089.0\n",
      "Mean reward of the batch: 155.57142857142858\n",
      "Average reward of all training: 144.95448745670532\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10015007108449936\n",
      "Cross Entropy: [0.33136058 0.20251162 0.33509833 ... 0.492862   0.18422723 0.4142777 ]\n",
      "=======================================================\n",
      "Epoch:  329 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1176.0\n",
      "Mean reward of the batch: 168.0\n",
      "Average reward of all training: 145.02453460729282\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3240119516849518\n",
      "Cross Entropy: [0.28250825 1.6950617  0.02723845 ... 0.15551738 0.77749956 0.11483385]\n",
      "=======================================================\n",
      "Epoch:  330 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1127.0\n",
      "Mean reward of the batch: 187.83333333333334\n",
      "Average reward of all training: 145.15425823979598\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.35995036363601685\n",
      "Cross Entropy: [0.43606213 0.14355077 0.5042988  ... 0.46304697 0.17159544 0.5625267 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  331 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1134.0\n",
      "Mean reward of the batch: 189.0\n",
      "Average reward of all training: 145.28672271641292\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1810331493616104\n",
      "Cross Entropy: [0.28519097 0.24543859 0.30588323 ... 0.04126486 1.4331611  0.04774564]\n",
      "=======================================================\n",
      "Epoch:  332 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1164.0\n",
      "Mean reward of the batch: 194.0\n",
      "Average reward of all training: 145.43344945521892\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.08103230595588684\n",
      "Cross Entropy: [0.23185848 1.3587363  0.04139114 ... 0.26849666 0.34950665 0.23570623]\n",
      "=======================================================\n",
      "Epoch:  333 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 145.5973129703684\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4088512659072876\n",
      "Cross Entropy: [0.29788193 0.27222794 0.24437182 ... 0.07949603 0.45456913 1.8213327 ]\n",
      "=======================================================\n",
      "Epoch:  334 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 145.76019526686432\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10174278169870377\n",
      "Cross Entropy: [0.5026506  0.15181825 0.5011666  ... 0.83994305 0.09849603 0.51892686]\n",
      "=======================================================\n",
      "Epoch:  335 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1194.0\n",
      "Mean reward of the batch: 199.0\n",
      "Average reward of all training: 145.91912005711248\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.14341408014297485\n",
      "Cross Entropy: [0.9304929  2.710605   0.0136417  ... 0.44212502 0.20939215 0.39778268]\n",
      "=======================================================\n",
      "Epoch:  336 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1146.0\n",
      "Mean reward of the batch: 191.0\n",
      "Average reward of all training: 146.05328934265677\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3211219608783722\n",
      "Cross Entropy: [0.35415763 0.19708072 1.1003265  ... 0.5208089  1.9628403  0.02232596]\n",
      "=======================================================\n",
      "Epoch:  337 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1160.0\n",
      "Mean reward of the batch: 165.71428571428572\n",
      "Average reward of all training: 146.1116305781809\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.41086623072624207\n",
      "Cross Entropy: [0.19943133 0.37214425 0.17091371 ... 1.6805855  0.03115929 0.15254842]\n",
      "=======================================================\n",
      "Epoch:  338 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1120.0\n",
      "Mean reward of the batch: 186.66666666666666\n",
      "Average reward of all training: 146.23161589205213\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.39286524057388306\n",
      "Cross Entropy: [0.34626225 0.19155964 1.1360977  ... 0.18021454 1.0542625  0.05939812]\n",
      "=======================================================\n",
      "Epoch:  339 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1178.0\n",
      "Mean reward of the batch: 196.33333333333334\n",
      "Average reward of all training: 146.37940856887008\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10016372054815292\n",
      "Cross Entropy: [0.5230702  0.11613935 0.7382149  ... 0.07223643 0.4554733  0.20450541]\n",
      "=======================================================\n",
      "Epoch:  340 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1070.0\n",
      "Mean reward of the batch: 152.85714285714286\n",
      "Average reward of all training: 146.39846072854147\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.22037827968597412\n",
      "Cross Entropy: [0.3968405  0.16261037 0.4936922  ... 1.0003674  2.5147572  0.01619928]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  341 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1103.0\n",
      "Mean reward of the batch: 183.83333333333334\n",
      "Average reward of all training: 146.50824041359954\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.41265296936035156\n",
      "Cross Entropy: [0.23244917 0.32228208 0.215183   ... 0.6037026  0.12240444 0.75207627]\n",
      "=======================================================\n",
      "Epoch:  342 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1135.0\n",
      "Mean reward of the batch: 189.16666666666666\n",
      "Average reward of all training: 146.63297265410554\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2560827434062958\n",
      "Cross Entropy: [0.33301207 1.7250856  0.0279805  ... 0.2879519  0.38562468 0.20940723]\n",
      "=======================================================\n",
      "Epoch:  343 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1187.0\n",
      "Mean reward of the batch: 197.83333333333334\n",
      "Average reward of all training: 146.78224484267474\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2084784209728241\n",
      "Cross Entropy: [0.17183119 0.43859202 0.14705843 ... 0.06836379 0.3633454  0.29225838]\n",
      "=======================================================\n",
      "Epoch:  344 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 146.9369476192949\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.8922706246376038\n",
      "Cross Entropy: [0.48021665 0.14937587 1.0452965  ... 0.02426009 0.1730246  0.49186617]\n",
      "=======================================================\n",
      "Epoch:  345 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.09075356822447\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.20458832383155823\n",
      "Cross Entropy: [0.54741585 0.11579997 0.626706   ... 0.8408632  0.08512539 0.56162757]\n",
      "=======================================================\n",
      "Epoch:  346 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.24367046542613\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.6188244819641113\n",
      "Cross Entropy: [0.33055618 1.549136   0.03468193 ... 0.09520399 0.84018385 0.1089827 ]\n",
      "=======================================================\n",
      "Epoch:  347 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.39570599722606\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.006041883025318384\n",
      "Cross Entropy: [0.61871386 0.10154768 0.6721231  ... 0.08611495 0.43731755 0.20360579]\n",
      "=======================================================\n",
      "Epoch:  348 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.54686776160185\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2238660752773285\n",
      "Cross Entropy: [0.9205332  0.07542121 0.918218   ... 0.01049358 0.03684893 0.2006307 ]\n",
      "=======================================================\n",
      "Epoch:  349 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.69716326944825\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.26042094826698303\n",
      "Cross Entropy: [1.2607783  0.04230253 0.2780798  ... 0.6886656  2.638331   4.2143483 ]\n",
      "=======================================================\n",
      "Epoch:  350 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.84659994582125\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.055080585181713104\n",
      "Cross Entropy: [0.55058676 0.09958759 0.60422075 ... 0.539938   0.12888783 1.0232631 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  351 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 147.9951851311608\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3916124999523163\n",
      "Cross Entropy: [0.5314787  0.10130878 0.771511   ... 0.06237706 0.45050722 0.18703395]\n",
      "=======================================================\n",
      "Epoch:  352 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 148.14292608249272\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08062567561864853\n",
      "Cross Entropy: [0.40492883 0.1413684  0.47468564 ... 0.37145045 0.2013606  1.2203498 ]\n",
      "=======================================================\n",
      "Epoch:  353 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 148.2898299746103\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.050559040158987045\n",
      "Cross Entropy: [0.77345467 0.09352426 0.7228937  ... 0.22759777 1.0797799  2.591847  ]\n",
      "=======================================================\n",
      "Epoch:  354 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 148.43590390123572\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5109202265739441\n",
      "Cross Entropy: [0.4562394  0.1288829  0.5373043  ... 0.13894412 0.690694   0.07897622]\n",
      "=======================================================\n",
      "Epoch:  355 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 148.5811548761618\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.010207363404333591\n",
      "Cross Entropy: [1.5411725  0.02873809 0.19431496 ... 1.2016785  0.04096454 1.5635685 ]\n",
      "=======================================================\n",
      "Epoch:  356 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 148.72558983437483\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3247317671775818\n",
      "Cross Entropy: [0.30850887 0.24607995 0.2672076  ... 0.05915522 1.3159752  0.04443188]\n",
      "=======================================================\n",
      "Epoch:  357 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 148.8692156331581\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2396174669265747\n",
      "Cross Entropy: [0.51025987 0.13490371 0.90321594 ... 0.04577743 0.3074009  0.26848912]\n",
      "=======================================================\n",
      "Epoch:  358 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.0120390531772\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.6290656924247742\n",
      "Cross Entropy: [0.59051216 0.08910719 0.6247355  ... 0.00643503 0.02823015 0.17834643]\n",
      "=======================================================\n",
      "Epoch:  359 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.1540667995472\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3407047390937805\n",
      "Cross Entropy: [1.0008743  0.05333516 0.98070574 ... 0.4564755  0.12674333 0.7288723 ]\n",
      "=======================================================\n",
      "Epoch:  360 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.29530550288177\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08929036557674408\n",
      "Cross Entropy: [0.30169696 0.20832327 0.2996174  ... 0.9137684  0.06068521 0.37230423]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  361 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.43576172032533\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4712587594985962\n",
      "Cross Entropy: [0.69610023 0.07788651 0.588093   ... 0.2069417  1.0369277  0.06645307]\n",
      "=======================================================\n",
      "Epoch:  362 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.57544193656753\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.17154273390769958\n",
      "Cross Entropy: [0.9314431  0.06396967 0.96381116 ... 0.09369263 0.7723311  0.10052176]\n",
      "=======================================================\n",
      "Epoch:  363 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.71435256484145\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.04740205034613609\n",
      "Cross Entropy: [0.75060517 0.10040456 0.66598284 ... 0.52535206 0.15769744 0.486708  ]\n",
      "=======================================================\n",
      "Epoch:  364 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.85249994790505\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.21247848868370056\n",
      "Cross Entropy: [0.60589725 0.10688001 0.654191   ... 0.157484   1.1561987  0.04115115]\n",
      "=======================================================\n",
      "Epoch:  365 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 149.9898903590067\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5000997185707092\n",
      "Cross Entropy: [0.29739773 0.25852016 0.28334486 ... 0.05924275 0.32695228 0.2919706 ]\n",
      "=======================================================\n",
      "Epoch:  366 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 150.12653000283453\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12195547670125961\n",
      "Cross Entropy: [0.28580242 1.4093294  0.04435183 ... 0.21020265 0.43135703 0.20717186]\n",
      "=======================================================\n",
      "Epoch:  367 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 150.2624250164508\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.13287605345249176\n",
      "Cross Entropy: [0.19522975 0.4644635  0.17272124 ... 0.0614913  0.2815029  0.40234777]\n",
      "=======================================================\n",
      "Epoch:  368 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 150.39758147021044\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5744731426239014\n",
      "Cross Entropy: [0.87253666 0.09724602 0.59934527 ... 0.51117563 0.23381373 0.98781806]\n",
      "=======================================================\n",
      "Epoch:  369 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1171.0\n",
      "Mean reward of the batch: 195.16666666666666\n",
      "Average reward of all training: 150.51890690434718\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.7738088369369507\n",
      "Cross Entropy: [0.26774615 1.2147462  0.06745379 ... 0.5217569  0.21014796 0.85976285]\n",
      "=======================================================\n",
      "Epoch:  370 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1107.0\n",
      "Mean reward of the batch: 184.5\n",
      "Average reward of all training: 150.6107476964976\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.11160491406917572\n",
      "Cross Entropy: [0.19747493 1.040468   0.07973392 ... 0.12012548 0.5916061  0.18093513]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  371 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 150.74387236577928\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3300250768661499\n",
      "Cross Entropy: [0.39103556 0.2375235  0.972831   ... 0.06687216 0.2152132  0.702622  ]\n",
      "=======================================================\n",
      "Epoch:  372 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 150.87628131103256\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4139949679374695\n",
      "Cross Entropy: [0.47641957 0.1998658  0.8810483  ... 0.25405106 0.8475546  0.16368589]\n",
      "=======================================================\n",
      "Epoch:  373 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.00798028875096\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4062904119491577\n",
      "Cross Entropy: [0.8769014  0.12117796 0.60011405 ... 0.30272007 0.47412923 0.2405462 ]\n",
      "=======================================================\n",
      "Epoch:  374 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.13897499386127\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.5341178178787231\n",
      "Cross Entropy: [0.33117786 0.3612969  0.2818814  ... 0.1652044  0.7768061  0.1465592 ]\n",
      "=======================================================\n",
      "Epoch:  375 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.2692710605443\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.061225954443216324\n",
      "Cross Entropy: [0.595327   0.16693047 0.73004436 ... 1.2992637  0.0792475  1.5131209 ]\n",
      "=======================================================\n",
      "Epoch:  376 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.39887406304285\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12239720672369003\n",
      "Cross Entropy: [0.9558503  0.10577182 0.5001852  ... 0.16349621 0.8214612  0.10348114]\n",
      "=======================================================\n",
      "Epoch:  377 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.52778951645652\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.6202333569526672\n",
      "Cross Entropy: [0.3095776  0.33540073 0.3266322  ... 0.3347103  1.0137175  0.12415107]\n",
      "=======================================================\n",
      "Epoch:  378 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.6560228775241\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.7013156414031982\n",
      "Cross Entropy: [0.32426438 0.33422124 0.30247575 ... 0.21583061 0.59085953 0.20908995]\n",
      "=======================================================\n",
      "Epoch:  379 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.78357954539342\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.07224739342927933\n",
      "Cross Entropy: [0.22410296 0.44141334 0.22507203 ... 0.3703199  0.3140401  1.021713  ]\n",
      "=======================================================\n",
      "Epoch:  380 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 151.91046486237923\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.0350097231566906\n",
      "Cross Entropy: [0.26974878 0.3611801  0.32679787 ... 0.48495835 0.30041477 1.0026019 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  381 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.03668411470895\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4707794785499573\n",
      "Cross Entropy: [0.3452536  0.31230062 1.1555829  ... 0.40128708 0.34918252 1.2000182 ]\n",
      "=======================================================\n",
      "Epoch:  382 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.16224253325683\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.5264025926589966\n",
      "Cross Entropy: [0.25496835 0.516682   0.21328524 ... 0.8319713  0.14114007 0.48079437]\n",
      "=======================================================\n",
      "Epoch:  383 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.28714529426662\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2125551551580429\n",
      "Cross Entropy: [1.2428986  0.07834923 0.3165701  ... 2.0687308  0.03344358 0.10895523]\n",
      "=======================================================\n",
      "Epoch:  384 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.41139752006276\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.413753479719162\n",
      "Cross Entropy: [0.24029864 0.8165734  0.1664431  ... 0.47203374 0.33885568 0.422294  ]\n",
      "=======================================================\n",
      "Epoch:  385 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.53500427975092\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.26120465993881226\n",
      "Cross Entropy: [0.33402023 0.3756889  0.36641657 ... 0.33578074 0.99304163 0.14658545]\n",
      "=======================================================\n",
      "Epoch:  386 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1086.0\n",
      "Mean reward of the batch: 181.0\n",
      "Average reward of all training: 152.60874779197954\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18302251398563385\n",
      "Cross Entropy: [0.39135906 1.1575944  0.10955467 ... 0.5387162  0.30385065 0.8491    ]\n",
      "=======================================================\n",
      "Epoch:  387 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.73120580802095\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.22518259286880493\n",
      "Cross Entropy: [1.4294558  0.06700729 0.24026452 ... 1.1871303  0.10861137 0.41762054]\n",
      "=======================================================\n",
      "Epoch:  388 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 152.85303259717554\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.3577602505683899\n",
      "Cross Entropy: [1.4219174  0.06682701 0.22059284 ... 0.31192964 0.5804879  0.29667273]\n",
      "=======================================================\n",
      "Epoch:  389 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1079.0\n",
      "Mean reward of the batch: 179.83333333333334\n",
      "Average reward of all training: 152.92239069675435\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.23693329095840454\n",
      "Cross Entropy: [0.40373865 0.36006805 0.4626701  ... 0.5042908  0.38625753 0.41344973]\n",
      "=======================================================\n",
      "Epoch:  390 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.04310251548063\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5173377990722656\n",
      "Cross Entropy: [0.36818287 1.2537303  0.08627363 ... 0.23599906 0.62866586 1.4855405 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  391 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.1631968824487\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.281592458486557\n",
      "Cross Entropy: [0.32663795 1.1648585  0.09494806 ... 0.07495746 0.20777345 0.55647993]\n",
      "=======================================================\n",
      "Epoch:  392 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.2826785230547\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.27129828929901123\n",
      "Cross Entropy: [1.0608588  0.10634992 1.0969718  ... 1.3812373  0.0891785  0.31825438]\n",
      "=======================================================\n",
      "Epoch:  393 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.4015521145991\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1561470478773117\n",
      "Cross Entropy: [0.21286051 0.6086327  0.19026487 ... 0.75646126 0.15212688 0.5121926 ]\n",
      "=======================================================\n",
      "Epoch:  394 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.51982228689707\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.403743714094162\n",
      "Cross Entropy: [0.4475166  1.4529331  0.06598502 ... 0.18660988 0.8508798  0.16218422]\n",
      "=======================================================\n",
      "Epoch:  395 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.6374936228796\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.263413667678833\n",
      "Cross Entropy: [0.7270089  0.140292   0.52363807 ... 1.3756351  0.07455502 0.35670477]\n",
      "=======================================================\n",
      "Epoch:  396 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.75457065918545\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.06527137756347656\n",
      "Cross Entropy: [1.1982017  0.083536   1.3406429  ... 0.72421527 1.7396483  0.05236534]\n",
      "=======================================================\n",
      "Epoch:  397 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.87105788674418\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.35313087701797485\n",
      "Cross Entropy: [0.5963759  0.20304371 0.6659038  ... 0.4893171  0.3033275  0.48562092]\n",
      "=======================================================\n",
      "Epoch:  398 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 153.98695975135035\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.27530840039253235\n",
      "Cross Entropy: [0.28461683 0.48293325 0.25068855 ... 0.7124854  0.22738497 0.7691089 ]\n",
      "=======================================================\n",
      "Epoch:  399 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.10228065422916\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.16828984022140503\n",
      "Cross Entropy: [0.35140833 0.38571686 1.1762465  ... 0.26829934 0.7252015  0.2486016 ]\n",
      "=======================================================\n",
      "Epoch:  400 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.2170249525936\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.7993751764297485\n",
      "Cross Entropy: [0.49146664 0.2530918  0.55803514 ... 0.5055011  0.30890167 0.5090908 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  401 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.3311969601931\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.209995299577713\n",
      "Cross Entropy: [0.31414512 0.3982321  0.35627168 ... 0.20422536 0.857993   0.1532485 ]\n",
      "=======================================================\n",
      "Epoch:  402 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1152.0\n",
      "Mean reward of the batch: 192.0\n",
      "Average reward of all training: 154.42490045034188\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.5861288905143738\n",
      "Cross Entropy: [0.2159991  0.745994   0.19294365 ... 1.7056825  0.05184809 0.14943747]\n",
      "=======================================================\n",
      "Epoch:  403 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.53799002738816\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.5522012114524841\n",
      "Cross Entropy: [0.5030504  0.27179566 0.90300196 ... 0.2076734  0.4949722  0.4193671 ]\n",
      "=======================================================\n",
      "Epoch:  404 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.65051975504315\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.20470057427883148\n",
      "Cross Entropy: [0.780207   0.1756426  0.8211194  ... 0.30549684 0.7175545  1.494535  ]\n",
      "=======================================================\n",
      "Epoch:  405 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.76249378033935\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.39560967683792114\n",
      "Cross Entropy: [0.49662158 0.31078032 0.52715015 ... 0.20854394 0.8839661  0.12855893]\n",
      "=======================================================\n",
      "Epoch:  406 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.8739162094518\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.5133553147315979\n",
      "Cross Entropy: [0.7889393  0.17583099 0.53436124 ... 0.17007928 0.46270698 0.42408186]\n",
      "=======================================================\n",
      "Epoch:  407 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 154.98479110820008\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2473822981119156\n",
      "Cross Entropy: [0.8142051  1.7502707  0.06091809 ... 0.8552445  0.16450869 0.44348058]\n",
      "=======================================================\n",
      "Epoch:  408 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1186.0\n",
      "Mean reward of the batch: 197.66666666666666\n",
      "Average reward of all training: 155.08940354829437\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.24975231289863586\n",
      "Cross Entropy: [0.5796183  0.34824708 0.83684254 ... 0.9979377  1.7836399  0.07723533]\n",
      "=======================================================\n",
      "Epoch:  409 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.1992094075895\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.16976936161518097\n",
      "Cross Entropy: [0.50609493 1.2772179  2.3313792  ... 0.0793835  1.7344074  0.05736662]\n",
      "=======================================================\n",
      "Epoch:  410 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.3084796285466\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.08862803131341934\n",
      "Cross Entropy: [0.6791564  1.6654843  0.06161382 ... 0.33630642 0.5916759  0.28483033]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  411 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.41721812093456\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.009937162511050701\n",
      "Cross Entropy: [1.287454   0.07203944 1.439873   ... 0.87110037 0.15060611 0.8258823 ]\n",
      "=======================================================\n",
      "Epoch:  412 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.52542875656337\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.05628049746155739\n",
      "Cross Entropy: [0.5590292  0.24907741 0.56867486 ... 0.39506415 0.3509884  0.42355728]\n",
      "=======================================================\n",
      "Epoch:  413 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.6331153697436\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08105640113353729\n",
      "Cross Entropy: [1.0300496  0.10832591 1.1812515  ... 0.9201814  0.11913242 0.37489212]\n",
      "=======================================================\n",
      "Epoch:  414 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.74028175773938\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.01070204097777605\n",
      "Cross Entropy: [0.7937941  0.15996489 0.9477918  ... 0.10510357 1.2943544  0.09816908]\n",
      "=======================================================\n",
      "Epoch:  415 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.8469316812147\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.03292310610413551\n",
      "Cross Entropy: [0.5101182  0.30171034 0.9960333  ... 0.83364207 0.1680462  0.5033014 ]\n",
      "=======================================================\n",
      "Epoch:  416 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 155.95306886467333\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.027613038197159767\n",
      "Cross Entropy: [0.88555264 2.153137   0.0299647  ... 0.85654366 1.9130955  0.04557618]\n",
      "=======================================================\n",
      "Epoch:  417 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.05869699689234\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.021973835304379463\n",
      "Cross Entropy: [0.36219645 0.4221569  0.3235757  ... 0.3889423  0.48412403 0.21807972]\n",
      "=======================================================\n",
      "Epoch:  418 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.16381973134952\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.01499536819756031\n",
      "Cross Entropy: [0.50474787 0.28504935 0.43569857 ... 1.1024613  2.2553937  0.04020046]\n",
      "=======================================================\n",
      "Epoch:  419 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.26844068664465\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.05235147476196289\n",
      "Cross Entropy: [0.8421482  0.13759318 0.8941741  ... 1.1936915  0.07720366 1.4830856 ]\n",
      "=======================================================\n",
      "Epoch:  420 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.37256344691454\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.03396304324269295\n",
      "Cross Entropy: [0.6016333  0.23480737 0.5239387  ... 0.08648741 0.29855147 0.90472764]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  421 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.47619156224255\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.011252603493630886\n",
      "Cross Entropy: [0.40601665 0.37625435 1.1991657  ... 0.03889463 0.1151606  0.34847158]\n",
      "=======================================================\n",
      "Epoch:  422 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.57932854906187\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.024585556238889694\n",
      "Cross Entropy: [0.7963221  0.15665928 0.81029516 ... 0.19936316 0.63895833 0.25184768]\n",
      "=======================================================\n",
      "Epoch:  423 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1162.0\n",
      "Mean reward of the batch: 193.66666666666666\n",
      "Average reward of all training: 156.6670054713257\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.025345196947455406\n",
      "Cross Entropy: [0.5168731  0.27528864 0.52622265 ... 2.697787   0.02271886 0.03706927]\n",
      "=======================================================\n",
      "Epoch:  424 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.76920593011974\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.04939969256520271\n",
      "Cross Entropy: [0.52687275 0.27187496 0.4696113  ... 1.0510905  0.1586148  0.68020695]\n",
      "=======================================================\n",
      "Epoch:  425 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.87092544557828\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.015012644231319427\n",
      "Cross Entropy: [0.6980592  0.19084746 0.688665   ... 1.616124   0.06289829 0.17939809]\n",
      "=======================================================\n",
      "Epoch:  426 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 156.97216740462622\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.011831334792077541\n",
      "Cross Entropy: [0.97190857 0.09967133 0.37912837 ... 0.16115782 0.6465585  0.20016514]\n",
      "=======================================================\n",
      "Epoch:  427 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1127.0\n",
      "Mean reward of the batch: 187.83333333333334\n",
      "Average reward of all training: 157.0444417979019\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08062862604856491\n",
      "Cross Entropy: [0.64084613 0.20949319 0.78719413 ... 1.0517286  0.13476422 1.0193083 ]\n",
      "=======================================================\n",
      "Epoch:  428 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.14480525164512\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.01825152151286602\n",
      "Cross Entropy: [0.6287156  0.21884212 0.8082423  ... 0.35341835 0.9272744  0.14412777]\n",
      "=======================================================\n",
      "Epoch:  429 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.2447008104991\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.034320127218961716\n",
      "Cross Entropy: [0.68792236 0.20641603 0.6958322  ... 0.65203476 0.25563502 0.61129576]\n",
      "=======================================================\n",
      "Epoch:  430 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.34413173884678\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.0897979736328125\n",
      "Cross Entropy: [0.33090073 0.5055423  0.2919683  ... 0.52514243 1.3305981  0.09355368]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  431 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.4431012707752\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.044300079345703125\n",
      "Cross Entropy: [0.5308012  1.3280343  0.09725392 ... 0.6435282  0.29166928 0.5247997 ]\n",
      "=======================================================\n",
      "Epoch:  432 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1177.0\n",
      "Mean reward of the batch: 196.16666666666666\n",
      "Average reward of all training: 157.53273915363604\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12469890713691711\n",
      "Cross Entropy: [0.9735076  0.14685516 0.45928335 ... 0.2911874  0.6981822  0.24876234]\n",
      "=======================================================\n",
      "Epoch:  433 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.6308159685237\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.33125609159469604\n",
      "Cross Entropy: [0.7761546  0.17356    0.8297032  ... 0.42768055 1.1764022  0.11497264]\n",
      "=======================================================\n",
      "Epoch:  434 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.72844081652252\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.28109660744667053\n",
      "Cross Entropy: [0.6311325  0.26630813 0.5572134  ... 1.9227785  0.04469833 0.10306852]\n",
      "=======================================================\n",
      "Epoch:  435 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.82561681464546\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.15996003150939941\n",
      "Cross Entropy: [0.5284909  0.25923052 0.8821187  ... 0.22631654 0.63313895 0.21382858]\n",
      "=======================================================\n",
      "Epoch:  436 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 157.92234705130912\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.42237940430641174\n",
      "Cross Entropy: [0.3757924  0.36540762 0.3757451  ... 0.4880765  0.34024182 0.37778115]\n",
      "=======================================================\n",
      "Epoch:  437 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.0186345866608\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.12165571749210358\n",
      "Cross Entropy: [1.0095818  0.11212768 0.37796074 ... 0.53536123 0.28701302 0.9647997 ]\n",
      "=======================================================\n",
      "Epoch:  438 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.11448245290129\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.06708899885416031\n",
      "Cross Entropy: [0.49596566 0.28414714 0.51158774 ... 0.5946742  1.4562613  0.07793239]\n",
      "=======================================================\n",
      "Epoch:  439 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.20989365460312\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.20481407642364502\n",
      "Cross Entropy: [0.3261864  0.99519694 0.13057484 ... 0.6194081  0.27287814 0.5564479 ]\n",
      "=======================================================\n",
      "Epoch:  440 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.30487116902447\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.1713724136352539\n",
      "Cross Entropy: [0.46488804 0.2942495  0.8982072  ... 1.2990273  0.10033987 1.3599305 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  441 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.39941794641896\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5079082250595093\n",
      "Cross Entropy: [0.45584548 0.27377462 0.5573133  ... 0.0789239  1.5690439  2.9738312 ]\n",
      "=======================================================\n",
      "Epoch:  442 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.4935369103411\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.595059335231781\n",
      "Cross Entropy: [0.35725015 1.1431084  0.09807961 ... 0.41340512 0.33044484 0.98786473]\n",
      "=======================================================\n",
      "Epoch:  443 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.58723095794755\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.887149453163147\n",
      "Cross Entropy: [0.7407213  0.1872639  0.6032362  ... 0.11569103 1.0184658  0.12571804]\n",
      "=======================================================\n",
      "Epoch:  444 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.68050296029452\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.2564462721347809\n",
      "Cross Entropy: [0.61710316 0.19363873 0.731737   ... 0.31145552 0.42068145 0.3786285 ]\n",
      "=======================================================\n",
      "Epoch:  445 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.77335576263093\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.26972973346710205\n",
      "Cross Entropy: [0.40843734 0.32858616 1.0503229  ... 0.3336269  0.96510935 2.0839958 ]\n",
      "=======================================================\n",
      "Epoch:  446 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.86579218468782\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.20253106951713562\n",
      "Cross Entropy: [0.74463224 0.15012814 0.56612873 ... 0.3008549  0.40013835 0.34986782]\n",
      "=======================================================\n",
      "Epoch:  447 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 158.9578150209637\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18637098371982574\n",
      "Cross Entropy: [0.6600443  0.22392184 0.6376177  ... 2.1729555  0.03524288 0.15326937]\n",
      "=======================================================\n",
      "Epoch:  448 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.04942704100617\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.15747888386249542\n",
      "Cross Entropy: [1.2704041  0.08216074 1.4308596  ... 1.5603559  0.06839307 1.6273154 ]\n",
      "=======================================================\n",
      "Epoch:  449 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.1406309896899\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.5764042735099792\n",
      "Cross Entropy: [0.26403487 0.95830405 0.11791432 ... 0.1459086  0.7648563  0.1629948 ]\n",
      "=======================================================\n",
      "Epoch:  450 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.2314295874906\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.44253525137901306\n",
      "Cross Entropy: [0.4286224  1.4403955  0.05942306 ... 0.04143655 0.1272452  0.4448467 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  451 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.32182553075558\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.3191157579421997\n",
      "Cross Entropy: [0.29015794 1.0676005  0.09712455 ... 0.77350754 0.16191407 0.8974818 ]\n",
      "=======================================================\n",
      "Epoch:  452 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.41182149197073\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.16461296379566193\n",
      "Cross Entropy: [0.96814674 0.11500505 0.9429851  ... 1.1064754  2.4393964  0.02513246]\n",
      "=======================================================\n",
      "Epoch:  453 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.50142012002377\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.17103128135204315\n",
      "Cross Entropy: [0.3834565  0.30305594 0.38380045 ... 1.0890299  0.09544761 0.95461845]\n",
      "=======================================================\n",
      "Epoch:  454 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.59062404046426\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.020419422537088394\n",
      "Cross Entropy: [0.9267103  0.11946126 0.8865384  ... 0.21069823 0.6590482  0.19876637]\n",
      "=======================================================\n",
      "Epoch:  455 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.67943585575992\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.28769150376319885\n",
      "Cross Entropy: [1.1346245  0.100568   1.1004913  ... 0.17195776 0.5977904  0.2740095 ]\n",
      "=======================================================\n",
      "Epoch:  456 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1194.0\n",
      "Mean reward of the batch: 199.0\n",
      "Average reward of all training: 159.7656651630938\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.16131408512592316\n",
      "Cross Entropy: [0.5421866  0.21746185 0.7432101  ... 1.0887868  0.12780923 1.059802  ]\n",
      "=======================================================\n",
      "Epoch:  457 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.853705283087\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.41997775435447693\n",
      "Cross Entropy: [0.94750977 0.12317117 0.9960544  ... 0.13790104 0.41735968 0.3916628 ]\n",
      "=======================================================\n",
      "Epoch:  458 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 159.9413609484078\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.1399693489074707\n",
      "Cross Entropy: [0.4263997  1.4346304  0.05897194 ... 1.161889   0.10836185 0.47824836]\n",
      "=======================================================\n",
      "Epoch:  459 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1104.0\n",
      "Mean reward of the batch: 184.0\n",
      "Average reward of all training: 159.99377628403218\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.30101725459098816\n",
      "Cross Entropy: [1.1047846  0.08091377 0.32976255 ... 0.01758414 0.036858   0.08276591]\n",
      "=======================================================\n",
      "Epoch:  460 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 160.08074633558863\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 1.278184413909912\n",
      "Cross Entropy: [0.23382044 0.9896845  0.10017444 ... 0.10071224 1.0712175  0.10980389]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  461 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 160.1673390767262\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.20262150466442108\n",
      "Cross Entropy: [1.9030297  0.03045511 0.12672664 ... 0.0079398  0.02000863 0.05423268]\n",
      "=======================================================\n",
      "Epoch:  462 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 160.2535569575125\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.6314345002174377\n",
      "Cross Entropy: [0.34322354 1.4455401  0.05298565 ... 0.29638332 1.0929914  0.08061907]\n",
      "=======================================================\n",
      "Epoch:  463 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1126.0\n",
      "Mean reward of the batch: 160.85714285714286\n",
      "Average reward of all training: 160.2548605987644\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.10995689034461975\n",
      "Cross Entropy: [0.14320074 0.7650157  0.12943731 ... 0.11773323 0.5077611  1.5182908 ]\n",
      "=======================================================\n",
      "Epoch:  464 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1117.0\n",
      "Mean reward of the batch: 186.16666666666666\n",
      "Average reward of all training: 160.3107050083935\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.4531957507133484\n",
      "Cross Entropy: [1.1820576  0.06272554 1.1631413  ... 0.23075469 0.9550577  0.09889586]\n",
      "=======================================================\n",
      "Epoch:  465 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1107.0\n",
      "Mean reward of the batch: 184.5\n",
      "Average reward of all training: 160.36272499762276\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4465492069721222\n",
      "Cross Entropy: [0.15797138 0.7850721  0.13553923 ... 0.51425576 0.25491697 0.9686997 ]\n",
      "=======================================================\n",
      "Epoch:  466 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1141.0\n",
      "Mean reward of the batch: 142.625\n",
      "Average reward of all training: 160.32466121007423\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.4589284360408783\n",
      "Cross Entropy: [0.3128319  0.3062567  0.33844608 ... 1.2001442  0.06927218 0.29910585]\n",
      "=======================================================\n",
      "Epoch:  467 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1076.0\n",
      "Mean reward of the batch: 134.5\n",
      "Average reward of all training: 160.26936214966722\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.22664834558963776\n",
      "Cross Entropy: [2.109005   0.0191368  0.10560118 ... 1.9674199  0.02328074 0.11905845]\n",
      "=======================================================\n",
      "Epoch:  468 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1066.0\n",
      "Mean reward of the batch: 152.28571428571428\n",
      "Average reward of all training: 160.25230307303482\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.400505006313324\n",
      "Cross Entropy: [0.4156558  1.6896001  0.04341685 ... 0.03560279 0.16880068 0.72329557]\n",
      "=======================================================\n",
      "Epoch:  469 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1119.0\n",
      "Mean reward of the batch: 159.85714285714286\n",
      "Average reward of all training: 160.2514605139391\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.17071108520030975\n",
      "Cross Entropy: [1.704857   0.03356612 0.20366724 ... 0.2606614  1.0195515  0.09214181]\n",
      "=======================================================\n",
      "Epoch:  470 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1123.0\n",
      "Mean reward of the batch: 187.16666666666666\n",
      "Average reward of all training: 160.30872691000874\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.8898282051086426\n",
      "Cross Entropy: [0.3292466  1.3959877  0.05736662 ... 0.30843222 0.42183742 0.29175004]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  471 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 160.39299712888348\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.24455441534519196\n",
      "Cross Entropy: [0.4748361  0.19184238 0.56052613 ... 0.11834343 1.0277777  0.08477532]\n",
      "=======================================================\n",
      "Epoch:  472 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1200.0\n",
      "Mean reward of the batch: 200.0\n",
      "Average reward of all training: 160.47691027055956\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18137909471988678\n",
      "Cross Entropy: [0.8739146  0.0962409  0.80536056 ... 0.443173   0.31929487 0.32942492]\n",
      "=======================================================\n",
      "Epoch:  473 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1154.0\n",
      "Mean reward of the batch: 192.33333333333334\n",
      "Average reward of all training: 160.5442600021933\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.27213147282600403\n",
      "Cross Entropy: [0.1452082  0.7518591  0.13576947 ... 0.5647489  0.21194841 0.8767382 ]\n",
      "=======================================================\n",
      "Epoch:  474 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1157.0\n",
      "Mean reward of the batch: 165.28571428571428\n",
      "Average reward of all training: 160.554263070302\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.2698321044445038\n",
      "Cross Entropy: [1.3711358  0.04414232 0.28974947 ... 0.3530758  0.34273937 0.3185926 ]\n",
      "=======================================================\n",
      "Epoch:  475 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1012.0\n",
      "Mean reward of the batch: 144.57142857142858\n",
      "Average reward of all training: 160.52061499767282\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.21969981491565704\n",
      "Cross Entropy: [1.4000435  0.04262609 0.2814376  ... 0.16245747 0.7866884  0.11217062]\n",
      "=======================================================\n",
      "Epoch:  476 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1113.0\n",
      "Mean reward of the batch: 159.0\n",
      "Average reward of all training: 160.51742042834996\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.12220747768878937\n",
      "Cross Entropy: [0.31864807 0.28698716 0.3473017  ... 0.8173038  2.0658584  0.03061328]\n",
      "=======================================================\n",
      "Epoch:  477 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1160.0\n",
      "Mean reward of the batch: 165.71428571428572\n",
      "Average reward of all training: 160.5283153241276\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.1695312261581421\n",
      "Cross Entropy: [0.28975892 0.32925048 1.3582954  ... 0.44736385 0.29120344 1.0079461 ]\n",
      "=======================================================\n",
      "Epoch:  478 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1063.0\n",
      "Mean reward of the batch: 132.875\n",
      "Average reward of all training: 160.47046320001854\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.38390442728996277\n",
      "Cross Entropy: [0.27673614 0.31280175 1.3006545  ... 1.3278204  0.06270169 1.4575415 ]\n",
      "=======================================================\n",
      "Epoch:  479 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1133.0\n",
      "Mean reward of the batch: 161.85714285714286\n",
      "Average reward of all training: 160.47335814711067\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.43184763193130493\n",
      "Cross Entropy: [1.4985498  0.03993716 0.25555313 ... 0.17889063 0.72490835 0.15183534]\n",
      "=======================================================\n",
      "Epoch:  480 / 500\n",
      "---------------\n",
      "Number of training episodes: 9\n",
      "Total reward of the batch: 1106.0\n",
      "Mean reward of the batch: 122.88888888888889\n",
      "Average reward of all training: 160.39505716948938\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.6228123307228088\n",
      "Cross Entropy: [0.31632224 0.24880351 1.0702232  ... 0.20570755 0.5130094  1.5463406 ]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  481 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1089.0\n",
      "Mean reward of the batch: 181.5\n",
      "Average reward of all training: 160.43893438951122\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.8335131406784058\n",
      "Cross Entropy: [1.4821261  0.0327621  1.707251   ... 1.2786789  0.06004799 0.3504955 ]\n",
      "=======================================================\n",
      "Epoch:  482 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1129.0\n",
      "Mean reward of the batch: 188.16666666666666\n",
      "Average reward of all training: 160.496460805024\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.11536751687526703\n",
      "Cross Entropy: [0.27017987 0.33225363 1.391218   ... 0.61155945 0.20445421 0.5310431 ]\n",
      "=======================================================\n",
      "Epoch:  483 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 168.5\n",
      "Average reward of all training: 160.51303127954776\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.03279034420847893\n",
      "Cross Entropy: [0.3640344  0.23998061 0.4540342  ... 0.9720402  0.09521266 0.3777466 ]\n",
      "=======================================================\n",
      "Epoch:  484 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1011.0\n",
      "Mean reward of the batch: 168.5\n",
      "Average reward of all training: 160.5295332810363\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.03182826563715935\n",
      "Cross Entropy: [0.34708863 1.445359   0.05398186 ... 0.09550904 0.34951657 0.4116845 ]\n",
      "=======================================================\n",
      "Epoch:  485 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1084.0\n",
      "Mean reward of the batch: 180.66666666666666\n",
      "Average reward of all training: 160.5710531436871\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18165729939937592\n",
      "Cross Entropy: [1.9132634  3.6434214  0.0069454  ... 0.09442256 1.0948048  0.08208552]\n",
      "=======================================================\n",
      "Epoch:  486 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1010.0\n",
      "Mean reward of the batch: 168.33333333333334\n",
      "Average reward of all training: 160.58702491362462\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.007037918549031019\n",
      "Cross Entropy: [0.15859893 0.80241144 2.1261892  ... 0.11160527 1.0141363  0.0936719 ]\n",
      "=======================================================\n",
      "Epoch:  487 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1078.0\n",
      "Mean reward of the batch: 179.66666666666666\n",
      "Average reward of all training: 160.62620282276845\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.31032222509384155\n",
      "Cross Entropy: [0.43098864 0.2056668  0.8740157  ... 1.2725222  0.06352653 0.25299463]\n",
      "=======================================================\n",
      "Epoch:  488 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1078.0\n",
      "Mean reward of the batch: 179.66666666666666\n",
      "Average reward of all training: 160.66522016671087\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.18805217742919922\n",
      "Cross Entropy: [0.31810072 0.30023238 0.33006835 ... 0.1998084  0.70072824 0.18844596]\n",
      "=======================================================\n",
      "Epoch:  489 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1050.0\n",
      "Mean reward of the batch: 175.0\n",
      "Average reward of all training: 160.69453464489754\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.010982201434671879\n",
      "Cross Entropy: [0.19727452 0.97746086 0.09337975 ... 0.21474561 0.69663286 1.7145736 ]\n",
      "=======================================================\n",
      "Epoch:  490 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1005.0\n",
      "Mean reward of the batch: 143.57142857142858\n",
      "Average reward of all training: 160.6595895304619\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.27150094509124756\n",
      "Cross Entropy: [0.15281315 0.8752582  0.10081054 ... 0.00937051 0.02834789 0.09142968]\n",
      "Model saved\n",
      "=======================================================\n",
      "Epoch:  491 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1137.0\n",
      "Mean reward of the batch: 162.42857142857142\n",
      "Average reward of all training: 160.66319234491831\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.4215465188026428\n",
      "Cross Entropy: [0.1109248  0.65785587 0.15038879 ... 0.02985108 0.11389786 1.0255473 ]\n",
      "=======================================================\n",
      "Epoch:  492 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1009.0\n",
      "Mean reward of the batch: 168.16666666666666\n",
      "Average reward of all training: 160.6784433089869\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.03665848821401596\n",
      "Cross Entropy: [0.09875224 0.8053613  0.08578743 ... 1.0281824  0.08395753 0.3219178 ]\n",
      "=======================================================\n",
      "Epoch:  493 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1148.0\n",
      "Mean reward of the batch: 164.0\n",
      "Average reward of all training: 160.68518074649404\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.15966959297657013\n",
      "Cross Entropy: [0.2994468  1.3636999  0.05492383 ... 0.14925481 0.81596506 0.10618302]\n",
      "=======================================================\n",
      "Epoch:  494 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1173.0\n",
      "Mean reward of the batch: 195.5\n",
      "Average reward of all training: 160.75565608911248\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.11231093108654022\n",
      "Cross Entropy: [0.18311502 0.90440285 0.10645249 ... 0.03224282 0.13360362 0.57759255]\n",
      "=======================================================\n",
      "Epoch:  495 / 500\n",
      "---------------\n",
      "Number of training episodes: 6\n",
      "Total reward of the batch: 1023.0\n",
      "Mean reward of the batch: 170.5\n",
      "Average reward of all training: 160.7753416323668\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.07912766933441162\n",
      "Cross Entropy: [0.36246288 0.24298131 0.44888726 ... 0.46910268 1.3884151  0.06362139]\n",
      "=======================================================\n",
      "Epoch:  496 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1022.0\n",
      "Mean reward of the batch: 146.0\n",
      "Average reward of all training: 160.74555263714026\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.13340015709400177\n",
      "Cross Entropy: [1.1915008  0.05837872 0.39064243 ... 0.95934176 0.11755476 0.84552515]\n",
      "=======================================================\n",
      "Epoch:  497 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1033.0\n",
      "Mean reward of the batch: 147.57142857142858\n",
      "Average reward of all training: 160.7190453452575\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.016905561089515686\n",
      "Cross Entropy: [0.20222244 0.39788735 1.5200016  ... 0.60795254 0.17842332 0.7380885 ]\n",
      "=======================================================\n",
      "Epoch:  498 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1046.0\n",
      "Mean reward of the batch: 149.42857142857142\n",
      "Average reward of all training: 160.6963737108867\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.08674238622188568\n",
      "Cross Entropy: [0.34634587 0.24237183 0.39339703 ... 0.63685805 0.19247636 0.5466826 ]\n",
      "=======================================================\n",
      "Epoch:  499 / 500\n",
      "---------------\n",
      "Number of training episodes: 7\n",
      "Total reward of the batch: 1030.0\n",
      "Mean reward of the batch: 147.14285714285714\n",
      "Average reward of all training: 160.66921235503892\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: 0.03471970185637474\n",
      "Cross Entropy: [0.44051316 0.18080221 0.5452569  ... 1.0205061  0.0942349  0.47184336]\n",
      "=======================================================\n",
      "Epoch:  500 / 500\n",
      "---------------\n",
      "Number of training episodes: 8\n",
      "Total reward of the batch: 1082.0\n",
      "Mean reward of the batch: 135.25\n",
      "Average reward of all training: 160.61837393032886\n",
      "Max reward for a batch so far: 1200.0\n",
      "Training Loss: -0.12866438925266266\n",
      "Cross Entropy: [0.22491734 0.35777718 0.29410616 ... 0.12036331 0.4891071  0.26720312]\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "\n",
    "maximumRewardRecorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "if training:\n",
    "    while epoch < num_epochs + 1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, discounted_reward_delta_mb, nb_episodes_mb = make_batch(batch_size)\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\n",
    "        allRewards.append(total_reward_of_that_batch)\n",
    "\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "\n",
    "        maximumRewardRecorded = np.amax(allRewards)\n",
    "\n",
    "        print(\"=======================================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"---------------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Total reward of the batch: {}\".format(total_reward_of_that_batch))\n",
    "        print(\"Mean reward of the batch: {}\".format(mean_reward_of_that_batch))\n",
    "        print(\"Average reward of all training: {}\".format(average_reward_of_all_training))\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\n",
    "\n",
    "        _, _ = sess.run([ValueEstimator.loss, ValueEstimator.train_opt], feed_dict={ValueEstimator.inputs_: states_mb.reshape((len(states_mb),state_size)), ValueEstimator.discounted_episode_rewards_: discounted_rewards_mb})\n",
    "        loss_, cross, _ = sess.run([PGNetwork.loss, PGNetwork.cross_entropy, PGNetwork.train_opt], feed_dict = {PGNetwork.inputs_: states_mb.reshape((len(states_mb),state_size)), PGNetwork.actions: actions_mb, PGNetwork.discounted_episode_reward_delta:  discounted_reward_delta_mb})\n",
    "\n",
    "        print(\"Training Loss: {}\".format(loss_))\n",
    "        print(\"Cross Entropy: {}\".format(cross))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "aT4jYueV1A_9",
    "outputId": "704d653f-5df5-4382-f123-4e38d9016588"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mean_reward_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Neo5BTkl8Awk"
   },
   "source": [
    "## **Problem 2**: Pong-v0\n",
    "Train a neural net that uses images of the game as state and models the policy. The discount factor is 0.99. Use frames of the game as input to a neural net which models the policy. The game has six actions [NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE], but you should train a network that chooses the best action only between [RIGHT, LEFT] (actions 2 and 3). You need to pre-process all the images from the game with following function before feeding them into the\n",
    "neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_JN0Xw5w9S0N"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# Environment hyperparameters\n",
    "state_size = [80,80,4]\n",
    "stack_size = 4\n",
    "action_size = 2\n",
    "possible_actions = np.identity(action_size,dtype=int).tolist()\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500\n",
    "batch_size = 3\n",
    "\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HO9aFdja9ORP"
   },
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ce75jSzCls9U"
   },
   "outputs": [],
   "source": [
    "stacked_frames = deque([np.zeros((80,80), dtype = np.float32) for i in range(stack_size)], maxlen = stack_size)\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((80,80), dtype = np.float32) for i in range(stack_size)], maxlen = stack_size)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        # stack the frames\n",
    "        stacked_states = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_states = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_states, stacked_frames\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1IPw7MsB1V77"
   },
   "outputs": [],
   "source": [
    "class PGNetwork2:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = \"PGNetwork2\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name = \"inputs_\")\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name = \"actions\")\n",
    "                self.discounted_reward_delta = tf.placeholder(tf.float32, [None, ], name = \"discounted_reward_delta\")\n",
    "\n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                              filters = 32,\n",
    "                                              kernel_size = [8,8],\n",
    "                                              strides = [4,4],\n",
    "                                              padding = \"VALID\",\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                              name = \"conv1\")\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                    training = True,\n",
    "                                                                    epsilon = 1e-5,\n",
    "                                                                    name = \"batch_norm1\")\n",
    "                self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name = \"conv1_out\")\n",
    "\n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                              filters = 64,\n",
    "                                              kernel_size = [4,4],\n",
    "                                              strides = [2,2],\n",
    "                                              padding = \"VALID\",\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                              name = \"conv2\")\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                    training = True,\n",
    "                                                                    epsilon = 1e-5,\n",
    "                                                                    name = \"batch_norm2\")\n",
    "                self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name = \"conv2_out\")\n",
    "\n",
    "            with tf.name_scope(\"conv3\"):\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                              filters = 64,\n",
    "                                              kernel_size = [3,3],\n",
    "                                              strides = [1,1],\n",
    "                                              padding = \"VALID\",\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                              name = \"conv3\")\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                    training = True,\n",
    "                                                                    epsilon = 1e-5,\n",
    "                                                                    name = \"batch_norm3\")\n",
    "                self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name = \"conv3_out\")\n",
    "\n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                          units = 512,\n",
    "                                          activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                          name = \"fc1\")\n",
    "\n",
    "            with tf.name_scope(\"logits\"):\n",
    "                self.logits = tf.layers.dense(inputs = self.fc,\n",
    "                                          units = action_size,\n",
    "                                          activation = None,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.actions)\n",
    "                self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_reward_delta)\n",
    "                self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-dG42QMTzh3_"
   },
   "outputs": [],
   "source": [
    "class ValueEstimator2:\n",
    "    def __init__(self, state_size, learning_rate, name = \"ValueEstimator2\"):\n",
    "        self.state_size = state_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name = \"inputs_\")\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name = \"discounted_episode_rewards_\")\n",
    "\n",
    "            with tf.name_scope(\"conv1\"):\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                              filters = 32,\n",
    "                                              kernel_size = [8,8],\n",
    "                                              strides = [4,4],\n",
    "                                              padding = \"VALID\",\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                              name = \"conv1\")\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                    training = True,\n",
    "                                                                    epsilon = 1e-5,\n",
    "                                                                    name = \"batch_norm1\")\n",
    "                self.conv1_out = tf.nn.relu(self.conv1_batchnorm, name = \"conv1_out\")\n",
    "\n",
    "            with tf.name_scope(\"conv2\"):\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                              filters = 64,\n",
    "                                              kernel_size = [4,4],\n",
    "                                              strides = [2,2],\n",
    "                                              padding = \"VALID\",\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                              name = \"conv2\")\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                    training = True,\n",
    "                                                                    epsilon = 1e-5,\n",
    "                                                                    name = \"batch_norm2\")\n",
    "                self.conv2_out = tf.nn.relu(self.conv2_batchnorm, name = \"conv2_out\")\n",
    "\n",
    "            with tf.name_scope(\"conv3\"):\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                              filters = 64,\n",
    "                                              kernel_size = [3,3],\n",
    "                                              strides = [1,1],\n",
    "                                              padding = \"VALID\",\n",
    "                                              kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                              name = \"conv3\")\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                    training = True,\n",
    "                                                                    epsilon = 1e-5,\n",
    "                                                                    name = \"batch_norm3\")\n",
    "                self.conv3_out = tf.nn.relu(self.conv3_batchnorm, name = \"conv3_out\")\n",
    "\n",
    "            with tf.name_scope(\"flatten\"):\n",
    "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "\n",
    "            with tf.name_scope(\"fc1\"):\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                          units = 512,\n",
    "                                          activation = tf.nn.relu,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                          name = \"fc1\")\n",
    "            with tf.name_scope(\"fc2\"):\n",
    "                self.fc2 = tf.layers.dense(inputs = self.fc,\n",
    "                                          units = 1,\n",
    "                                          activation = None,\n",
    "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                          name = \"fc2\")\n",
    "\n",
    "            with tf.name_scope(\"output\"):\n",
    "                self.state_value_estimation = tf.squeeze(self.fc2)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.state_value_estimation, self.discounted_episode_rewards_))\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "                self.train_opt = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0c7Kgm1L_ckB"
   },
   "outputs": [],
   "source": [
    "def make_batch2(batch_size, stacked_frames):\n",
    "    states, episode_states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards, discounted_reward_delta = [],[],[],[],[],[],[]\n",
    "\n",
    "    episode_num = 1\n",
    "\n",
    "    state = env.reset()\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        action_probability_distribution = sess.run(PGNetwork2.action_distribution, feed_dict = {PGNetwork2.inputs_:state.reshape(1, *state_size)})\n",
    "\n",
    "        action = np.random.choice(range(2), p = action_probability_distribution.ravel())\n",
    "        next_step, reward, done, info = env.step(action+2)\n",
    "\n",
    "        states.append(state)\n",
    "        episode_states.append(state)\n",
    "        action_ = [1 if i==action else 0 for i in range(action_size)]\n",
    "        actions.append(action_)\n",
    "        rewards_of_episode.append(reward)\n",
    "\n",
    "\n",
    "        if done:\n",
    "            discounted_rewards_of_episode = discount_rewards(rewards_of_episode, gamma = 0.99, normalization = True)\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            discounted_rewards.append(discounted_rewards_of_episode)\n",
    "\n",
    "            value_est = sess.run(ValueEstimator2.state_value_estimation, feed_dict={ValueEstimator2.inputs_:np.stack(episode_states).reshape((len(episode_states), *state_size))})\n",
    "            discounted_reward_delta.append(discounted_rewards_of_episode - value_est)\n",
    "\n",
    "\n",
    "            if episode_num == batch_size:\n",
    "                break\n",
    "\n",
    "            rewards_of_episode = []\n",
    "            episode_states = []\n",
    "            episode_num += 1\n",
    "\n",
    "            state = env.reset()\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        else:\n",
    "            next_step, stacked_frames = stack_frames(stacked_frames, next_step, False)\n",
    "            state = next_step\n",
    "\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), np.concatenate(discounted_reward_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FLp8UieU-0q2"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "PGNetwork2 = PGNetwork2(state_size, action_size, learning_rate)\n",
    "ValueEstimator2 = ValueEstimator2(state_size, learning_rate)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJs-CpSm_NHP",
    "outputId": "4d9cb3ae-4ad6-4613-fab1-d6890a93eefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Epoch:  1 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  2 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  3 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  4 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  5 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  6 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  7 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  8 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  9 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  10 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  11 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  12 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  13 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  14 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  15 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  16 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  17 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  18 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  19 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  20 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  21 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  22 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  23 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  24 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  25 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  26 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  27 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  28 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  29 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  30 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  31 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  32 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  33 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  34 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  35 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  36 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  37 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  38 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  39 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  40 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  41 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  42 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  43 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  44 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  45 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  46 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  47 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  48 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  49 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  50 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  51 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  52 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  53 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  54 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  55 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  56 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  57 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  58 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  59 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  60 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  61 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  62 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n",
      "=======================================================\n",
      "Epoch:  63 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  64 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  65 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  66 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  67 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  68 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  69 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  70 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  71 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  72 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  73 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  74 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  75 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  76 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  77 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  78 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  79 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  80 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  81 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  82 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  83 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  84 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  85 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  86 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  87 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  88 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  89 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  90 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  91 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  92 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  93 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  94 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  95 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  96 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  97 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  98 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  99 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  100 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  101 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  102 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  103 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  104 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  105 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  106 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  107 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  108 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  109 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  110 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  111 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  112 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  113 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  114 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  115 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  116 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  117 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  118 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  119 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  120 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  121 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  122 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  123 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  124 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  125 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  126 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  127 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  128 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  129 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  130 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  131 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  132 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  133 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  134 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  135 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  136 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  137 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  138 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  139 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  140 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  141 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  142 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  143 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  144 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  145 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  146 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  147 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  148 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  149 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  150 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  151 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  152 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  153 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  154 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  155 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  156 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  157 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  158 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  159 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  160 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  161 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  162 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  163 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  164 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  165 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  166 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  167 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  168 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  169 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  170 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  171 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  172 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  173 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  174 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  175 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  176 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  177 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  178 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  179 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  180 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  181 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  182 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  183 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  184 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  185 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  186 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  187 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  188 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  189 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  190 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  191 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  192 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  193 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  194 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  195 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  196 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  197 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  198 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  199 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  200 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  201 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  202 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  203 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  204 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  205 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  206 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  207 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  208 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  209 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  210 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  211 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  212 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  213 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  214 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  215 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  216 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  217 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  218 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  219 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  220 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  221 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  222 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  223 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  224 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  225 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  226 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  227 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n",
      "=======================================================\n",
      "Epoch:  228 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  229 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  230 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  231 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  232 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  233 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  234 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  235 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  236 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  237 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  238 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  239 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  240 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  241 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  242 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  243 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  244 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  245 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  246 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  247 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  248 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -16.666666666666668\n",
      "=======================================================\n",
      "Epoch:  249 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  250 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.333333333333332\n",
      "=======================================================\n",
      "Epoch:  251 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  252 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  253 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  254 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n",
      "=======================================================\n",
      "Epoch:  255 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  256 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.0\n",
      "=======================================================\n",
      "Epoch:  257 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -16.666666666666668\n",
      "=======================================================\n",
      "Epoch:  258 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.333333333333332\n",
      "=======================================================\n",
      "Epoch:  259 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n",
      "=======================================================\n",
      "Epoch:  260 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  261 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  262 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  263 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  264 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -16.333333333333332\n",
      "=======================================================\n",
      "Epoch:  265 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  266 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  267 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  268 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  269 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -16.666666666666668\n",
      "=======================================================\n",
      "Epoch:  270 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  271 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n",
      "=======================================================\n",
      "Epoch:  272 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.0\n",
      "=======================================================\n",
      "Epoch:  273 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  274 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  275 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  276 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n",
      "=======================================================\n",
      "Epoch:  277 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  278 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  279 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  280 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  281 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  282 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  283 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  284 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  285 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.0\n",
      "=======================================================\n",
      "Epoch:  286 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  287 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  288 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.0\n",
      "=======================================================\n",
      "Epoch:  289 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  290 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  291 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  292 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  293 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  294 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  295 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  296 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  297 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  298 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.666666666666668\n",
      "=======================================================\n",
      "Epoch:  299 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  300 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  301 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  302 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  303 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -21.0\n",
      "=======================================================\n",
      "Epoch:  304 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  305 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.666666666666668\n",
      "=======================================================\n",
      "Epoch:  306 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -20.0\n",
      "=======================================================\n",
      "Epoch:  307 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.0\n",
      "=======================================================\n",
      "Epoch:  308 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.333333333333332\n",
      "=======================================================\n",
      "Epoch:  309 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -19.666666666666668\n",
      "=======================================================\n",
      "Epoch:  310 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  311 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.0\n",
      "=======================================================\n",
      "Epoch:  312 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -18.333333333333332\n",
      "=======================================================\n",
      "Epoch:  313 / 500\n",
      "---------------\n",
      "Number of training episodes: 3\n",
      "Mean reward of the batch: -17.666666666666668\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "mean_reward_total = []\n",
    "\n",
    "if training:\n",
    "    while epoch < num_epochs + 1:\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, discounted_reward_delta_mb = make_batch2(batch_size, stacked_frames)\n",
    "        mean_reward_of_that_batch = np.divide(np.sum(rewards_of_batch), batch_size)\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\n",
    "\n",
    "        print(\"=======================================================\")\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\n",
    "        print(\"---------------\")\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\n",
    "        print(\"Mean reward of the batch: {}\".format(mean_reward_of_that_batch))\n",
    "\n",
    "        sess.run([ValueEstimator2.loss, ValueEstimator2.train_opt], feed_dict={ValueEstimator2.inputs_: states_mb.reshape((len(states_mb),*state_size)), ValueEstimator2.discounted_episode_rewards_: discounted_rewards_mb})\n",
    "        sess.run([PGNetwork2.loss, PGNetwork2.cross_entropy, PGNetwork2.train_opt], feed_dict = {PGNetwork2.inputs_: states_mb.reshape((len(states_mb),*state_size)), PGNetwork2.actions: actions_mb, PGNetwork2.discounted_reward_delta: discounted_reward_delta_mb})\n",
    "\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "2tNjtnx6_JVa",
    "outputId": "3744595d-41f0-454c-b81b-d9ff710f204b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9ebgcV3km/p5auu8i6V7JkjewLRtssA2ERSGGgA3BBCdADAQCSQaGbJBlsjBhSMBZSCYLBJLMEEIYT0xCSAIEMv5BCGAw2BhjDJbB+yrbkuVFsrZ7pbt113J+f1R9p845dWrp7uq+t6/O+zx6bqu6uurU9tV73m9jnHNYWFhYWKxPOKs9AAsLCwuL4cEaeQsLC4t1DGvkLSwsLNYxrJG3sLCwWMewRt7CwsJiHcNb7QHI2Lp1K9++fftqD8PCwsJirHDzzTcf5JxvM323poz89u3bsXPnztUehoWFhcVYgTG2p+g7K9dYWFhYrGNYI29hYWGxjmGNvIWFhcU6hjXyFhYWFusY1shbWFhYrGNYI29hYWGxjmGNvIWFhcU6hjXyFhYWY49dTxzDjQ8eWu1hrElYI29hYTH2+Mg1D+A9V96+2sNYk7BG3sLCYuzRjWKEkW2AZII18hYWFmMPDiCKrZE3wRp5CwuLsQfnHLaVqRnWyFtYWIw94hiwRN4Ma+QtLCzGHhwcsWXyRlgjb2FhMfaIuWXyRbBG3sLCYuzBOSyTL4A18hYWFmMPzq1cUwRr5C0sLMYeMeeIrV5jhDXyFhYWYw+ORLKxyMMaeQsLi7FHzIHIWnkjrJG3sLAYe1hNvhjWyFtYWKw6OOf41q6DpVmrx1YCfO/hIwW/z4dQ7jm0iL2Hl5oc5ljCGnkLC4tVx2d2PoKf/fvv4MrvP1q4ztv+6Wa87iM3YCWIct/FhrIG77nydrz383c2PtZxgzXyFhYWq47dhxYBAI/PrxSuc+NDSb34ThDnvjMx+cVOhMVu2NwgxxTWyFtYWKw6yGnqMFa4DhH1ldDM5PUqlDHnNgsW1shbWFisAVCMu1Ns4wVMcg3ZclmyiWIbOw9YI29hYbEGQLa4jMkTVoxyDVe2A6RG3kbcWCNvYWGx+iCpxalB5c2OV/rLpWUctlmUNfIWFhZrAGSc3X7lGsHkVbnGNhKxRt7CwmINQBj5AiYvG+tOmJdrBJOP1WW2JaA18hYWFmsAUWqcWYEmL+vwvTB5a+OtkbewsFgD4BVMfm65Kz6vGJg82fKckbdW3hp5CwuL1UdUEUI5txSIz0UZr8lfdZmNrhnQyDPG3sAYu5MxFjPGdkjLf5Yxdov0L2aMPXvw4VpYWKxHVCVDzS9nRr5jlGuSvzJzj2JuK1NicCZ/B4DXAbhOXsg5/xfO+bM5588G8GYAD3HObxlwXxYWFusUZIsL5RqFyZc4XrUQSmvjBzTynPO7Oef3Vqz20wA+Nch+LCws1jcyuaaIyUuafKnjVd0mbffAsQ6OrgS535Vh98FFsd2HDi729Nu1hFFo8m8E8MmiLxljb2OM7WSM7Txw4MAIhmNhYbHWIOSaAiZ/bCUrNGYKoSTGnitrkP7/B//0alz0F9fUHs+++RX8yF9ei2/cdwD37juGl37wWtyyd67279cSvKoVGGNXAzjZ8NVlnPPPVfz2hwAscc7vKFqHc345gMsBYMeOHXZyZWFxHIK09KLkpSDKHLP1Ha+qRn9kqT6TP7YSIOaJL6DtuQCAw4ud2r9fS6g08pzziwfY/ptQwuItLCwsgMxIF2no9P1UyzNWoaSfRbwZxyu9G6KYI0wzrLrheHLQSiPfLxhjDoCfAvDiYe3DwsJifcDkOJVB2vpUyy1wvKZMXo6uGaDUsDwzCNONhHF+v+OAQUMoX8sYewTACwD8J2PsKunrCwHs5Zw/OMg+LCws1j/IOBcZZdXIF4dQyu+IeIBkKDkkM0yloiAaTyM/EJPnnF8J4MqC764FcMEg27ewsDg+EAnmbDbKMedgDJjwzUzeVNaAkqH6Mc6xtL0oZfDBmMo1NuPVwsJi1SEKiZUweZcxTPguOsbOUPQ3c+BSgTJTNE7tcXEu5JrumDJ5a+QtLCxWHbxKk+ccjsMw4TvmHq9QmXwsyTcmeacKiiY/5nKNNfIWFharDlMIpPK9xOSNPV5j2k7yl2YGEed9Gvlsv8TkrZG3sLCw6BNRXK7JR3ESIz/hFTledSaf/TVp+FWQtxemxj0Y0zZT1shbWFisOoSWXvK94zC0fcfseKX10q/ESyPuV67JtiM0+QG0/dWENfIWFharDjLKRRmvMedwHVbI5HUGL0frmBy11cgz+eMyTt7CwsKiCcgauAlZdI1Tq5F3rGjy/YRQZn8zTX485ZqhZbxaWFhY1EWl41VE17jmzlCSUQbkmUGfcg29JGIOKplm5RoLCwuLPiGMcsn3FF3TDeNcg27d8SrXrFnuw8jTrzm30TUWFhYWAyOLay+OrnEdhk2TPgBgQSo9DKhGGcgcsACw1Ok/Tj6KbZy8hYWFxcCIK0IoE7kGmEmNvNzYW/4d2WGZyS91kxdCUdcpIySNnxyu46rJWyNvYWGx6ogqNHmSa2ZTIy/3fAXyLwnZgbvYTZi814ORjxUjb8saWFhYWAwEPQRSB5U1mJ1KmbzWAETEyfPMYUpYTo2879Y3d/J4RDKUdbxaWFhY9IdYioYp+t5lTJJrNCOvlRpW5ZrEyPci14gmJLFcT97KNRYWFhZ9odrxmiRDzUyZ5ZpcWQOZyQeh8l298WTJWZGNrrGwsLAYDFntGvP3MedwJCY/v6Q7XtXtyEx+MY2u6aWBCJdkH3K42jh5CwsLiz5RqcmnTL7tuZhquTlNXu8RK2vyJNf0IrfIyVWiaYhl8hYWFhb9IarQ5CMOOKmmPjPp5+Wa9G8m12TfUQhlb3JNtr0sTt5q8hYWFhZ9obJAWczhpn7TmUnf4HhV5R5FrumLyUvRNVaTt7CwsBgMYYUmH8WJJg8As1MGJq8XKJPLGqRMnvP6urxaajgx7jZO3sLCwqJPUCx6VZw8kMo1BZq8SIoyaPK0nTqQZwa2rIGFhYXFgCAmX9g0JI2TB4DZyZahrIH615QMpS8vg9D44yyEMhyiJr8SRLXH1iuskR9D/PmX7sZv/9utqz0Mi+MY7/38nbjsytsb214m16iG7oYHDuLFf/F1LHYjkcw0o8k1so5vqkK52M2KmZEhfep7vog//o+7Cscjyz7BCDT5V37om/iNT35/KNu2Rn4Mcffjx3D340dXexgWxzHuevwo7tl3rJFtyQlHuprywBML2Ht4GYcWOkKumfAcJWZd/o0puqYjrStnr37sWw+VjCn5G3EuQiiHGSe/EsRoe8Mxx9bIjyGCMO4pHMzComnIMsagkLejO0ZFIlIUi+ga13EQS05U+RcmJq+8BGo7XrOXTjCCEMpOGKPtu0PZtjXyY4ggyjdNsLAYJcKYN0Y05NBG/bam+7wbxkKu8VJrb5J4iMEXGfO6YZRyUtUoyhp0gggTvmXyFimCKK4dJWBhMQzEvDkmLxterrleg9Rqd8JYhFBSyWAKbYxNmnzB2GLOa7F5bmzkXe+3/WAljDBhmbwFoRsN72azsKiDMGrQyEdmfR0Aoigz2sTkXUdl8vJvTFUo9d/UYfNiRsDV9YO4eTZP9XEmPGvkLVJYJm+x2oj5sOQaXvgdOV6pLjy9AMyOV3U75NSs60vIQijVWcEwdHlqNG7lGguBIIoxBEJhYVEb4bAcrzkjn93oFCdPrDwwyDWRwfEKZEY+jLnCxlcKmnzLDlxZix9G45DMyFsmb5GiyamyhUU/iGNeWIKgVwQlco3M5IXjNf0rwi7lcRmSoQCgnUohUczFDADI16XPxpHNCKIhyzUU4mlDKC0EulausVhlDI/Ja/uRDLJwvKZyDX0nM3m9eQihlRrQSGPyxUaexsMViWa4co1l8hYpErnGGnmL1UPUoJGXDadehTJSmHzy1ytxvNJzoUc7tiUjL29Tr0svtiMcuMlv6CUxHLkm2abV5C0EgtAyeYvVRdRgnHxtTV6Pk08tuVrWIP2ryzV+ZuTl2cGc1mFKHwdPo2smU5Y9jFj5lTBKx7gGmTxj7A2MsTsZYzFjbIe03GeMfZwxdjtj7G7G2LsHH6oFIbCavMUqI2o0Tr5EkzfJNY6eDJWtb8p4BSRNXguJLJRr0r9Uaniqlfx+GOWGhVyzRkMo7wDwOgDXacvfAKDNOX8mgOcBeDtjbPuA+7JAwiy6Vq6xWGU0yeRlQ57T5A2OV9fJWDlQUKCsIIQyimNRiwao4XjliaM2Y/LNP3edtSzXcM7v5pzfa/oKwDRjzAMwCaALwFbUagB00xNT+auv3Iu/vWbXUPd53X0H8II//1phuFkd/MkX7sI/lhSEMuHYSoDXfeRbeODAQt/7HTdccf1D+PMv3j2Sfd20+zB2/MnVOLpiNnRlCBssraFkvOpyjcScM8drGkIpatBn619+3UP4sy/enXsBiRDKSHWkzi0F+IV/vAnfuO+Asr5wvMZJRA05RZsuUva+L92D//21+wGMn+P1swAWATwO4GEAH+ScHzatyBh7G2NsJ2Ns54EDB0yrWEgQN3Z6r33j/oO4/v6DQ93nH/3HnXh8fgWPHFnqextfv/cJ3PDAoZ5+8+jcMr738BzufOz44QfffuBgzuAMC3/91ftwcKGD2/bO9/zbmNevzV6FTpiRh9JkKFYUQpmtc3Chg6/etb84hFKTmQ4vdfG1e57AzXuOKOvLpYajmGND2wPQvJH/yl37cMveOQCraOQZY1czxu4w/Lu05GfPBxABOBXAmQB+mzF2lmlFzvnlnPMdnPMd27Zt6+sgjicEocrkozhWNM2h7DNlPp7TPyfoJ7af1o+Oo8yvmPfWcHoQkPzRjxM/jOPG4uRJrgDKQyiz6Jo00sWQ8QokztSckZccr7LzlDpMhZrWnkXXJMx/up0Y4EFmsybIHa6GFSfvVa3AOb+4j+3+DIAvc84DAE8wxr4FYAeAB/vYloUEcvxkBjCpZTNM0ENBRqHfbfRqTMi2D7PE61pDk4W/qpAx4t5fonEMRIV9nHqDbDj1LZrKGpBcE8UZ25YxvxyUaPJaCGXaYUqPmiHZiF4y0ymTXwmbM/Kcc6Uh+bjJNQ8D+BEAYIxNA7gAwD1D2tdxBTUSIWloMIzYXRmBCFUbbBu9Gq+qioLrEVHMBzrPvYAcmP20tUuYfENGPjWcDssb7MhQ1sDTyhrow4g5cn4GOeNVfnFQnLxOJGibJM+QXLMSNPesLXbVln9r0vHKGHstY+wRAC8A8J+MsavSr/4WwAbG2J0AbgLwD5zz2wYbqgWQyTVAxkqG3WCYbvRBYvO7Ye9GnvZXtwb4egDng53nXqBr23XBOW9UViK5Zqrl5R2vxrIGaoEy0zgOL6rx7zKTD6O8kddDI0nnp+Vk5DsNMnk9Rn9YIZSVck0ZOOdXArjSsHwBSRilRcOQb0ZyIjU1bS6CiOgZwNgGUb0SrzKyKfPxpMk3F5pYBVdrvlEXUQP3gwySayZbbq7wnilOvqzUMCFn5OVkqHQnDgOOpnKJPhuONSY/PQQmr2fbOgPIoWWwGa9jBpm1x3HmGBrFPgcxPv2UYqBDPZ7kmqSpxWj21S+Tp5lGzPMhj/1gJTWkk76baxpiynj1xcupWEbMM3kpGSp9Xja0PRzrJE2+9dkw3etUPGzjBBn55pj80YIY/aZhjfyYIdCZfMSHkoWn7nMw5hanOmivjJEetOPL8Tr66Jp+mTyQj4bpB2Q4p1puzWSocscrABwqk2vizMgTijX5rHgYY0mbvqYwZ428hQmKkY95rt71MNGvkTfV/a6DrNjUcSTXNFj4qwr9RtfI42tirCtBjJbrwHWYIRkqL9dQ0xARQmnYZqkmnx7vlGTkc5o8VzV532Voe46YdTSBouJoTcMa+TFDV3K8kkEYdnSN2F+fDJMexl6jOMh+HF9Mvrk67VVwtXjzuigrKNYPVoIIbc8BY1VlDeiv+nIyjUEvV9BK5Rq5RLLK5HUjn/wlTd5zHEz4bqNyTVFJhaZhjfyYQZdrkk43o7EK/e6GXkK9GoQs4et4MvKjk2v61eTDhpl8J4zR9l04jJWGUOoZr0XJUKZlpuiaMiMvqlmmfz2XYcJzlcStQUEx+sOGNfJjBtXxmoVQNuEAq8Kgck2/mvzxFUI5wuiaPo287EBvItyzE0SY8B0wxkqrUGalhosLlBWBEo1kuaZMk9evgeswTPhOo8lQ80sBtm5oNba9IlgjP2aQb0YKoeR8NGx3ULmm1+gaWv/4CqEc3cylCSbfRDXUlTDChO8ak6HKHK+mUsNFMDlep8vkGu33w5JrNk9ZIz92iGKOd37mVty//5iyfH45wNs/sRMHFzoDbT/neI1HF4HSN5MP+2Xy6Ot344gv3/E4/vaaXaPNeB0wTl7/3C9WghgTvgPHyOTzco0vNQ35+j378ZdfMRXCVUFx8h+86l58+Y59ALKwSKC4rAHBcxjavlsrTv5buw7iNX/7LVz64evx3s/fWbje3FKAmUm/cnuDwhr5hnFooYPP3vwIbnxQrbj4mZ17cdWd+/GRax4YaPu5OPlYjQIYJvplbTTmfguUDbsA21rAl+7Yh0/d9PBY1K5RjHxDjtcJzwVD70z+qjv24yt37Tdu15OSiyhO/lgnxDfTqq1UdAxQM8mBvKbve04SXVODyV93/wHc9sgcDi50ceX3Hy1cbzmIMNX28P6ffCb+/i07CtcbFNbIN4wiZ+GmieSN3U/tbhm5OHkRSz58Q9jvA60XVasLfhw5XqM4SYLiI42TT2vX9JkMBaCRxK2VgOQaA5OXjbxwvMq14YsHQKGWgLnC41SrzPGqDsR3WCLX1IhkCyOOqZaHHz3/pNJrGcYxPIfhjT94Oi4+76TK7fYLa+QbRtZBXl1OU8NBs9zkipNRHIuHYlhGXp629i3XUAhln8bkeAihJOM+yrIGxHN7DW1tnsnHUgilHief3depjYfrMDCW3P+mGSw13aZqlfIyGbJco29Hv1V9z8GE59RKhopiDtdhxpeWjDDiA1V2rQtr5BuG3GxABjl5jq2EA21fjonvSJ/16WZTkA1s/47XPpOh0tWPFyZPLfVGdbj0Ag8GkGuacLx2wmImHxnkGiCRYoKC4nzbNrQBAK0KJj9dwuT18gq+mzheOzWYfBDF8F0G18mHhMoIYy78C8OENfINI/P4axpf+rdJuUY2wMPS5OWQsX53IRyvPW5ARNccB0aejPsoo2tEslmPBGEYjte2b2byQaGRd3IVJQlbNyZGvorJK5p8QVkDgu+mIZQ9MPlktlF8fpL1hm+CrZFvGFkqvrY8vWsGZvLShuVWZMNyTso3db8PdFcw+d5+Jxyvx0EIpS7VjKJRe1YbaHUdrwqTL9kXRdcACZMPC+o2EZOXNXn5c/L/JFqGkKtCqZ3/lls/hDKIODzHHC0kI4xj+FauGT8UMvn0/4MyeVmTV1j9kOQatTXboJp8j0z+OEqGokzXIrlvWPsEejfyjcfJB3ESXaMxea5FGslM3nUZwjg2yzUbk9hzWa7xNGPqOkyp356vJ6/Cdym6pvpcRXEMz2XGuH8ZVpMfU4jKeNrNT/ZtUCYfFjD5ock1DTB5GnOvkw1h5I8DJi80eSqvPAIjrxfhqgvZcDUWQpnGycu3mP5yV5m8k5T0MMg1xOR1eUeG7zhKJyZ9X7noGi9l8mFUmWEbSo7XKk3ec61cM3YQDRW4+aYZVMOUmYvieB2akR+cyZMR6Z3JI/3d8cDkkyQoMiCjCLDJXqK97SxUIrwGG2gYxQhjjnbK5GUDqo9Ld7yGURGTz4w8/cTVHJyuy0TsPB2HfCxmTd4F59UvxTDi8J3spVX0UgijODfDGAaskW8YRanWTU2/gyK5ZiSO18HkmrIb3oSmOxCtZVAS1CiLsvUr15QZw15BcecZk5eMvEYKZNLruQxhzJXZLGFryuQZY1LLQNWYemktGhnyedDv01Yq1wDV3aFkJp9sq3y9YcMa+YZR5DiT/zuIjtktMOzDMvKyJt+/ke9vG7xPpjmOiGO1zPBoNPk+Ha8N5E4QKO6catfIh61v2+R4NY1983QLDkteCowVGXlHFC0jKNnkOSafrV/V5zWUNPlkW+ZzFNkQyvFElgxVrPFRy7F+EBRINN0hOV5lTX7QOHmgNw33eCprEKVOVy5IwvD3KWqm95wM1d/1NEFm8kzT5HW9XZFrXCetwJrff9tzMDPpw2VMZMnqjNlhMBh5aYaiuV69VK4BUFluOIo5PIeJnq1F78HE8Wo1+bGD0N5zRj77PD9AR5iiEMrRyDX9bUMeZy/M77jS5EUyVPr/UTL5HpvOyPfBoNE1KzkmXzxLcHUmXxBd47uJkXdSTd5zmGD0hDDmQq6hGPpSJu/Ick05kw+iGJ7jiAzdomsZxrFl8uOIog7y8sMwSLMANQHKrM83CVl/7Je1BX066vp1DA4LnHP8r6vvw97DS41vO0uGypOEz93yKK6994mh7BPoR5MfXMIjkMFsey4YMk3+4EIHf/KfdynrOnIIZYlc0/IczEy14LKETZt07yjmIoSS6kp94Kp7sefQIoD88+s4GZMv0uR3PbGAj1ybVBL13GwWYTLycfpCt5r8GCJLhiqWaxY7/dek7hYw+WEZQr1JyaDb6MvIrxG55qGDi/hfV9+PX/7nmxvfNl2+yCD3/eanbsFb/+GmxvfZv+NV+jzgjIPu4bbnwHEy4/onX7gLX7jtcWVdXa4pCqH0HIbXPPtU/NgzT4HDmEiE+qUXn4nnnD4LIDlmx2F40w+ehpc9/UQAwGdvfkTs0xQgQMy/qHHIF29/HH/x5Xux0IkUx6vplicyqCdpDQPWyDeMomQo+UIPwn6CKBYOHUW6GRKTb6LVW79Gnn62VuQaMijDmDXpdWRGo8kn++xVk5dfuoOOk85pyyNNPvm/7GSljznHa4lc83M/fCZ+4UVnwpWY/GWvPA//9QXb02NI9vO+n3wWLnraNvFbKiAoP7/0+4zJm408jWUliOC75XINnUPL5McQhclQ0oXutSCUjDDi4mYbhSYvH8egcfJAf0x+rYRQ0oPpsOYfzKyEg5kkDAN0G/Z67zSZDEX79l21DIDctWkyvd/zcfLFcg3BYVB0b2LjMnGQI2+oubZ8WCS7kLxTJNfQfb7UDdUQSsPqtH8bJz+GEEZeu/cVh9IA0ko3ioUDaBQhlGEDRl5JnulhG7Fm+FYbZBS9ITjLdCfzKNs59lzWQK5MOuA4u8LIM6VpiGzkidQojtc0Tt4k18gSiMNUTb4t9XoV60svhbklYvLZ9jImn6xXFEJJpUWWuxE8aQZhem7IBlgjP4aIDI4zQNUxB5EfgiiW4nXNFSmbhNK0ud8qlNIPezHYmeFbG5q8mGIPgcnrxtL0Lmy6aJk4vz3eOwqTHzSDO5SZfFYzRq71PpEaYfm0u46D5a7Z2MrM3XWYUtJA7vVKkGvcUFCEHEJJxrpdweQzuSaGlx4PYCY2NJt3rSY/foiEplos1wxitGQjXxRO2SSaYPJ9a/JrLLpG6MVDYF/6uTUZhkHrHunot3aN4qcZWK7JHJByxqtseCcMco3vsEJtXGfyniLXuKXrzy8n51iRazQmX6XJd9NyBawkuoaeA1uFcgxRpKk20WEJSKaExEZGosk3MG45UasXoyAyXteIJk+neBhMvqjWkYxBQm9NaKLU8KCzi0CWaxgTkph8zUljV6pQOgxLhUxeMvKO+ju58mS2vqTJLyXn2OR4bVc6XlWdv6ysAdkJ63gdQ2TOQn159nkQaUVm8kUlDppEE8Wo+o+uISa/NuQaGs8wmHxZXgVhboAkOhNECGXPyVDDcbzKBcrkWHy632WHt+86WC4wtrLhdBiD78izgrzJ8xW5Ju94pf1mmny5XAOgsqyBDaEcY2TJUMXMLBpArulGsbjZirpENQkaq6lrT10MLNesESZPMtswnGX6eTEZz7kB+wProD30eu802RlKDqFM6q/nx0T3u87kizR5Ga7meDXJNXI0zlI3QjeMNSafrpe+iKrkGhofkQHTOSLiMgom71WvYtELRDJUzvEqa/INMfkRyDURT1uZYTUKlPX+m2EiGOKDmcurMFzO+YaNfCxp8pzzXOp/ERS5pukQSuSji4zRNQ6r5UtgWgilqderzqbnlwOFyZPjlrGk0UgdI0+doYACucaGUI4vikoNyxd6EEdiEGXp2KMoayDKpjqs76l5t0/JJytQxo0ZiKMG+RaGYuRLHPUE0oubglK7vYfroibIDTaGnCZPTF6Wa9L7Xa7lVTeMVU6GAoocr+q25pe7SnSNvN+27xTKNd2cJp98LnO8rvmmIYyxNzDG7mSMxYyxHdLyFmPsHxhjtzPGbmWMvWTgkY4J6iRDDc7kyfEaScuHYwTjmItqfn2XNVB60dbfRpOhek1AMPlhhFDqmryhZHXjmrxkq3ohCUqCXGNx8o5SoCyqlGvqmS6HqSGUVdE1QHKe5XOj/L6MyYeyJu+UljWg8z0OTP4OAK8DcJ22/JcAgHP+TAAvB/CXjLHjYtaQJUPpRj77PIgjUQ2hzDY6zLIGlNjRRJx8L9N72YCsBV1+mHJNkbwnM9phyTVAbz2CGw2hDDMHJJM0eSUjNTXCruJ4rXcN9BBK07UzyjUyk5d+MuEX93lV5RpWWtYgY/Jr3Mhzzu/mnN9r+Oo8AF9P13kCwByAHYb11h2KOvs0x+TzIZRtzymNkPiPWx/DI0f6q5wYxxyuoSnxw4eW8MXbHy/5pTzmLEu3Sqr6t517cXChk+xbfjGucyOfd9Qnf+X7qGnHq3xKdZKw59Bi7vre8eg8rrvvQLPJUFEsJJWkrAFJdPkcEL0KZR3oIZQm6N/PLQXKudFnAkeWuvinb+9GHHMcWeziU999WByL+I0rZbxKG7t17xxu2HVQELRxDqG8FcBPMMY8xtiZAJ4H4DTTioyxtzHGdjLGdh44cGBIwxkdsiqC6nI1c7S/B4P6ULa1ZKgJ3y2cbscxx69/8vt4w0e/3dc+Q5JrHKaM+5M3PYx3fPqWWtuQX0xlTP6xuWW865wPLKcAACAASURBVLO34e2fSKo8yixxLYRRkuY6jBDKIlIgz9aOrTQcXSOdX12CuPivvoFf/ZfvKcv+99fuxx9+/s5GEuQIgdTnVG7kTeej7Tm49Nmn4jmnz4qSwMnyvOzykqdtwwVnbVGWXXDmCXj+dnXZOSdtwDt/9Bzx/+mWiy3TLfz+q84DkJxn+dzI17vtu/jm/QfxB5+7E3fvO4pf+9fv4Xf/3+146OCiQZPPyzV/8/X78cdfuCtLhhqBJl8ZXcMYuxrAyYavLuOcf67gZx8DcC6AnQD2ALgBgFHI4pxfDuByANixY8fq07UBQYZpGHKNMOoak5/03UJNnsqi7ju60tc+Y4qu4arRDUJzBUATwjiRmI6uhKWMnL55bG45+X9Ds5+mQLOloSRD6UbeUMOmqrdor5Dv0fnlQGFhpvtpbqmLI0vdRggLoRvFIrtVDtMNIo5TZyZww7tfBgB42bknKb+bnfKh449+4nycccK0suz3UsMt4yvvuEj5v+c6+N7vvxxL3RD/8wt3YSWMtegaOZlKrXPz0MFFsY78XLuOY3S8dsIYR5a6I61CWWnkOecX97pRznkI4B30f8bYDQDu63U744ii5tN0oZMSqX06MNObqK0lQ034TqEmT4bB77PNWNKijAGxNhuhJhcxr2S2YczRTp1nZY46epjoOJuMx24Cw3SW6YQ4MsgWVR2JeoV8SouculGcNZueXw5wdDlQXgCDXpYgikWBMAamhM2WRZ7MTOaN/KDVQbMqk5FimHUmT5hfDrAglZqQSQ9FCwGqke+GMeaXAyFbjoPj1QjG2BRjbDr9/HIAIef8roqfrQsUOV6TOOSsel4/oIdLr11TJtdQxbx+GQPFyetyjagQWeNYopiL6XXZ+rR9mqGoWcKrL9fQ2Ici1+Tul3SfkcoCm0TMOaZbyXUpKpkgn3fSqueWu0a9uR+EUdbMWo6ukWUcE2YNRn7QCZbjMLTcJERSPirZN6ozeerXrPebVUoNa/fxShBjsZv8zlvrPV4ZY69ljD0C4AUA/pMxdlX61YkAvscYuxvA7wB482DDHB8UGfmIcxHO1W+cfCAxdyAzhhO+W7hNYvL9evGJyTlMjZM3Mc0ihJImX8bII+3F0aQs0AS6Q5RrcveLUa5plslzDmyebgEojtyRjTytc2ihKwzzwJ2holjo0o6javJlxGTGINfUTeYqQ9t3UiafLSuKs5dfjGHMldm05zoiU1a+hvQiOLTQTddbA3JNGTjnVwK40rB8N4CnDbLtcUVmoNTlMU+Mg+eyvssakJGh9OqM2TuF020yDP1OC+lh49xcdrgOkw/juJaR1wtmKSF+a6ASpWlcTSF/v6jnAhgOk9881cIjR5YL7x867ytBJPZ/eLGLlpuEEjZR1sCkyYcVcs3sZCu3rIkJ1oTvYiWIVccrk428XLEyO2cJk9dDKPNyDa1zaLEj1hs2jovY9VFCMDBD7RrGkosaDKjJtzwHLmMKky/W5Emu6e9SR3J0jTTsXhp6JEw+bdZQYiBFbHhE5zD/3WoiM/LNbzvvqFeZ/FSrOAlnkH1Otly0PUcxWNxglOSXwOHFrqj3MnAVylBi8lJnqLBCrjEyeTTA5D0HnSBSHa8FpYrnlzQjL72Ei3q80nN68FjK5Ne6XGORBz2Uetwz51n2Xb+doeTKdY5Uu2OyRJMXcs2ATN5h5laAteQayfFaNospMnTA2tDkAxEe27yVz5GCWN3nVMsbiuPVYUmkimywFqXCX0Qk5JfAwYWOMMxNVKH0veTelDtDJUy+N02+MSYfao5XViDX6EZeeq59qQql6aUpmPxaT4ayyKMwGSrmcFjyhu+3x2tX6qIj68KTvluYsUghlE1o8qbennUYdiiFyZUx/2HkFjSJ7pDkGs55YXQNHffGCa/xEEqe+olmJn1FX56TauRkTD5b1gljcT81UdZAYfLp8jDipcRkqpWPk29Ck6eM1qIQSrnAmarJx8ps2nXMZQ3oOT1ImryVa8YPRclQwvHqsr4Nlgjhc/N9Kwuja4LsxdAPIi6XNTBE19SRa6QErjIDOaws4aZA0/GmXzimzemZn9PthGE2WagtTmeXs5MthZXKrJ3YqZ5t6zlOKuE1wOSFkVdlqjLHq8mgN+EPp9o0pqYhgMrkH5vLck/0zmy+JNeojleVyY9zxutxi6JkKM6T6IHB4uST37Wk/pFAwi6KNPmBQyjTOPg8k0/+1g+hdCrXz9VUlw5pLWS80gPatFpjrm2S/KXztaHtgfNmHdDkJ5qZ8hXDLks3dMx69I3rJEXrBq9CmYVQslST55wjjOOeicmgcfJAYsQ7WjKU/OzI9+jDh7NSISuakSeJEyiQa0R0jdXkxw5lyVAOQxpCOVjGq+864sZzHZbUrqlwvA6iyZcz+fJjSR5YqaxBjega0//XglwjHMINj8W0Pb2/7YZ2Egi3EjanyxOTn5lUjfzcssHIp4Z/YzoOlzE4TjNlDXwpugZIXqJhBZM3oQlO3PaSEEpTI28gI0069AYmnps1DVHzPZL/UOtCK9eMIbI4eXV53IBck5VlZYqR912nuKzBgHHyYZyM29Gia/SY9iJkNUjqJ0MRFMfrGjDyw9PkTctUuUYY+QadrzwlHrOTfg25JkmAOnV2EoDM5AfU5LXoGiApb5Fo8qvD5PNx8tk46HnaOKFGn+vG3ysoa6CTMet4XUO467Gj2HNosXI9uZ78Nfc+gZUgwtV37UcQJp13PIeVTrnnlrr49gOHjN8FkuOVbmiXJUY+irlgybfsncNjc8u4ec9h7E2nlPTAfPP+A1johIatFx+P5zK4WnQNMc3H5pbx/YePFP6ejHoWXVOfyUepszr5PDy55p59R7H7oPnafvehwziUVsUMDUaec46r79rf8+zswLEOdu4+DMAcoRLFHPfvP4Z79x0DAEynRr6jOV9v2p2Nr1cQ8Zid8rEcROiEEQ4udPDRbzwg1gmiGHsPL+Fj1+/GxglP1IwRjWQGzXiNszh52SiGcXkIpQlNFDNvC8erxOSlYdBL9qRNE8rvckxe1uR5ZhN0kmNDKNcQfvxD38RFH7i2cj266R85soSf+4eb8JFrduEX/2knvnHfAbhp2dOyB+NTN+3Fm6/4Ts6RA6j9MInJew4TIWgUtfP2T+zE33x9F976sZvw99c/BCDZ74FjHbz5iu/iNz75/drHHaVMvkiu+fA1u0TVSBOEka+V8ar+n/Os/+Ywk6F+599vx59/6W7jdz/1f76N137kBmUM8jFcdec+/OI/7cQV6Xmui9f93bfw+rQyqGlmEHPg5X99Hf7kP5NxTRcw+bdc8V184sY9Pe1b7CNOdHCqAzO/HODTN+3FnkOZ1tyNYnz467uwHEQ456SNOOekjQCA07ZMwnVYQ3JNpskDZOSTEtdleOMOtbBtE5w40eQTJn/6likAwKXPfpL4/seecQoA4C0vOEP5nX5d5KYholSDRlQYs47XsQQZNYo1PriYOFgWO2Eq1xTr57ReGHNjJ3q9HyaQ1dtIvk/a5B1c6OLRuWVRV4NAcsNtj8zXPh7S5IvKGswvB8axEojh9pIMJf+fpvKDhuqVYbET4thK8eyGHGyZXJN999DB5LvDi7215tt7OKm0GUszMBn6skyuye4dzpP7pE5DaxPITzQzlZY2WAqEsbryV18IIJk9LnRCbNvYxr/+4g/hjy89H7f8wcvxNz/93EbkGjkZStHkIw6/wgC+//XPwu73vVL8vxG5xssyXs/aNo3d73slXvr0E8X3Lzp7K3a/75V4ywu2444/egX+/VdeAABYDkyOVwozTZbpRGUUejxgjXzj0GUFml53ozhNhip/MOhGMDl4AoMm76URO0DywCx2I0Qxx8OatBRGsZgddHrQdSmUzXWYMW59Jd1fEXJMvjROXpVBYp5N5QcN1StDEMXGkgG6oTWVNVjoJPo1GeFe0Y1iYwilzpCFXCPdF/S7fqO1KEFvVmLyyYuVif0FEUcnjLBtQxue64AxhtmplpBrBmXy3YiLKpSOxOST+64389RICKXvoBNG4KieGWxoe5hqJedJJzpJFcrksyhRkd5jtHwULB6wRr5x6DaMoiHCmKpQOqUPJRkSXXtNvstCKOkGcRwmHpIgikXSyiNHlpXfhjEXLK2XCA3ZyMuGlm7c5SAqjZWnFwDJLnWZfGL8MiY/zOiaMOJGh6Z+nYxGPp0BbJjoz8ivBOaXpL7vjQYmT07ZfqO1Ys7hOFnZ3rmlQFxvuezzShArNVsIjTB5KVHOkZl8HNdu8Udogsm3vaQ3QxDxWtuj51AnTqayBnT/nJAWheu3/HevsEa+R1Qlo+SZfJQu54LJl5UCoBvBZHRUuSZZ5qXRNUBiGClKQjcSkWTke9G3k2SoRB4yMfmlblTreKioWqkmL53blSBhuORv6LdyZx10o9h4vvWxUraivJwksek+mXxynPlj0+UjkyZfN8KpCEmcPBPO1LnlQHQCk++plSAyNsAepO8vQdbkZSYv+hj0gKaYPJCc5zoZtDTGnCbvZESMri/JfVs3tJPfjiCyBrBGvha4ZnzKoBsGeX1iSGUGKzPyJiafz3h1NU3+aEnJ2H7S4ikZKsfk0011wlg0Dyn6PY25SqqSt9FJGW5TNVLKUHRu9H2aCpQRk+/1cSX7oWdXEo4sqRo/zRTkWZgw8n2+ADO5Jis3TEy+Jc0OV0KzkW8+Tj5jvqF07euiiQJldJxL3bDWS8MTRl4tA+5JtWv0Vo7bNrbT31omv2Yg67VFzRUIeSOfPZR1moZ0U7ZoklS6CpNX4+SBVK4pMPJRzPtKpFEcr5Id1A1g0THJDYudium93uqOy5r8EOWaIIyNPhDdfyAcr9JYKBy1V2MnjENolmt0R+6GNnUtyi5CE0zeYUnMN2PA/FJXGHm6pxIpazhyDec8zXilzlDZ8jBt8N0LmilQloxluRvV2h49h6TJk0avlhpO1iVZbduGtlhnFLBGvgZkQ11Ud5uQM/KS8UhCEZ0Kx2uJXJO+APSMV5rudsO4tC54Lw5XQhZCWV4wrOiY5IbFlUxenjGFEaIRafJBash0yC+yrtTTVnW8Jka+V0PrSgzQ9H44ohl5Mh5muaY/zYQDItFt04SPueUg7QTmiHsqcUpHxsbZzoC1a4gAyJ2hgCzjtec4+YaSoYDEaNeZGVAy04ow8m663JGOR5VriMlbx+sagmwAijroEHRjJDtQHZYULqqnyZvlGnKCKslQ0tS6aHyJJp9ts25IolLWwOB4FWMrOCa5YbFTUbdHnimsBBHiODMAwzLynPNCTV6+TvNSb1OZ4JN23uv4aKpe5Hg9rMs1Q9Pkk8+zaf2aKOJwHWiafDGTH6RgmuxjAiCVAeAiCa8XNGEz21JeRh01xdWY/LTE5GUfA20TyDT5Xh3L/cIa+RqQp/KVTF676fNMvp4mXxRCKViPQZMPY14oJ4UxV7ZZFheu/04UKCtj8gXHJDcs9ipC7uRzl2j9w2fyoWQo9SgV+b01v9wVIXCyYevbyKfXkY5TR16u8cT6BFGOuE9NPo4zuWEmLW1AjnYhAYaJw97I5AeUa3QjT0w8omSonkMoG4iukXwPvTheicxNpbIakTEgIy+BZfJrFyqTL9fkdQO+ojB5ViOEkud+R5BrbxMJUDT5MFYqCCrjilXnYpVvgRBzM5PPRZ4UMnlyvCYSU9mxyy8Rcki2amTKDoKy9noyk59bCsQxymOhOPlBmHwdIz/VdsGYGqpH91rfcg3PykYkNeWD1NGeGakgitEJ4gLH62DRNUKu8VRNno6rKhlqGJiQXmZ19k6GWmfyvuMUxslnTN46XtcM5ClylVyjP7Dybx0HaXRNfyGUodQPUylrQJp8GkI5mT6Qk9KDGWmx4FUzkmyfsbEYlW6XiowcHSu9KMpkIt3xGsXDT4aSm63o51xl8plcQ8PkPJPAejfymZZr+qk+0/IdJ6mQKL2I5C5K/SDmGZOfnWrhaGrk6QXkuwwrQYRuVCDXDBhdk4XXqiGUtHxUIYYy5OPsJU5e1+RdN5NrdE1+qu1iquVaJr+W0Itx1B+4jsbkq9hsVZy8XrHPUaJrOOaXA5xxQlJzg/4CCdNWmXw9Ix/zTE83FSgjFElQIoQyfVGUavJcNbicY+hyjVyHX68JLjPkQ4tdpfgcoGY59ut47dRshu04TFRI1PfZbwglxckDwMykh7k0uoZsj+86wrFsjJNvWK6h/QYSMRg1JhS5pnp9YeRTKZTyGVRNPllXzFwcB7OT/khqyQPWyANIWNpeqQGAjpVQNY57Dy8VyiI6U5WNCGMJ66aKkXc9dhRAclNTtcGuKGsQ4+hKgK/cuQ/37T8mtkXJQSqTTy7jffuP4dsPHsKTZpPiUVRgCciHUH5r10Fcc+8TCKIY337gEK66cx+uv/8g4pjjzsey2jZhnDH5xW6Er929H3c8Op87zoMLHTxyJDuH++ZXcNWd+7A3Xea5DG5FmWVdrol4lvI+PE0+uz4PHlhQolpklnpQqvQYc46b9xzGF2593Dh2se0oxj37jhr3K6IyQrNcYwJ1LdL3qcs1N+85gqvu3Idr0+tbhITJJ59nJ1vpbCWWmLwjkr0mvLypGLSsQd7IU5QYEYPRmyfZyPfC5Je7ybFMUnSNk8XJ070rjtdjmJlqjewl1l+a3jrDa/72W3jo4KJS7EiG/GAdXQ7wX//hu7jonG34w1efn1u3jNG5LKlNHUYcV1z/EP70i3fj33/lhXjwwAJ+9//djpsuu1joditBhI9c8wA++o0HcOLGNr572cXohFkK+OY0NXp2qiXkmg9cdS8A4MmbJ3HGCVN4+imb8JW79gOgMMHkOHyX4fLrHsTl1z2I3375OfjLr94nxvg7lzwd7//yPfjqOy7E2SdtRBxnTH5+OcAvfHwnPIfh5Bm11OpffPleHF7s4qp3XAgA+L3/73ZcffcT2bE7DnynuIMVoDH5MGG4pMsOq/2fLNe8+Yrv4o07TsP7X/+s3D4PL2TGf//RFfzk331b2Y5pfB/6+i586Gv346rfuhBPO3mj8p0i19TUtdu+o/gNTEz+wLEOfvLvbhD//z9vfh5ecf7Jxu1xKbpm06SHmCcyETn1fZcJ2ahtYPJViX1V6IqQ4HQQ6R+6R0ZRa12H3MO1FpNnalmD0zZP4YTprLYPIFWhlF5qZ26darzDWBGskQfwUFpLnEvTVxmycQxS3buIyZcxTidl8mEc456UuT9wYAH751cQxRwHFzqKXHMoZZVPHOsgjpNsVqoz8sHX/wB+9SVPwZlbp0UrMQD4we2b8Z5XnotOGGPCc/GLLz4Tf3/dg/jQ13dhqRNh64YW/uPXX4RdTyzgzVd8F7vTsrL/5YLT8c83PoxH55L/k5wTxnHSBUg6LUmkjmqZDix0FClLr1nvOQybJv3CjFxAzSQNo1i0TKzS8geB/tJ5/GjWt1M2YLI0czQ1fP/jFU/DJc84GRf/1TeMjPb2R+YAAHsPLxmMPDlezdE1QGJw5POst440hVDSjOMXXnQmrrj+ISyW9A6gevJAxqZXwki8gHzXwcJKcr3MmvxgcfKZlKcy+UyuGT2Tb8lGvobrVXe8vvWF2/GzF5wOxvJyDRUIbLkO/vINz26kDEMdWCMvYTmIRNKJDHrQNk74SfGiMC5kpFVGPqnBDUy2kptpqRMKgzq3FEghlGqkzLGVEHNLAbZuSBj8ZMvF+afOJN952YN8wVknoO25IuSt5TmChS12Q7Q9F6fMTAqnLDmSt58wDSCL6gnCOK0EqYaDiXOi+QyWu5Ex8oPguUndcj1dX4ZsyIMoFvpwlZY/CHQ5Y37JLNeYop1O3zKFp2zbUNi3l6b+pkxjesA7adKXCTOTPp44lslEeutIU6tJetGee8omAOV6vex4JcPeCWJh6FquI5j8hCGE0nMcLHXrN6DRQdFKrpYMRbPZ1dDk5aJhdXbPND/bhO9gkrnK7/U4ed91hKwzClhNXkKRU5WM14a2hyBKDHyR1llm5BnLblwywovdSOx3bqkrhVBGinN0brmLueUuZtPa3zLkUCxi+jJon4udUDAyMkDErPVkm26UOQTl5CuC/pJbDiLFmJk64MxqDaN1qF3tk1LDogLmkDpD6ddRPufyMZjyFui8Fs006BybK4pmSW9FCUVUOEzsT/Np0PjkY6AQ3xNSMlAasirJNeQE7ISReKH7spE3yTUDtLIEpGxojcnTvTWq6BMZ5PMC6hc8c8VsiClKQK6sQRyL9UYJa+SRXcwiAyT3daT09qJKjmXTV4qTBzLtb6kbiv2S44v2Ob8cCFY1v5xIRCYjLt80RiOf7nOhE4qHlbR9ipef0krZBlHWqszI5DW5ZrkbIYh4Yaq957BcL1EdMnPupklCxJQGrXZYhByTl+4B1RGcH4AnchaKmHx2jXWIevxBVHhsVDhM7M9RW0fS+ZINLY1/63QSi10WQ88lJi+ifcKsZozvMSG7tQ1yTVUryyoEmjGn55C2uRqavEyY6pYull+KpuV0H5Fc4xuc2MOENfIANqQSTZEBIna7ccLDSlp1sZDJl9z0cp1uV7DrSLAv1cgnDH97GgZ5aLGLoythgZHPLqOJ6dM+FzpZNUHHSSoN0jFv1JpSBFGmFZuMvP4uI02SzlWRXHN0JaisWAkkxinmiQEdJpPvhupY5pcDKWolz+RlzVa+liZGS7M1E3mQk96K2PCmSZ3Jq3WPsmSovFwjmHxFk5YsXFIy8pJOXxZC6VXUYapCVteIjPzqa/KyRFQ3g7bIyBfJNa0RhU6KcYx0b2sUFNtalM26kk5hJ30XyykrKzTyJUxelmvIGMpMPtHk04c/jDG/3MXpWxKt/JE0xFOfwgO6kc9/7wojHyjRAxOeI+QJvV55EMVZtqpBrimCMPIGuWZmqgXOi0sq0LljDGJW4LDBHXxl0K+jPD49pBMA2tK5JqZZZORpmYk8CCYfRrXlGspAJWTJUGqNHXJy69/pkB2vVEKgE0TiuOT7yqTJuy4rzHSuA3oBiQY4epz8KjB5xrIyIbXlmkIjr8fJx8b1hg1r5AFMp/Umipl8jAkvqeex1C1vvFGmgTqMwXVpCp9sR9bk55cDoUceWUz0eUpooubKZUYcMMs1xJQWO2pd8AnfFVNIvYdoEGW9R6kKJQBsmc7PFGSQjKMbPdfJ2swVlVSg/U14rpBrRC37ETlegYx5hwa5RmbyWSVQx/gSoheDKfGMjG8nKHe8ytCreJpCKOeWA8xO+YJMlGvyGVsVjtcwFsZJZpxFcs0g1yUjEaom31lFxyuQr4pZBboPWtpLKVfWIIoFaRklrJEHsGEi63FpAnXG8T3ZyJsZTCyl4utwJCZP8sbR5UAYgSNLXbHdfWkoHxl5CnU0GXEZs4bviaUtSI5XQJ2C645Xhcm7TLCREyqMfMbk1fPjp3INUPwypVM64SfNzuM4LXPcQJu5IpiuI72E9AxcQDXyoo6QY5bpKInOdF/RuaUOWCZsmsjLNbLRNslK80sBNk36WaZwiVwj166RjXwWQpkZoyK5ZpA4eZLgPD26ZhUdr4BcS6c3uUbPYDWVNRg1iweskQeQTcGLUv07YVKgqeU6Iu64G5qNfNLRxnxzUPs/IDMa+4+uiG0dXuwKrftAGjp34sY2Jn0XDx9OYvlnJsuNrK7jAtnD2k1j5wmydEMhXcSiyMgCyU18LI2XrmLyxHj1h9911DZzJpBRTfpsxkr45vBCKPPbpZeQbCDpvJiYvFfA5Ckqy5RTQdvuhFGhj4LCbAl660g6J7rjdXbSzwxmBZMXIZTSPesY5AdTxmtVK8sqCAerMOapJh+ujqxB0MssVEGOrjEtF1Uow967XTUBa+SRsc4yx2vbTxopyEZQBz2sRd5zx2HiYaIZATF0QE2dJ8xMtjA75Yv1THKNjKJenIS2JtcAibGn2YcIoQwzJu8yJtrcbTY4dmXIjctl+K4jxl40Y4pTDd73kqiNiFPz8+ElQ5XJNUYmL2vyFY5XYvImeYq07LJkKP1a6hmmero87Wt2qgXGqN1ilSZP284fl2LkhxhC6WlGNdC0+lEj0+QHc7ya5JpRh08CAxp5xtgHGGP3MMZuY4xdyRiblb57N2NsF2PsXsbYKwYf6vBABqkoGzPR5F3lIpoYIG2n6G2dyDXJd8vdzJgCCUM8uJA3BjOTPmYmfbFelVxjgvwAq3JNFjNPCSlys+9IYfKpkZ8u338WXROLiny0DZqFzBckREU8az3XjZL4cZdVFzYbBGa5Ri0f7DpMynqWjWFWEdRo5InJG+4rNYSywMhrzk69dWRRMhTdI55bXHaAcw5u0OQBKGUNxFgKyho0EUJJ+9bj5FfDIMr77dXx2vJ0I8/AmFrWYByZ/FcBPINz/iwA9wF4NwAwxs4D8CYA5wO4BMBHGGOjS/HqEfQgFDkEO2GECd/RjHzeONDDVqzJs5wmTzht86SoIS7fXLNTvmLY+zPy5oeVPk/4jqLJAlnGKaAaeVOIpgz6fRhzEbFDY6jS5EmDb7kOgpAyXilLeDhGvmswUvQSouOXywsoIZQV0TUk1+jHS71NASpQZh6b7uzUW0fSTEM25PNS6QvPKe5dQKczk2uKmbzv5kNoTePpFXKTdwCiExMRml6bhjQFvWBaFUwzH4LDMn/WWGrynPOvcM4pHu5GAE9OP18K4FOc8w7n/CEAuwA8f5B9DRMk13z/4TmjfkqdcWRmYTTyXI371SEnQxGTJ1BZASBzggKJkSeZY6rl9nWTyHW5ZR2e4rgnfNds5KU4+aOpJr+5Qi46uhxgsRMiirlyHIwlcflTLdfIbOeXksgiYvJhnJRUoOiaug6+MG2BGESxGLOMlSBS6rkEmm9l0s/GJ17aRUae5JrUMbz74CJ2p3WQDi10hH+iE8aiPlIYxTgi3WPltWtUXqS3jiQphpY9OreMYyuhl2sYJQAAIABJREFUuF8SJl8QIMApcir5v2zEhcacHmtRvDrVYeoXcrIdkDk6V7PUMJC98Oru3TTzEd8x9WWss/1RoMk9/jyAL6WfnwRgr/TdI+myNQm62Za6EX72ihtz3y8HESZb1XINOdMK5RopGWpJM/JPPWmD+EztwTa0PUz6Lk7alFR8PHmTWvmxLuR6HJMKk0/lGi/fwEAva/Cc0zcDAJ40O4Uy/OanbsH5f3gVwkg18oTZtAORjDCKcdEHr8Gnb9qb9Kx1mUg+opCzukz+UzftxUs+cA2uuP4hXPLX1+W+f+/n78TbPrFT/F9+WXupc5iYdygxeUJbMfKZXHP7o/N4yQevxUs+eC0+fsNuPP/PvoYHDiyIdV/6wWux0AnxqZv24sXv/7pYvtItLjWsFwXTX3Y09JgnpOFF6XZP3DghjqeIydNiMlCyQaV7YeNEcv2KnO29vHxNyBrKOMpYlg3SWBmqIr56hWhiUvMlU83k0+gaKXJplKgsUMYYuxqAqVbpZZzzz6XrXAYgBPAvvQ6AMfY2AG8DgNNPP73XnzeCKOa46JxtePDgAh45spz7/uhyiKdu8yrlGjJMUwXFhxyWGYmFToinbJvGH7z6fGxoe4LpAcAvvfgsnLixjVNnJ8EYw39/+Tl48dnb8JRt08btAsC33/0jxj6cgMrS5OgbWr/tO8JRJ+qhhJkm7zkM7/2J8/ALL9pe2RmLEMaxyD+QsclQ2oA6WgGJHOW7jmDBlPFaV5Pff3QFR5aSmv/7j+Ud2Y/Nr2D/0Ww5Xcdvvuul2ND28NP/90bxEhJx+9KLUZbiXEmuOSDt64YHDopz99rnPAmLnRBfuWs/Fjsh9h9dwWL6gp/0XRzrhIXhuBO+i++852VKiJ6qyWe/O7KURGa97Okn4nXPfZIYV5ERJsOT1a7J7hH6/GsvfSp2nLEFZ241v9hpPEXVW6sQanINvVQeTZ/BTZP16id+/bdfgmOdevdlHWQ9Z+utn8XJm408vcMT2Xf0qnXlWeScX1z2PWPsrQBeBeBlPEvdexTAadJqT06XmbZ/OYDLAWDHjh3DEV4rEEYcWze08cwnzeAj1+5KtGHJMM4tJREL8lTL9GCSYSqqMOcwJqJbFjohTt8yhYvO2QYASrOK2UkfLzv3pOz/Uy28/LyTUIZTZiYLv5OnkXIcvczkASjGNIwzJu8whrbn4qknbsQte+dKx0GIOcxMfipfbliu6e46iayzmOrivSZDkdNuqZs4NKOYKy+5lSBSrh1p8k/enLxQZyb9XDKUzN6VOHkpukYu2LZHipjaOOHhhU85AV+5az+6WvXSEza08MiR5UIfxYQ0iwPyrSPlW5Dq47ziGScLQ9KTJq9UX0yWbZrwS+87YqVRzPvKTpUzqoHs3nz4cL2cEMLMlI+ZChmxFwgj32OcvJnJZ2RhJTC3URw2Bo2uuQTAuwD8BOdcbq30eQBvYoy1GWNnAjgbwHcH2dcwEcaxSNaJObAgFZSKYo5jnTBlmLImz3Pp6KIFmKFcMZAwA7rIkRZPL4dGNu2cUTJip2QjnzF5fb+y41VheT1MN6eNck0r5+CWDV9Scz9j8r0mQ9ELg3R3/WXcCSJFhw+ipBELMdHZKV/4ZeS4fYI541U9J7KRn/Bd8ZsgipUX2glpQ+cji2aHv1GuKWDyC51I7I+QhDjW0+T7ucZ03P1GPun15Mmpv+fQElquo0iLowT5InrNeDW96GTH68oqMflBrcmHAWwE8FXG2C2MsY8CAOf8TgD/BuAuAF8G8Guc83yt1jWCMErYHhlAtY57AM4zGUGGrstTyFwZk5cvclGJ4Kar1MksbcbE5P2MyRO6YVaFUo4y6IWxmZj8jEGukQ2x6yRGRtfk6xqSQGLy+raBhE3JETVBGCvHNDPpZxmv6W8VJq/Ursk0eRly5BSVw0jGwpXxbE215MMFIaV6CCW1jiTI52RhJd+mz3NYYTJUZuSLQyirQCSl3wibUEuG2pTKNctBhJkpvy8JqAm0eg2hNJSBIDCWneuVIC6UVIeJgZqGcM6fWvLdnwL400G2PyokWaqOmC7OLwdCayKDNDvl55ylQRQrzI7YZ5Em72pGXv7tjMLkm725PWXGkDmpJqToGkB90OUqlGqiTP0XUJFcozteFSPPGHxP0uRTuaYow1gHbWtRFJLLz7bk/dG1z8bXkpKhkmVyKGNRdE0R2r4rGXm1DwFVijyyWCzXyHC1MgKycc1KAktM3nEKyxrQT0WcvCGEsgoU4tiv8zWMk1ouwvnrOtjY9nCsExrLc4wKvYZQlsk1ctBAJ4jGT65ZLwjT0D1THDcZpNkpP/emzrNEcrya352Oo7LCIibfdClS+aFVNfnUyFOonBYiSg+vbNcHlWtmppLELrlvrnweHSeJk6cYc6dHx6vQ5DtFTF7X5OPcdVgJ4jRJKQ2bdAuMvJufpuvnp+054vtupGryW1O5hvIj9FPb1mZ0esiiychPaOMrDHEUmnx+3L0y+X7DKMOY50gDkZ1+8kGaQuZ47dHIe/n15egaCsUeNayRB91sTLBcOYKEPs9M+rmLqHdHyoy8+UKynFwjx6+7QoNsWpOXWZo8NmKo9Fd+4BRNXlreS6q50cgbXqRyTXfqg0tx6b2GUBJzJyavzwA6ocqmgzBWqgfOSLM5Wq1Ik9erJwKZ4SZQzSPalzyzEJp8KtfoBa7yTD7Rd8mRJ5ddWDTUfS8PoVTlGvm6jkqTJ3Ilg3xTVeU7hgl6Kde908uYPFM0+TF0vK4XUHRAVkAr00jn0gdwZrJVqcmTYSoLoZwoYPJAdmM3buSlB0lmJ0KuSf/KjLSb1o4BAHk4vYzNdB6o25F8jlVNPnG8dsJMrtFrtpQh1DR53QAlTJ6rqebSNZHr6xCTl+UauZ68iQVTjgNhQpFruOL0pfhuatjuG2YBMvTywbIUY2ruUSeEknYp51KUyU8y6Dd9G3lDVA69ZKsK8Q0TejvCKpSHUGZlDTrBeDpexx6cJw5G13GMLHNekmv0qaWeTVgp16QZr0XJE7T/lmHaNwiK2He7xPGalBXIp5f3wuRNjHDW4NxW5Bqmnhfqel+fyZORz0fXcM6zKpmiuJemydNLaElm8nm5Ru7n6ZQaeUfM2II4VqSNCd/FdMsV0TW5UrXa+aPvaRtmJq86houkFF2Tl7Oi65YTEEy+z96MYcRz9wid/9Vk8m6PjteyZCg5/Ncy+VVC1kw4kVLanqPEcctJOrrxzem9lUw++f2EkGXMU9Xm5Rrz3SrXrgFUNpfINclnmdnJY6bFRTeu6TjEi1Q6x12NyethinphrjJQ5AwZc1mu6Wihk7RvU2etuaVupskbjHyRvLF1g8pA5cJ2QahG9pBEmBW2K7cqOSYfV8s1RZEvvCS6pu7t57nqeHpFwuTXniafFUyrtz6dw7LaNSR/mjpsDRvHvZEX9TPcLE5aZ/LTaUkD/SLq/UHJWThl0KIB2cjn49IBSNUDhxdCKSMz8gYmH8lMXjYA2WeanhbFM5tYv9C8FSaf1+Sz/6cPSu04efXFK7+IO4GsxWdyTZEmT74a+fzRMcsvREcx8gZNXsTJq3KN5zIlA9lznFLDIpKPoryRP0bRNZ7K5IuqRIqyBgbJqS6Tp/PSbwhlFOfT/On8ryqTd4jJ17Pyol2iYQZOIZQ0y7dyzSpAsChKyNCSdeaWAuGQzWvyBXJNwYWk+5kceTlNfpL2M7wQShnkH6CwOzW6hmdMXnoQ5TG3PAeM5QtpZeuWyDUSk5cNH2nyBKrc2WucvHwcBIq9B7LZg17+dUbW5NPSx7IhIv3eNUTU+C7L1dv3XIaiEErPcZRoJ6+g2iOBWkdSLfpBmHyTjteisgxVCKO8Jj87ufpMvveyBvkXP4HKGtDM0so1I8L8ciAeeNIT6YaVk3XimGPPoUVxw1Ub+TgxegUXkhgffa9XpCMDOMwQShlk3In96XHyYQWTb6eJPkUvERMj3ND24DoM+46u4MhiF/NLgdHxSnAYg1NirHTkjXz2/5VAlWvimOPAsY4y/o3pLGzPoSVEaZIcGXTPYUK6UpivNF3X0+u7YdYooqsbecnZT78vY4/kmDXVkV8wMHm98bcMvXYN1S4CRpgMZQihzKJrVs/xSvd43erWdPsUVqGMMyY/dslQ44Jv3HcA//3Tt+C6d70UX7pjH975mVtx0qY2Lj73JPzLdx4GkLHYmSkfe9PaGb/9mVuxc88RvPjsrQDyxtcUQjnhuYXRCULD9sya/NYNbbgOa/xGoAfpB06bVZZThiH9LQ6hNLO8jRN+EplUYBRMyxlj2DLdwhXXP4Qrrn8IAPCD2zeL73W5hph0/do1XPu/ZORDNTb/tz59C+7bv4CXPm2bMr6tG9r4xI170mP0lA5QplovcgjdtlSu+aEzt+A7Dx3GdNvLQiijvCY/o8g1rFSuyRydVF9INvIR2p76ktAzZGXotWto+2HJ9SwaT7+NQ8I4H0JJclfTlSV7gaed5yoIJm/IVHfS8F8ilUUEcJg4Loz8A08s4NBiF4cXu3j4UFLtcf/RjjDwgFQ/Y9LHHamUsDtd9/deeR6AvOZmCqGc8J1CJuRWaPJvfP5pOP9JmwrLIvSLlufgk790Ac47ZZOy/KxtG/Cxt+7Ai89OjJwu11CjkA0Tal14ihj4pRefhaefshHv+uxtAIC/+9nn4rZH5/F31z4gtvfVd1yYe5D/5qefg3sePwoA+PMv3YNdT2QleXUmnzB/cw9VE3JMPpSZvGrk79t/DADwnh8/V/nNh3/mOXjT5TeK8dC94TlMXFtF3nAzI3/hOdvwf9+yAxedsw3X7zqA552xWUTPBFqMvlxKI9mOA4cxXPPOlxgZuO7ojLXoGmOGbJVco4XHdsK4hxK7g2nypuiai87Zhr9/yw6cf+qmgl8NH9lx1ZOh6HYtcryqco1l8kMBMbhOGIkIGB10s8lyzUoQ4+JzT8LTTt4IwCDXhAYm77uCHbXSNnaEfHSNur1NEz5e+JStvR9gDbzgKScYl//I07Mqg0rtGq38rwwy8idtauO5p28W5+7MbdMKk3QdhrNP2pjb5wVnnYALzkrG8+FrdimObpepRn52yofr1DckejifbORkuaYbcswvB3jD856cG+MFZ52A07ZMYu/h5dTxmrJ3N3OMqs7hTJN3HSYqN9K5JeOs165JSmlkjLXlJjOFM7eaS0rrxifUNPlchqxTnPEaFzD5ZD+9ORz7zXg1Va/0XAcXV1RcHTZ6jRoiJl8UJy8zeet4HRLo4aZ0dRPows5O+VgOInTCKFdrQr+I+s2dxMG64s2uT83o2aEL3bT2Pij0KpTzywE2tL3cy4i0YZ3V+q6jhJnWCQVte67yMDmOel4oP6FuLLY+u1I1eZXJy63ydJCkJnfz8pzMMWpyVBYdLy3vRnHOySxr8m6FXEP7oWPUk6FMGbLFtWvyy+nFVTcZqldZQ0dg0OTXAnrN5K1i8rGUnzFhkHSGjbV3hocACm1cCaJCIy8cr1JpgxUtQy0XQmmoQinrovpDx3JyTbNRNINCT4aaW+4ajaAwdFpxq5YWZloncSrfx5QpstimSV8p11oF3U9SFCe/2Amx1I0KQ/Xkom0Zk88SoDxDCGXR9ZSja+R7hspbA1moaJlUQudZOF65buQdbf3iKpR6nDxQXDq57nh6hSmEci2g15cX3Qum688YQxTDhlAOGyvCyMfKlF0GPYhyHHdHy1DzpCgLwCzXtP3M8apPn+nhycoIrK3Tr0bXcMwvmZkunSt9eq/nEphCynToySGOJtfMTPrlhbY0lIVQyi/4AwtJJ6eZgigOuu6Ow6Tr7kgvtvpMnipphhFXjsOVQijdVO8vS6XPmHw+hJLzvAHxShptm+QauZ1hHejj6RWBIYRyLSBLOqt3XEKuM7B011Ezra2RHxIyuSaRYUzsjW5sudywXjWOHmLKaM03pIgx4Tk57Z0g4uT9jPmuJcgvnW4UY245KD1Xerak77KembypMYa8jbbnpslQ9Y6hLBlKNvIHF6gmkZnJ03WXmXxirCGWE4h9lzVp9l2WNg2R4+Qzx2tSvqE8y5LOpymEEsi/MOuEUMr7I4Nb18jr4+kV0VqVa0T5iJpM3s18cDoyuYZCKK1cMxSQ43UljLASxMaG2KIFmUhrD3K1JugiUnXFfFmD1PGa/mRCM+ZMMPzU8dpwjZpBoU+dDy10jEZeNnqAXHLX0Zpq1DHymmHSQihpf/WZfIkmLxnYgymTL6pbLjN5cqwpco2h/nqZD8JPnfDy+DxJrqFetmVx8rIDF0iMpHy+demrLISSTqfuKJf/VsHv0UGpw1SFci1Azyyugqk0BIGqUK5Yx+twQensndTxumnSz1f40+SagwudXK0JuqmJyeuafCft4SiYvKcmG1VF16w29Jv04ELXWA0w64iE9K+kyUsvrjp6a262ozF5WhZz5Not6uCc5zV5payBJNekjbeLNPm2rMkbkqGUBDEpuqYILdfJ9XhNHK/J+SVNvszpqYcsRjFX7mM9v0JvMiLDxOT9HuUa0TRkgHrya80vBfTueBUveVOcPEXX2IzX4UJh8mkEjP5wZ82Ek4du/9HECOilWxmrZvJCe9d6qGbRNeY4+dWGzrwX0t62ufU0ucZ3MiPnKwy3TnRNPgJJPy9yw+gymB5Kuaeq7HglI18rukb4HhwxS1MTtqqvp+86SltAIDGq0y1XxN87rCK6RgtZDGOusHfdgOhNRmQUJUMBo4uu0ZusrxVkmbw1NXmS68rkGsvkhwvZ8doJIkx4Tu7hpht244QHxoD9x1YAqA8OS52CVJDLGCfvuTljTgxLN/5rTpMnRm6or66sp2m39PJzHTawXBPFmbGk8yy03womb9KfizV5kmvKHa+eK4VQupljVC2/nPwtu56+x0S3KvG7VP6ZSSOIHFZeFEs3qjHnCnuvajIiw5wMxcS46mDQKpRBFK+54AMgu7ZFkUk6SksNp/4k8gtaTX5IkB2vFBapP9x0wzoOw6YJH/vnEyPf1h6cluuIjFRT7RpZrtFrw4gQSm9tMnkypnJVSZNmTQ+BXGKV6q74Bq26DDr7jDmXfB+aka946Ezp9UFsNvIHjnXAWPJSN4+L9u0oPghzad56TJ66VYll6e9mpnwRgVM2+dFlhFCTa3IhlCWyQ1a7Js/kR9UZqqwkxmpC1AiqXdagWK6jKpSdIF92YlRYW1ZmSKCHuxNEIixSLyQlSwuzU77E5FUj77sJW225Tu5NTyGUwsh7KjPOGL65ds1qgx44uR6+icn7Bibfktiu2F7NZCgZUcyFrk/NV0g+qDbyBiYvyTVy+OyhxS42TfiFcelkMF0mMTXHMRpCEXFTcj19x8k1gpcjuojFl4VQ0kuEJJg45sqsS4+uKYtjN4ZQkq+l185Qg4RQrsXomh5fXmXt/7KyBqvTFQo4Tow8abEraQPptucaU/UJs5M+9s0n0/l8M2UHvpd0+5HlGs558gLxnJwsQ4w+Y/gp81uFqVsZPE0mAaDUOyfoURhJOd38jV4vhFK98WPOhVGil019Jl8t10yn24xiXlqzXH75yPIU2T/PoMlXyTVUDlhsNz0ukmvcCserfh7COFZmmnp0jYhjN2jL3BRC2WN0jevWuy5FWKtMvtdyDXQMphBax0lkxpUgXhWpBlintWvu2XcUb7r8RrRcB1/+rQslTT4SkopeBEx+aGemWrjt0XkAeSPU9h20PQctz8E/3rAb//rdh+Ewhj989Xnp967YFpWtndQcr8RQV+uiF0Ew+XZ2zHp9dHk9ecZCRrE1oFwjG4zTt0wB6MHIhwa5JjXyl1/3AD5z8yM4edMEFlNGXRQ+KY+LWkMCyT2SlRqWXmbpYVbJNcdWusoymkVsnm6Jks1lBjZjzqkmH6v3kN68pSwUsKx2Te0QygZ6vNbV/0eJjRPJfWG6902g617keL15zxHcvOeIuJ9HjXVp5O9+/KgoerX70GLG5INIRMD81I7TcPaJG/A/0gqK8kM7M+mL6AO91sT/vPQZOHlmAhedsw13pC+Cj397D667/yCApGzvKTOTeN/rnokfe8YpePKWKXSCCDc+eFg81Bedsw1/fOn5OPfk1au0ZwK9nC48ext+5Gknou27OMdQYEzPjPz5Hz5TFOXyFbmm+gE2yTXnnbIJf/baZ+KVzzpF2U+VkdfDJ+VlN+0+AgD4w1efh1/5l+8BMM9SCPRy74ax0GjlKpSKXOPSzKxcaiG55l2XPA1nbMmKkP3yRU/Bq551Ck7aNIHDi92iTQiDmEXXxJhqefiz1z4TDx9ewmue8yRtfXIg5s+LXk+exgj0ngzVd4/XOM41L18L2HHGZvzZa5+JV//AKbXWf9WzTsF028NmQ3lk+SV62SvPzX0/CqxLI6+071sKBJNf6IQi/fu0LVN40uykZORVuYagM/mXPO1EAMDTT96ES5+dPFRfvH2fKGFMafJvev7pAIA3X3AGPn3TwwDUOPm3vGB7MwfbIOSEsLdd+JTi9chJnZ6ys0/aKCo5kqTBeb0QSpPjlTGGn/mh08WywaJrkt/MLwX4oTO34JJnnCy+K2tMQfp2KIX5uY4jRddIRr6kxyeh5TpYSuWas7ZO45JnZAbknJM2Gl+mOnI9XnkyDvlcmdY3a/IN1K4Z1PEaZbOktQT9/qvCCRvaeP3znmz8jk7l87dvwSvOP9m4zrCx9s5wA5Bby80td4WRJ+MvkpMcM+uUtdo6zpKZSR970kYj5oJeqlyzVpFVWyy/LXS5RgZF2FBIZRXyIZR5g6E3yyiC2fGaLJtPSzQwljmJy+Qa0rdDKczPd7M4dlPGa6km7zIhE/UbVaWHUEaGphtl68swxcnrDvUqJLH9g1ShjNdc8EHToPNbNmsc+hhWbc9DhMzkDx7rCv2RjL/JcOtyDaFOhprc/Nsccpj8XY3wqV4gV1ssg+uUT+tbrlPboZbT5A32YqDomnTZ3HI310O3rI8o3SOB1NiCwhwBc+2aKk3e9LkXeJoGHkblyURlceymjNeq61o0pkFCKNdiMlSToGd+NRuTr0sjP78c4JSZpD7NvqMrYrnO5GUojlfp4a/Tik9e31zQq7fQtNWCnNlZhirG57usllQDZLIIbcuUuEPXpkqu6aaOV1l2IE1+bikQYbMU1VQeXZPq2VGsdH4ScfI9VKGU91m1XhnEeaAQSl4enVLW4YhOs9IusMc4eRpT3cxQHWHM12QyVJOgwyubNQ4b6/IMzy8H2DLdwsa2h/2SkS9n8rJck2m1dZm8+GzIoCTjvgblRwWukGuqmHyxXAMkRqw+k0+uxZQU2li0v+qyBrGyremWizDiafXRONeQvQ6Tl5tNFydDpUa+xPEqSzmtPgvT6T1Vw5iX15/X1pdhCqGsuq5FY+qnx2sU89Rvs7aJz6Cge7bsXhs21rjZ6Q9zS13MTvmYmfLxRFqDBsg62puMvMxKZ0ocryZQEa+iDMp+Hp7VQF25piqe2ned2qFxxJjJMJs6FvUq19C2plqe6AAFZC/jVg9GPggz3dt3s4xUmYHS91WafPa5v8eOfkfnIa6IMy9LhjJp8nLd/F7G1E+cPL2Q17tcczTtk2zlmoYxtxxgdrKF2SlfyDXyvWRi53pfUUKdWHZavyiDkh7EcdHkqx68qszIXuQaSuaZTnMHBmHyJNfQtqZaLgJDr1oyuKXRNX4WfijLU24Jk69jcJP99/fY0eYpZDGMeUXVSmLyxSGUxqYhPThDXad+GWgZ5Kxd747XoynBKGpOMwqsSyN/dDnRX2cnW0KuqWLnesYrgNq1JmjbRW/rjMnXPIBVQl0mJ6JrClbrTa5JmXyagGXS3XvNeKVtTbVddCOeMfl0xkUGt4xdiRBKybmZVKEsCaEsIQQtxcj3dyOwtCJmKDH5shdy2XnLNPn8+r34jjyH9RVdQ8ewFkMom8QxYvJWrmkOnPPEyTbpY2bSF4lQis5ucKbKRonCnerWmqALWDT97+fhWQ14UmZn6XqVjlendls3ocn7Cfs2+Vbr1hLJ5BpP/E2YfJJgREa9V03el3wVclE2MT5DSQcdTcg1ANJWiJkmP3h0Tf8hlPp4egHNRtY7kz+2os4iVwPrzsgvdiOEMcfspK8UIdtUERYpM/YJ38WE79Qu8E/7qTLy4yLXVLFwT8T9Fxh5r3fH60QNx6tJr5eha/LTJNcsqw9aq1YIZV5zd93ypiHlmvzgcg2QhixKpYbL4+TVMggyuKHU8ChDKCPB5Nf2MzEoxl6TZ4x9gDF2D2PsNsbYlYyx2XT5CYyxaxhjC4yxDzcz1HqQnWzyFKnXsMjZyVat9WjdZJ9m3W185BpH+Vu4XoV235JqsFeBykaQ72OQZCjq1CU0+baHIIwlXTRj8slLvPj6Kr19qZSw42QFyuREuhHFyQNqyGJY6XglJl8cQjkwk3f6C6GkF0OdZu/rAUV9C0aBQc/wVwE8g3P+LAD3AXh3unwFwO8DeOeA2+8ZNDUnuYYw22OC08yk3weTN1eJGLvomqoQSjIGDYRQkuOVDG5ZdE0lkw/zTL4bJfKdw4ANqfH33XzTmNy4vLwcU50MVRJCKW1vkGYxnsNEieuoZgil6eVoTobq3cj3G0JJY1rvTJ5Q1LdgFBhoz5zzr0j/vRHA69PliwCuZ4w9dZDt18U9+47i1//1+wAgWqxtmvTV+HU5YqZOWOSUrzSZKENdTX6tk5YqrZ1A7KvIwPiuU/vhJSY/pTVYMY3rXZ+9Tal1r+NI+oKn9oxTLQ+HFzv45+/swabJLPLJN3QG0+EYmLonGXlXMtRlPT4JvRZuK4LrMHzh1sdw00OHsdAJa5Um/v3P3YH3f/ke5TtyCAJ5X0GvyVA37DqIl//VN2r/BsgKxw1yLsYJZS/jYaPJ18vPA/h0rz9ijL0NwNsA4PTT6xcFkjHhuTj7pA3i/y98ygl49mmm8/r5AAAG4klEQVSzOHPrNF77nCdh44SHt7xgO+aWkkzYTdJb9XO/9sO4Pa0mKePtF56l9AQtw3Tbw+9c8nS8/LwTjd+ff+omvP3Cs/D8M0/o8chGi+edsRlvv/AsPPu02dL1Xv0Dp5ZqjD/3w9ux2Kn3gvRcB5f9+Lm46GnbcPoJU3jlM/OV/849ZRPeuOM0HOsEhi2oOH3LNF7znFPx5M2TeM7pm3HgWAccHM87Y4s2vrBkKwne++rzsGP7Fkz4Ln7nkqfjR88/CSdubOM3XnY2Lj43u9bPfPJMcn23bync1svOPQl3PHoUp85Olr6oqvDLFz0FN+0+DAA45+SNeNUPnFq47lNP3ICffv5pSi0nGZunWjjjhKz87Y8942Q4jPXU3OLnXngmvnbP/trry3ju6ZtxwVlr+5kYFP/+Ky/EvfuOreoYGK+YAjPGrgZgKp92Gef8c+k6lwHYAeB1XNogY+ytAHZwzv9bncHs2LGD79y5s+bQLSwsLCwAgDF2M+d8h+m7SibPOb+4YuNvBfAqAC/jVW8MCwsLC4uRYiC5hjF2CYB3AbiIc77UzJAsLCwsLJrCoJr8hwG0AXw1jQG/kXP+ywDAGNsNYBOAFmPsNQB+lHN+14D7s7CwsLDoAYNG1xRGz3DOtw+ybQsLCwuLwbHGg/osLCwsLAaBNfIWFhYW6xjWyFtYWFisY1gjb2FhYbGOUZkMNUowxg4A2DPAJrYCONjQcFYD4z5+wB7DWsG4H8O4jx8Y7TGcwTnfZvpiTRn5QcEY21mU9TUOGPfxA/YY1grG/RjGffzA2jkGK9dYWFhYrGNYI29hYWGxjrHejPzlqz2AATHu4wfsMawVjPsxjPv4gTVyDOtKk7ewsLCwULHemLyFhYWFhQRr5C0sLCzWMdaFkWeMXcIYu5cxtosx9rurPZ66YIztZozdzhi7hTG2M122hTH2VcbY/enfzas9ThmMsY8xxp5gjN0hLTOOmSX4UHpdbmOMPXf1Ri7Gahr/exljj6bX4RbG2I9L3707Hf+9jLFXrM6oVTDGTmOMXcMYu4sxdidj7DfT5eN0HYqOYWyuBWNsgjH2XcbYrekx/FG6/EzG2HfSsX6aMdZKl7fT/+9Kv98+koFyzsf6HwAXwAMAzgLQAnArgPNWe1w1x74bwFZt2V8A+N308+8CeP9qj1Mb34UAngvgjqoxA/hxAF9C0kj0AgDfWaPjfy+AdxrWPS+9n9oAzkzvM3cNHMMpAJ6bft4I4L50rON0HYqOYWyuRXo+N6SffQDfSc/vvwF4U7r8owB+Jf38qwA+mn5+E4BPj2Kc64HJPx/ALs75g5zzLoBPAbh0lcc0CC4F8PH088cBvGYVx5ID5/w6AIe1xUVjvhTAP/EENwKYZYzlm7iOEAXjL8KlAD7FOe9wzh8CsAvJ/baq4Jw/zjn//9s7f9cogiiOfx6SqGhQBAmSWHgSsJIoFgrBQlGInZDCyhSWWtgH/A+0EwvRRsTCqJjSX+kVNcZIUNNpiDkQErUTfRbzNi5rNl4gl90Z3geWnZ3Z4jv33X1382a4eWXl78A00ENcPpT1oYzaeWGf5w+77LBDgWPAqNUXfcj8GQWOi6ywE/sakUKQ7wE+5a4/s/LDUicUeCQiL21Dc4BuVZ2z8heguxppq6JMc0zeXLBUxs1ciqz2+m3If4DwKzJKHwp9gIi8EJENIjIBNIHHhBHGgqpmO8XndS71wdoXgbbvZJ5CkI+ZAVU9CAwC50XkaL5Rw7guqjWuMWoGrgF7gX5gDrhcrZzWEJGtwD3goqp+y7fF4sMyfYjKC1X9par9QC9hZLGvYkn/kEKQnwV25657ra72qOqsnZvAA8JDMp8Npe3crE5hy5RpjsIbVZ23l/U3cJ2/aYDa6heRDkJwvK2q9606Kh+W60OMXgCo6gIwDhwhpMOyXffyOpf6YO3bgK/t1pZCkH8B9NmMdidhQmOsYk3/RUS2iEhXVgZOAlME7cN22zDwsBqFq6JM8xhw1lZ3HAYWc+mE2lDIT58m+ABB/xlbFbEH6AOer7e+IpbHvQFMq+qVXFM0PpT1ISYvRGSniGy38mbgBGFuYRwYstuKPmT+DAHPbMTVXqqcnV6rg7B64AMhHzZStZ4WNTcIqwXeAO8y3YQc3VPgI/AE2FG11oLuO4Rh9E9CvvFcmWbC6oOr5stb4FBN9d8yfZOEF3FX7v4R0/8eGKxav2kaIKRiJoEJO05F5kNZH6LxAtgPvDatU8Alq28QvoBmgLvARqvfZNcz1t5YD53+twaO4zgJk0K6xnEcxynBg7zjOE7CeJB3HMdJGA/yjuM4CeNB3nEcJ2E8yDuO4ySMB3nHcZyE+QMBmfrDXcXO4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mean_reward_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grDvrvILeafU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
